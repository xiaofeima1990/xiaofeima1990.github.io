<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[US Airline Flight Route Map]]></title>
    <url>%2F2018%2F05%2F19%2Fairline%20flight%20route%20map%2F</url>
    <content type="text"><![CDATA[In this post, I try to summarize the the construction of US Airline Flight Map. The data source comes from Bureau of Transportation Statistics. THe first thing I want to do is get the local airport information. For example, the GIS location, the carrier’s market share and market construction. and the distance from the near HUB. Secondly, I want to draw the flight route map. Airport InformationTo get the local airport information, we can extract from the data and use gis information to calculate distance. First, we need to extract all the flight routes flying into the particular airport. Second, we use airport address to get GIS information. Third, organize the information and output. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import pandas as pdimport geocoderfrom geopy import distanceimport timeDICT_HUB=&#123; "PA":'PHL', "MI":'DTW', "WV":"DCA", "IL":"ORD", # and we can also add other hubs in different States &#125;path="your path directory"STATE=input("input the state you want ")market_pd=pd.read_csv(path+STATE+"_market_data.csv")airport_pd=pd.read_csv(path+"airport_statistics.csv") col=list(market_pd.columns)market_pd.drop(col[-1],axis=1,inplace=True) #drop a columns ## first let me focus on only one routes from other state to PAmarket_pd_1=market_pd.loc[market_pd['DEST_STATE_ABR']==STATE,]STATE_airports=list(market_pd_1['DEST'].unique())year=list(market_pd_1['YEAR'].unique())[0]## gropuby airports from origin to dest group_airport=market_pd_1.groupby(['ORIGIN','DEST'])routes_list=group_airport.groups.keys()col_info=['code','year','name','address','lat','lgt','dis_to_hub0','carriers','major_airline','ratio_1']State_airport_info=pd.DataFrame(index=STATE_airports,columns=col_info)## HUB airport in Stateadd_HUB=airport_pd.loc[airport_pd['Code']==DICT_HUB[STATE],'add'].values[0]geo_info_HUB= geocoder.google(add_HUB)## return GIS information and calculate the distance for nn_airport in STATE_airports: new_gb = pd.concat( [ group_airport.get_group(name) for name,group in group_airport if name[1]==nn_airport ] ) new_gb.drop_duplicates(['ORIGIN'],keep='first', inplace=True) ## get carrier information from different routes new_gb['last_carrier']=new_gb['TK_CARRIER_GROUP'].str.strip("-|:") new_gb['last_carrier']=new_gb['last_carrier'].str[-2:] new_gb = new_gb[new_gb['last_carrier'] != ''] ## get market ratio carr_pd=new_gb.groupby("last_carrier") carr_pd=pd.DataFrame(carr_pd['ITIN_ID'].count()) carr_pd['ratio']=carr_pd['ITIN_ID']/sum(carr_pd['ITIN_ID']) carr_pd.sort_values(by='ratio',inplace=True) carriers=list(zip(carr_pd.index, carr_pd.ratio)) ## rest is the geographic infomation and population ## use goocode to calculate the distance add_temp=airport_pd.loc[airport_pd['Code']==nn_airport,'add'].values[0] geo_info_temp= geocoder.google(add_temp) time.sleep(0.1) while not geo_info_temp.ok: time.sleep(0.5) geo_info_temp= geocoder.google(nn_airport) airport_name=airport_pd.loc[airport_pd['Code']==nn_airport,'name'].values[0] dis_hub=distance.distance(geo_info_temp.latlng, geo_info_HUB.latlng).miles s=pd.Series([nn_airport,year,airport_name,add_temp,geo_info_temp.lat,geo_info_temp.lng,dis_hub,carriers,carriers[-1][0],carriers[-1][1]],index=col_info) State_airport_info.loc[nn_airport,]=sState_airport_info.reset_index(level=0,inplace=True)## output to airport informationState_airport_info.to_csv(path+STATE+"_airport_info.csv",index=False) draw the airport location mapOnce we get airport infromation in different State, we can then construct GIS information table and draw the map . The airport_gis.csv includes the airport information, i.e. gis information, major carriers and market ratio for the major carriers plus State. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import foliumimport pandas as pdimport copypath ="your data path "data_airport_gis=pd.read_csv(path+"airport_gis.csv")map_1 = folium.Map(location=[40.8499873, -77.8486889], tiles="Mapbox Bright", zoom_start=4) # focus location can be anywheretemp_pd=data_airport_gis[data_airport_gis['Major']==0].copy()for i in range(0,len(temp_pd)): folium.Marker([temp_pd.iloc[i]['lat'], temp_pd.iloc[i]['lgt']], popup=temp_pd.iloc[i]['code'], icon = folium.Icon(color='green',prefix="fa", icon="plane"),).add_to(map_1)temp_pd=data_airport_gis[data_airport_gis['Major']==1].copy()for i in range(0,len(temp_pd)): print(temp_pd.iloc[i]['code']) folium.Marker([temp_pd.iloc[i]['lat'], temp_pd.iloc[i]['lgt']], popup=temp_pd.iloc[i]['code'], icon = folium.Icon(color='blue',prefix="glyphicon", icon="plane"),).add_to(map_1) folium.Circle( location=[temp_pd.iloc[i]['lat'], temp_pd.iloc[i]['lgt']], radius=1609*200, popup="200 miles", color='crimson', fill=True, fill_color='crimson' ).add_to(map_1) folium.Circle( location=[temp_pd.iloc[i]['lat'], temp_pd.iloc[i]['lgt']], radius=1609*300, popup="300 miles", color='#32aaff', fill=True, fill_color='#32aaff' ).add_to(map_1) folium.Circle( location=[temp_pd.iloc[i]['lat'], temp_pd.iloc[i]['lgt']], radius=1609*450, popup="450 miles", color='#7f8eff', fill=True, fill_color='#7f8eff' ).add_to(map_1)map_1.save(path+'map.html') construct the flight route data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import sysimport pandas as pdimport geocoderfrom geopy import distanceimport timeDICT_HUB=&#123; "PA":'PHL', "MI":'DTW', "WV":"DCA", "IL":"ORD", &#125;path ="your data path"#STATE=input("input the state you want ")#Year=input("time period ")STATE="PA"Year="2000Q1"market_pd=pd.read_csv(path+STATE+Year+".csv")airport_pd=pd.read_csv(path+"airport_statistics.csv") col=list(market_pd.columns)market_pd.drop(col[-1],axis=1,inplace=True) #drop a columns ## first let me focus on only one routes from other state to PAmarket_pd_1=market_pd.loc[market_pd['DEST_STATE_ABR']==STATE,]STATE_airports=list(market_pd_1['DEST'].unique())year=list(market_pd_1['YEAR'].unique())[0]#,'ORIGIN','ORIGIN_STATE_ABR','DEST','DEST_STATE_ABR'col_map=['ITIN_ID', 'MKT_ID','AIRPORT_GROUP','TK_CARRIER_GROUP','MARKET_FARE','MARKET_DISTANCE', 'DISTANCE_GROUP','MKT_GEO_TYPE']market_pd_map=market_pd_1[col_map]# deal with the aiprot info def air_cat1(x): xx=str(x).split(":") if len(xx)==2: return xx[0] else: return xx[-2]def air_cat2(x): xx=str(x).split(":") if len(xx)==2: return xx[1] else: return xx[-1] market_pd_map['pos1']=market_pd_map['AIRPORT_GROUP'].apply(air_cat1)market_pd_map['pos2']=market_pd_map['AIRPORT_GROUP'].apply(air_cat2)airport_list=list(pd.unique(market_pd_map['pos1']))+list(pd.unique(market_pd_map['pos2']))col_info = ['Code','name','address','lat','lng']airport_gis_pd=pd.DataFrame(columns=col_info)for nn_airport in airport_list: add_temp=airport_pd.loc[airport_pd['Code']==nn_airport,'add'].values[0] airport_name=airport_pd.loc[airport_pd['Code']==nn_airport,'name'].values[0] geo_info_temp= geocoder.google(add_temp) time.sleep(1) while not geo_info_temp.ok: time.sleep(1) geo_info_temp= geocoder.google(airport_name+", "+STATE) s=pd.Series([nn_airport,airport_name,add_temp,geo_info_temp.lat,geo_info_temp.lng],index=col_info) airport_gis_pd=airport_gis_pd.append(s,ignore_index=True)airport_gis_pd.to_csv(path+"airport_PA00Q1_gis.csv",index=False)group_airport=market_pd_map.groupby(['pos1','pos2'])airport_m_pd=group_airport['ITIN_ID'].count()airport_m_pd=pd.DataFrame(airport_m_pd)airport_m_pd = airport_m_pd.rename(columns=&#123;'ITIN_ID': 'num_flights',&#125;)airport_m_pd.reset_index(level=0,inplace=True)airport_m_pd.reset_index(level=0,inplace=True)merg_fligh_od=pd.merge(airport_m_pd,airport_gis_pd[['Code','lat','lng']],left_on='pos1', right_on='Code',how="left")merg_fligh_od = merg_fligh_od.rename(columns=&#123;'lat': 'lat_1', 'lng': 'lng_1'&#125;)merg_fligh_od=pd.merge(merg_fligh_od,airport_gis_pd[['Code','lat','lng']],left_on='pos2', right_on='Code',how="left")merg_fligh_od = merg_fligh_od.rename(columns=&#123;'lat': 'lat_2', 'lng': 'lng_2'&#125;)merg_fligh_od.to_csv(path+"flight_routes.csv",index=False) Create Flight Map with plotlyFinally, with all the gis information, I can construct the flight route map. After searching for different data visualization tools, I finally decide to use plotly to do the graph. The following is the simple example for a basic route map. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import plotly.plotly as pyimport plotly import pandas as pdpath ="E:/Dropbox/psu/2017-2018-spring/IO 597/airline/data/"df_airports = pd.read_csv(path+'airport_PA00Q1_gis.csv')df_airports.head()df_flight_paths = pd.read_csv(path+"flight_routes.csv")df_flight_paths.head()airports = [ dict( type = 'scattergeo', locationmode = 'USA-states', lon = df_airports['lng'], lat = df_airports['lat'], hoverinfo = 'text', text = df_airports['name'], mode = 'markers', marker = dict( size=2, color='rgb(255, 0, 0)', line = dict( width=3, color='rgba(68, 68, 68, 0)' ) ))] flight_paths = []for i in range( len( df_flight_paths ) ): flight_paths.append( dict( type = 'scattergeo', locationmode = 'USA-states', lon = [ df_flight_paths['lng_1'][i], df_flight_paths['lng_2'][i] ], lat = [ df_flight_paths['lat_1'][i], df_flight_paths['lat_2'][i] ], mode = 'lines', line = dict( width = 1, color = 'red', ), opacity = float(df_flight_paths['num_flights'][i])/float(df_flight_paths['num_flights'].max()), ) ) layout = dict( title = '2000 Q1 flight paths&lt;br&gt;(PA)', showlegend = False, geo = dict( scope='north america', projection=dict( type='azimuthal equal area' ), showland = True, landcolor = 'rgb(243, 243, 243)', countrycolor = 'rgb(204, 204, 204)', ), ) fig = dict( data=flight_paths + airports, layout=layout )plotly.offline.plot( fig, filename=path+'d3-flight-paths.html' ) Resourcesto be continued]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>GIS</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F19%2FUse%20Typora%20for%20Math%20Formula%2F</url>
    <content type="text"><![CDATA[title: Typora for Math FormulaDate : 2018-05-19 20:08:00tag: guidecategories: Skills Use Typora for Math FormulaHow to number the equations As far as I can see, in mac system, Typora automatically number the equations in Math block . But in windows system, there is no automatically numbering function. whether it has automatically or not, we can always choose to use \tag{} and \nonumber to number or remove the number for certain equations. How to cite the equations If we want to cite the equations later, we can use \label{name} and \ref{label_name} to cite the particular equation. Of course, we need to add $ $ outsides.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F05%2F18%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Economics Important Data and Tools]]></title>
    <url>%2F2018%2F05%2F18%2FEconomics-Important-Data-and-Tools%2F</url>
    <content type="text"><![CDATA[This script lists some importance data source and tools for economic research and study NBER Recession Period NBER recession date Value Function Iteration ExampleThe example is written in matlab: mainfuction]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>economic source</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mechanism Design and Contract Theory Introduction]]></title>
    <url>%2F2018%2F05%2F18%2Fmechanism%20design%20contract%20theory%2F</url>
    <content type="text"><![CDATA[Mechanism Design Mechanism design is a field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally. Participants in the Mechanism design understand that they are playing a non-cooperative game. Game theory takes the rules of the game as given. The theory of mechanism design is about the optimal choice of the rules of the game Because it starts at the end of the game, then goes backwards, it is also called reverse game theory. Mechanism design studies solution concepts for a class of private-information games. Mechanism design theory provides a coherent framework for analyzing this great variety of institutions, or allocation mechanisms, with a focus on the problems associated with incentives and private information. By using game theory, mechanism design can go beyond the classical approach, and, for example, explicitly model how prices are set. The theory shows, for example, that so-called double auctions (where buyers and sellers post their bid- and ask-prices) can be efficient trading institutions when each trader has private information about his or her valuations of the goods traded. There are at least two reasons why we study mechanism design. normative it aids in practice the designers of real-world mechanisms. The theory of optimal auctions, for example, is frequently invoked in discussions about the design of government and industry auctions. positive it can explain why real-world institutions are. For example, we might seek to explain the use of auctions in some house sales, as well as the use of posted prices in other house sales by appealing to the theory of mechanism design One cases for the application of mechanism is that theory has been used to identify conditions under which commonly observed auction forms maximize the sellers expected revenue. Other cases such as public goods,the theory thus helps to justify governmental financing of public goods through taxation. The development of mechanism design theory began with the work of Leonid Hurwicz (1960). He defined a mechanism as a communication system in which participants send messages to each other and/or to a message center, and where a pre-specified rule assigns an outcome (such as an allocation of goods and services) for every collection of received messages. Hurwicz (1972) introduced the key notion of incentive-compatibility, which allows the analysis to incorporate the incentives of self-interested participants. The incentives created by the choice of rules of games are central to the theory of mechanism design. In the 1970s, the formulation of the so-called revelation principle and the development of implementation theory led to great advances in the theory of mechanism design. revelation principle was developed by Roger Myerson. And the resulting theory, known as implementation theory was developed by Eric Maskin. distinction between the theory of mechanism design and contract theory : In contract theory, we study the optimal design of incentives for a single agent. In mechanism design, we study the optimal design of incentives for a group of agents, such as the buyers in our first example and the colleagues in the second example. Contract theory therefore, unlike the theory of mechanism design, does not have to deal with strategic interaction. direct mechanism: the agent report their private information . (incentive compatible) (IC) A mechanism is called incentive-compatible if every participant can achieve the best outcome to him/herself just by acting according to his/her true preferences (participation constraint) no agent should be made worse off by participating in the mechanism. Incentive compatibility and the revelation principleIn Hurwiczs formulation, a mechanism is a communication system in which participants exchange messages with each other, messages that jointly determine the outcome. These messages may contain private information, such as an individuals (true or pretended) willingness to pay for a public good. The mechanism is like a machine that compiles and processes the received messages, thereby aggregating (true or false) private information provided by many agents. Each agent strives to maximize his or her expected payoff (utility or profit), and may decide to withhold disadvantageous information or send false information (hoping to pay less for a public good, say). Hurwiczs (1972) notion of incentive-compatibility can now be expressed as follows: the mechanism is incentive-compatible if it is a dominant strategy for each participant to report his private information truthfully. In a standard exchange economy, no incentive-compatible mechanism which satisfies the participation constraint can produce Pareto-optimal outcomes. Private information precludes full efficiency. This lead to the following questions : Can Pareto optimality be attained if we consider a wider class of mechanisms and/or a less demanding equilibrium concept than dominant-strategy equilibrium, such as Nash equilibrium or Bayesian Nash equilibrium? If not, then we would like to know how large the unavoidable social welfare losses are, and what the appropriate standard of efficiency should be. The revelation principle states that any equilibrium outcome of an arbitrary mechanism can be replicated by an incentive-compatible direct mechanism. Although the set of all possible mechanisms is huge, the revelation principle implies that an optimal mechanism can always be found within the well-structured subclass consisting of direct mechanisms. Direct Mechanisms A direct revelation mechanism is one where each agent is asked to report his individual preferences. In an indirect mechanism agents are asked to send messages other than preferences. (Incentive Efficient) A direct mechanism is said to be incentive efficient if it maximizes some weighted sum of the agents expected payoffs subject to their IC constraints. Dominant-strategy mechanisms for public goods provisionBefore 1970, economists generally believed that public goods could not be provided at an efficient level, precisely because people would not reveal their true willingness to pay. But Edward Clarke (1971) and Theodore Groves (1973) shows a case if there are no income effects on the demand for public goods (technically, if utility functions are quasi-linear), then there exists a class of mechanisms in which (a) truthful revelation of ones willingness to pay is a dominant strategy, and (b) the equilibrium level of the public good maximizes the social surplus. Each person is asked to report his or her willingness to pay for the project, and the project is undertaken if and only if the aggregate reported willingness to pay exceeds the cost of the project. If the project is undertaken, then each person pays a tax or fee equal to the difference between the cost of the project and everyone elses reported total willingness to pay. With such taxes, each person internalizes the total social surplus, and truth-telling is a dominant strategy. But due to the drawbacks of Dominant-strategy mechanisms, focus of the literature shifted from dominantstrategy solutions to so-called Bayesian mechanism design. Bayesian mechanisms for public goods provisionIn a Bayesian model, the agents are expected-utility maximizers. The solution concept is typically Bayesian Nash equilibrium. Regarding Clarke-Groves dominant-strategy mechanism, Claude dAspremont and Louis- AndrGrard-Varet (1979) showed that this problem can be solved in the Bayesian version of the model. In the Bayesian model, agents are expected utility maximizers, and the IC constraints only have to hold in expectation. So it is easier to satisfy. The fact that English villages were much earlier than French villages in deciding on public goods such as enclosure of open fields and drainage of marshlands can arguably be ascribed to the fact that French villages required unanimity on such issues whereas the English did not. (why) Why if participation is voluntary and decisions to start the project must be taken unanimously, then the problem of free-riding becomes severe? -bilateral trade ImplementationIncentive compatibility guarantees that truth-telling is an equilibrium, but not that it is the only equilibrium. Many mechanisms have multiple equilibria that produce different outcomes. In view of these difficulties, it is desirable to design mechanisms in which all equilibrium outcomes are optimal for the given goal function. The quest for this property is known as the implementation problem. Groves and Ledyard (1977) and Hurwicz and Schmeidler (1978) showed that, in certain situations, it is possible to construct mechanisms in which all Nash equilibria are Pareto optimal, while Eric Maskin (1977) gave a general characterization of Nash implementable social-choice functions. ApplicationOptimal selling and procurement mechanismsGeneral revenue-equivalence theorem. In particular, four well-known auction forms (the so-called English and Dutch auctions, and first-price and second-price sealed bid auctions, respectively) generate the same expected revenue. Myerson (1981) showed that if the bidders are symmetric (drawn from one and the same type pool) and if the seller sets an appropriate reserve price (a lowest price below which the object will not be sold), then all of the four well-known auction formats are in fact optimal if the bidders types are independently drawn from a uniform distribution on the interval from zero to one hundred, then the optimal reserve price is 50, independently of the number of bidders. This reserve price induces bidders whose valuations exceed 50 to bid higher than they would otherwise have done, which raises the expected revenue. On the other hand, if it so happens that no bidder thinks the object is worth 50, then the object is not sold even if it has a positive value to some bidder and no value at all to the seller.(not understand) This outcome is clearly not Pareto efficient in the classical sense Maskin (1992) found that, under certain conditions, an English auction maximizes social welfare even if each bidders valuation depends on other bidders private information. One might be tempted to discount the need for the governments auction to maximize social welfare, for the following reason. Suppose there are two potential bidders, A and B, and B values the asset more than A. Then, even if the government allocates the asset to the wrong person, A, would not then B simply buy the asset from A (assuming it can be traded)? If so, then B (who values the asset the most) would always get the asset in the end - so the government should not worry too much about getting the initial allocation right. However, this argument is incorrect, because it does not take informational constraints into account. The Laffont-Maskin and Myerson-Satterthwaite impossibility results (see Section 2.4) imply that B may not buy the asset from A even if B values it the most. Therefore, getting the initial allocation of ownership right may be of the utmost importance. Regulation and auditingThis is about monopolies and oligopolies. The situation changed dramatically with the pioneering contributions of Baron and Myerson (1982) and Sappington (1982, 1983). the regulatory process was modeled as a game of incomplete information. The regulator did not have direct access to information about the monopolists true production costs. Social Choice TheoryIn the axiomatic social-choice theory pioneered by Kenneth Arrow (1951), there is a set X of feasible alternatives and n individuals who have preferences over these. A social choice rule is a rule that selects one or several alternatives from X on the basis of the individuals preferences, for any given such preference profile Maskin monotonicity is the following property of a social choice rule. Suppose that, for preference profile P1, the chosen alternative is A1. Consider another preference profile P2 such that, the position of A1 relative to each of the other alternatives either improves or stays the same as in P1. Then, A1 should still be chosen at P2. plurality rule. An alternative in $X$ is said to be the plurality alternative if it is top-ranked by the greatest number of voters Asymmetric InformationIn 1996, the Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel was awarded to James Mirrlees and William Vickrey for their fundamental contributions to the theory of incentives under asymmetric information, in particular its applications to the design of optimal income taxation and resource allocation through different types of auctions. The theory of markets with asymmetric information rests firmly on the work of three researchers: George Akerlof (University of California, Berkeley), Michael Spence (Stanford University) and Joseph Stiglitz (Columbia University). Some questions like Why do people looking for a good used car typically turn to a dealer rather than a private seller? Why dofirms pay dividends even if they are taxed more heavily than capital gains? Why is it in the interest of insurance companies to offer a menu of policies with different mixes of premiums, coverage and deductibles? challenge the traditional economic theory. Those questions share a same realistic assumption : one side of the market has better information than the other. Akerlof showed how informational asymmetries can give rise toadverse selection in markets. Spence demonstrated that informed economic agents in such markets may have incentives to take observable and costly actions to crediblysignaltheir private information to uninformed agents, so as to improve their market outcome. Stiglitz showed that poorly informed agents can indirectly extract information from those who are better informed, by offering a menu of alternative contracts for a specific transaction, so-called screening through self-selection. Akerlof market failure and asymmetric information. (lemon market) Michael Spence Spence’s most important work demonstrates how agents in a market can use signaling to counteract the effects of adverse selection. In this context, signaling refers to observable actions taken by economic agents to convince the opposite party of the value or quality of their products. A fundamental insight is that signaling can succeed only if the signaling cost differs sufficiently among the ’senders’. Stiglitz insurance market Contract TheoryAn eternal obstacle to human cooperation is that people have different interests. In modern societies, conflicts of interests are often mitigated if not completely resolved by contractual arrangements. The idea that incentives must be aligned to exploit the gains from cooperation has a long history within economics. In the 1700’s, Adam Smith argued that sharecropping contracts do not give tenants sufficient incentives to improve the land. Today, incentive problems are almost universally seen through its lens. The theory has had a major impact on organizational economics and corporate finance, and it has deeply influenced other fields such as industrial organization, labor economics, public economics, political science, and law. A classic contracting problem has the following structure. A principal engages an agent to take certain actions on the principals behalf. However, the principal cannot directly observe the agents actions, which creates a problem of moral hazard : the agent may take actions that increase his own payoff but reduce the overall surplus of the relationship. To be specific, suppose the principal is the main shareholder of a company and the agent is the company’s manager. As Adam Smith noted, the separation of ownership and control in a company might cause the manager to make decisions contrary to the interests of shareholders. Therefore contract theory has traditionally been divided into two parts: the theory of hidden information (also referred to as the theory of adverse selection) and the theory of hidden action (also referred to as the theory of moral hazard). To alleviate this moral-hazard problem, the principal may offer a compensation package which ties the managers income to some (observable and verifiable) performance measure. We refer to this as paying for performance. But any performance measure is likely to be imprecise and noisy, so in the end the optimal compensation schedule must trade off incentive provision against risk-sharing. informativeness principle Formally, suppose P is considering making the transfer t a function of some signal s in addition to $\beta$. The informativeness principle implies that she should do so if and only if is not a sufficient statistic for a given ($\beta$; s). Paying for performance requires both the ability to write sufficiently detailed contracts ex ante, as well as the ability to measure and verify performance ex post. incomplete contracting approach allocation of decision rights. Decision rights are often determined by property rights, property rights generate bargaining power,which in turn determines incentives. image the company want to do R&amp;D. the researcher’s right and company’s right during the innovation of the product. More generally, when performance-based contracts are hard to write or hard to enforce, carefully allocated decision rights may produce good incentives and thus substitute for contractually specified rewards. This insight is a cornerstone in the theory of incomplete contracts. The theory has been highly influential within corporate finance and organizational economics, costs and benefits of mergers the distribution of authority within organizations whether or not providers of public services should be privately owned how outside owners can control a company’s inside managers through the design of corporate governance and capital structure Complete Contracts: principal-agent modelA simple framework : imprecise performance measure Incomplete Contracts: Allocating Decision Rights Hold up problem property right In property-rights models of Hart and coauthors, decision rights over physical assets are the crucial source of bargaining power and incentives. What is the connection between mechanism design, asymmetric information and contract theory?! Screening Contract and Incentive Contract Voting Scheme of the social choice.Condorcet conditions: If there is a candidate that is preferred to every other candidate in pair wise majority -rule comparisons , that candidate should be choose $(a\succ b),\ (a\succ c)\ (a\succ d)$ $a$ is the winner. The potential candidate should win all the pairs Arrow impossibility theorem : Any social welfare function W over three or more outcomes that is pareto efficient and independent of irrelevant alternatives is dictatorial. Pareto Efficiency : W is Pareto efficient if for any $o_{1},\ o_{2}\in O,\ \forall i\ o_{1}\succ_{i}o_{2}$ implies that $o_{1}\succ_{W}o_{2}$ If everyone agrees $a$ better than b, social welfare function is also required that $a$ is better than $b$ . If one person feels differently, this does not restrict soical welfare function at all. Idependence of Irrelevant Alternatives : $W$ is independent of Irrelevant Alternatives if for any $o_{1},\ o_{2}\in O,$ and any two preference profiles $[\succ’],[\succ’’]\in L^{n}$ $\forall i\ (o_{1}\succ_{i}’o_{2}\ \text{iff},\ o_{1}\succ_{i}’’o_{2})$ implies that $(o_{1}\succ_{W}([\succ’])o_{2})$ iff $(o_{1}\succ_{W}([\succ’’])o_{2})$ if agent 1 likes $o_{1}$ better than $o_{2}$, in preference $\succ’$ , he must also likes $o_{1}$ better than $o_{2}$, in preference $\succ’’$ . Non-dictatorship : $W$ does not have a dictator if $\lnot\exists i\ \forall o_{1},o_{2}(o_{1}\succ_{i}o_{2}\Rightarrow o_{1}\succ_{W}o_{2})$ given a set of agents $N=\{1,2,3…\}$a finite set of outcomes (or alternative or candidates ) $O$ and the set of preferences over outcomes $L_{NS}$ Social choice function $C:\ L_{NS}^{N}\rightarrow O$ Social welfare function $W:\ L_{NS}^{n}\rightarrow L_{NS}$ social choice function VS social welfare function Assume that finite set of alternatives are a,b and c. social choice function can have one single output which can be a or b or c. A social welfare function can have any ranking as output such as $a &lt; b &lt; c​ $ . Tian’s notesThe incentives structure and information structure are thus two basic features of any economic system. Indeed, delegation of a task to an agent who has different objectives than the principal who delegates this task is problematic when information about the agent is imperfect. This problem is the essence of incentive questions. Thus, conflicting objectives and decentralized information are the two basic ingredients of incentive theory. The three words contracts, mechanisms and institutions are to a large extent synonymous. They all mean rules of the game, which describe what actions the parties can undertake, and what outcomes these actions would be obtained. While mechanism design theory may be able answer big questions, such as socialism vs. capitalism, contract theory is developed and useful for more manageable smaller questions, concerning specific contracting practices and mechanisms. mechanism design is normative economics, in contrast to game theory, which is positive economics. (normative economics) a part of economics that expresses value or normative judgments about economic fairness or what the outcome of the economy or goals of public policy ought to be. (positive economics) is the branch of economics that concerns the description and explanation of economic phenomena. It focuses on facts and cause-and-effect behavioral relationships and includes the development and testing of economics theories. Screening and SignalingFor the most part we will focus on the situation where the Principal has no private information and the agents do. This framework is called screening, because the principal will in general try to screen different types of agents by inducing them to choose different bundles. The opposite situation, in which the Principal has private information and the agents do not, is called signaling, since the Principal could signal his type with the design of his contract. First Best and Second BestIn the class, what we learned of first best is that principal can extract all the rent from agents; second best involves the incentive compatibility constraint and principal need to share some rent to agents to pay for the “information value” In Contract Theory The first-best refers to the best you could do if you knew agent’s preferences over labor an income (i.e., if you did not have to impose the incentive compatibility constraint), and the second-best is the best you can do if agents have to reveal their preferences themselves. Difference There is not much connection between the two notions as defined above. Every combination of the two notions is a priori possible. Both a mechanism and a contract can be First-best ex-post efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract must be deterministic) First-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is not imposed and the outcome of the mechanism/contract can be random) Second-best ex-post efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract must be deterministic) Second-best ex-ante efficient (i.e., efficient when incentive compatibility constraint is imposed and the outcome of the mechanism/contract can be random) AppendixQuasi-linear Utility quasi-linear utility functions are linear in one argument, generally the numeraire. This utility function has the representation $u(x_{1},x_{2},\ldots,x_{n})=x_{1}+\theta(x_{2},\ldots,x_{n})$. Informally, an agent has quasi-linear utility if it can express all its preferences in terms of money and the amount of money it has will not create a wealth effect. As a practical matter in mechanism design, quasi-linear utility ensures that agents can compensate each other with side payments. In regard to surplus, quasi-linear preferences entail that Marshallian surplus will equal Hicksian surplus since there would be no wealth effect for a change in price.]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>Mechanism Design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决next主题和mathjax下划线冲突问题]]></title>
    <url>%2F2018%2F05%2F18%2Fsolve-mathjax-display%20in%20next%2F</url>
    <content type="text"><![CDATA[源于网络 在markdown写技术文章时，经常会用到latex渲染公式，markdwon集成了mathjax用于渲染公式。默认next主题是集成公式的，只需要在每篇博客前面加上mathjax: true即可。 Hexo默认使用hexo-renderer-marked引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线_代表斜体，会被渲染引擎处理为标签。因为类Latex格式书写的数学公式下划线 _ 表示下标，有特殊的含义，如果被强制转换为标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，xi在开始被渲染的时候，处理为$xi$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。 解决办法更换markdown公式渲染引擎12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save hexo 默认渲染引擎是hexo-renderer-marked, 另一个渲染引擎 hexo-renderer-kramed 在默认引擎上修复了一下bug。 修改语义转义规则更换引擎后，行间公式 确实可以正常渲染了，但行内公式$ $还是有一些问题，接下来到node_modules\kramed\lib\rules\inline.js下，修改第11行 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 修改第20行：12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 重启hexo 12hexo clean hexo g]]></content>
      <categories>
        <category>Skills</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Continuous Time Model]]></title>
    <url>%2F2018%2F05%2F14%2Fcontinous-time-model%2F</url>
    <content type="text"><![CDATA[Abstract This notes is about the study of continuous time dynamic model based on “The Art of Smooth Pasting” Brownian MotionThe Brownian Motion was first formulated to represent the motion of small particles suspended in a liquid. To decribe the motion, given the initial value $x_{0}$ at time $t=0$ , the random variable $x_{t}$ for any $t&gt;0$ is normally distributed with mean $(x_{0}+\mu t)$ and variance $(\sigma^{2}t)$ . The parameter $\mu$ measures the trend , and $\sigma$ the volatility of the process. For the Brownian motion, $x_{t}$ as its ’position’, and a graph of $x_{t}$ against $t$ as its ’path’. The infinitesimal random increment $dx$ over the infinitesimal time $dt$ having mean $\mu dt$ and variance $\sigma^{2}dt$. Just as we write normal random varible as $\mu+\sigma w$ where $w$ is a standard normal varible of $N(0,1)$, we can dx=\mu dt+\sigma dw \tag{1}where $w$ is a standardized Brownian motion (Wiener process) whose increment $dw$ has zeros mean and variance $dt$ The (Ito) Calculus of such infinitesimal random variables differs in some important ways from the usual non-random calculus. Since a fully rigorous treatment of ito calculus is quite difficult, a non-rigorous exposition should suffices for many economic applications. So here we can approximate Brownian motion by a discrete random walk. Then the normal distribution arises as the limit of a sum of independent binary variables $\bigtriangleup x$ over discret time intervals $\bigtriangleup t$ when these go to zero in a particular way. Random Walk and Brownian MotionThis subsection try to bridge the discrete random walk with continuous Brownian motion. To do so, we can divide time into discrete period of length $\bigtriangleup t$, and $\bigtriangleup h$ can be seen as the step-length or the distance between successive points. Let $\bigtriangleup x$ be a random variable that follows a random walk: in one time period it moves up one step in space with probability $p$, and one step down with probability $q=1-p$. Note: $\bigtriangleup h$ is given positive number, and $\bigtriangleup x$ is a random variable that takes values $\pm\bigtriangleup h$ . Figure 1.1 illustrate this with time marching downward and position shown horizontally. So the mean and variance of $\bigtriangleup x$ are \begin{align} E[\bigtriangleup x] & =p\bigtriangleup h+q(-\bigtriangleup h)=(p-q)\bigtriangleup h \tag{2} \\ Var[\bigtriangleup x] & =E[(\bigtriangleup h)^{2}]-(E[\bigtriangleup x])^{2}\nonumber \\ & =4pq(\bigtriangleup h)^{2} \tag{3} \end{align}A time interval of length $t$ has $n=t/\bigtriangleup t$ such discrete steps. Since the successive steps of the random walk are independent, the cumulated change $(x_{t}-x_{0})$ is a binominal variate with mean n(p-q)\bigtriangleup h=(p-q)\bigtriangleup h\cdot t/\bigtriangleup tand variance 4npq(\bigtriangleup h)^{2}=4pq(\bigtriangleup h)^{2}\cdot t/\bigtriangleup t(Recall the relation between Bernoulli distribution and Binomial distribution) Remember the binomial distribution, a ’success’ in any one trial counts as 1 and occurs with probability p, while a failure counts as 0 and occurs with probability $q=1-p$. The (random) number of successes in $n$ independent trials has expectation $np$ and variance $npq$. So back to this problem, now the success counts as $\bigtriangleup h$ and failure as $-\bigtriangleup h$. Now set \bigtriangleup h=\sigma\sqrt{\bigtriangleup t} \ \ (4)​and p=\frac{1}{2}[1+\frac{\mu}{\sigma}\sqrt{\bigtriangleup t}],\ \ q=\frac{1}{2}[1-\frac{\mu}{\sigma}\sqrt{\bigtriangleup t}] \ \ (5)or p=\frac{1}{2}[1+\frac{\mu}{\sigma^{2}}\sqrt{\bigtriangleup t}],\ \ q=\frac{1}{2}[1-\frac{\mu}{\sigma^{2}}\sqrt{\bigtriangleup t}]Then 4pq=1-\left(\frac{\mu}{\sigma}\right)^{2}\bigtriangleup tSubstitute these into the above expression. and let $\bigtriangleup t$ go to zero. Then the Binomial distribution converges to the normal, with mean t\frac{\mu}{\sigma^{2}}\bigtriangleup h\frac{\bigtriangleup h}{\bigtriangleup t}=\mu tand variance t[1-\left(\frac{\mu}{\sigma}\right)^{2}\bigtriangleup t]\frac{\sigma^{2}\bigtriangleup t}{\bigtriangleup t}\rightarrow\sigma^{2}t Thus we can regard Brownian motion as the limit of the random walk, when the time interval and the space step-ength go to zero together. The mean of $(x_{t}-x_{0})$ is $\mu t$ and its standard deviation is $\sigma\sqrt{t}$. For large $t$, we have $\sqrt{t}\ll t$; in the long run, the trend is the dominant determinant of brownian motion. But for small $t$, we have $t\ll\sqrt{t}$, so volatility dominates in the short run. Another manifestation of this volatility is seen by calculating the expected length of a path. We have E(|\bigtriangleup x|)=\bigtriangleup hso the total expectedd length of the path over time interval from $0$ to $t$ is t\bigtriangleup h/\bigtriangleup t=t\sigma/\sqrt{\bigtriangleup t}\rightarrow\inftyas $\bigtriangleup t$ goes to zero. For small but finite $\bigtriangleup t$, the total length of almost all sample path is very large. Therefore each path must have many ups and sowns and look very jagged. Most such sample paths are not differentiable. When discussing the expected rate of change, therefore, we must write $E[dx]/dt$ not $E[dx/dt]$ Ito’s LemmaSuppose $x$ follows Brownian motion with parameters $(\mu,\sigma)$. Consider a stochastic process y that is related to $x$ by $y=f(x)$ where $f$ is a given non-random function. We want to related changes in y to those in x. The rules of conventional calculus suggest writing $dy=f’(x)dx$ and taking expectations. How the changes in x relate to changes in y ? The rules of conventional calculus suggest writing $dy=f’(x)dx$ and taking expectations. But this turns out to be wrong. The reason is as follows. Starting at $y_{0}=f(x_{0})$ , consider the position a small amount of time $t$ later. y_{t}-y_{0}=f'(x_{0})(x_{t}-x_{0})+\frac{1}{2}f''(x_{0})(x_{t}-x_{0})^{2}+...Hence \begin{aligned} E[y{t}-y{0}] & =f'(x{0})E[x{t}-x{0}]+\frac{1}{2}f^{''}(x{0})E[(x{t}-x{0})^{2}]+...\\ & =f'(x{0})\mu t+\frac{1}{2}f^{''}(x{0})[\sigma^{2}t+\mu^{2}t^{2}]+...\\ & =[\mu f'(x{0})+\frac{1}{2}\sigma^{2}f^{''}(x{0})]t+...\end{aligned}Note that the second order term in the Taylor expansion of $f(x)$ contributes a term that is not ignorable. The reason is that the variance of the increments of $x$ is linear in t. This is the feature that makes the calculus of Brownian motion so different from the usual calculus of non-random variables. A similar calculation will show that Var[y_{t}-y_{0}]=f'(x_{0})^{2}\sigma^{2}t+...Let $x$ denote a general starting and $y=f(x)$. Consider the infinitesimal increment $dy$ over the next infinitesimal time interval $dt$. We can use the above expression replacing $t$ with $dt$ and ignoreing higher order terms in $dt$. Therefore $dy$ has mean E[dy]=[f^{'}(x)\mu+\frac{1}{2}f^{''}(x)\sigma^{2}]dtand variance Var[dy]=f'(x)^{2}\sigma^{2}dtSo $y$ follows the general diffusion process defined by dy=[f'(x)\mu+\frac{1}{2}f^{''}(x)\sigma^{2}]dt+f'(x)\sigma dw \ \ (6)This is Ito Lemma in the form that will prove most useful for the study. A slight generalization is easily available : if $y=f(x,t)$, the Taylor expansion has an addition term in $f_{t}$, and dy=[f_{x}(x,t)\mu+\frac{1}{2}f_{xx}(x,t)\sigma^{2}+f_{t}(x,t)]dt+f_{x}(x,t)\sigma dw \ \ (7)For a simple intuition, return to the discrete random walk formulation, and suppose $x$ ahs zeros trend, so $p=q=\frac{1}{2}$. Now $E[\bigtriangleup x]=0$ and as time passes the distribution of $x$ merely spreads out with linearly increasing variance around an unchanging mean. From the standard intution of risk -aversion , or Jensen’s inequality, we know the sign of $E[\bigtriangleup y]$ will be negative if $f$ is concave and positive if $f$ is convex. GEometric Brownian motionNow suppose x follows the Brownian motion, and let $X=e^{x}$. Ito’s Lemma gives E[dX]=[e^{x}\mu+\frac{1}{2}e^{x}\sigma^{2}]dt=X[\mu+\frac{1}{2}\sigma^{2}]dtand Var(dX)=[e^{x}]^{2}\sigma^{2}dt=X^{2}\sigma^{2}dt Therefore the process of $X$ can be written dX/X=[\mu+\frac{1}{2}\sigma^{2}]dt+\sigma dwThis is called the geometric or proportional Brownian motion. It provides a good first approximation of the dynamics of exchange rates, prices of natural resources, and more generally many asset prices. Conversely, if $X$ follows the geometric Brownian motion dX/X=vdt+\sigma dz \ \ (8)then using Ito’s Lemma we find that $x=\ln X$ follows the ordinary or absolute Brownian motion dx=[v-\frac{1}{2}\sigma^{2}]dt+\sigma dwNotice that $d\text{\ensuremath{\ln}}x\neq dX/X$ . Suppose $X$ follows geometric Brownian motion (8) with the known position $X_{0}$ , with the relation of $x=\ln X$ we finally can get E(X_{t})=\exp(x_{0}+\mu t+\frac{1}{2}\sigma^{2}t)=X_{0}e^{vt}Since exponential is a convex function, therefore E[X_{t}]=E[\exp x_{t}]>\exp E[x_{t}]=\exp(x_{0}+\mu t)Some generalizationsIf the trend and volatility coefficients are functions of the current state ($x$), this is often called a diffusion process.When they are functions of time as well, it is sometimes called an Ito process. dx=\mu(x,t)dt+\sigma(x,t)dw \ \ (9)By the defintion of diffiusion process, we can see that (8) geometric brownian motion is a special case of diffusion process dX=\mu Xdt+\sigma XdwIn come cases, we may need processes that revert toward some central level $\bar{x}$ of the state variable dx=-\theta(x-\bar{x})dt+\sigma dw \ \ (10)where $\theta$ is some positive constant. Also, several variabes $x_{i}$ for $i=1,2,..m$ may follow Brownian motion, their volatility components being linear combinations of independent standard Wiener processes $w_{j}$ for $j=1,2,…n$ dx_{i}=\mu_{i}dt+\sum_{j=1}^{n}a_{ij}dw_{j}Then \begin{aligned} E[dx{i}] & =\mu{i}dt \\ Var[dx_{i}] &=\sum_{j=1}^{n}(a{ij})^{2}dt \\ Cov[dx_{i},dx_{k}]&=\sum_{j=1}^{n}a_{ij}a_{kj}dt \end{aligned}Such multi-variable processes are important in financial economics. Discounted Present ValuesNow we can move to some real economic examples. First let’s consider an economic unit, such as a firm , in a dynamic stochastic setting. Its state at time $t$ is given by a state variable $x_{t}$ that follows a Brownian motion with exogenous parameters $\mu$ and $\sigma$. There is a net flow payoff $f(x_{t})$ , such as profit or dividend, that depends on the state $x_{t}$. The expected present value $F(x)$ of the payoff starting at a given initial position $x_{0}=x,$and using an exogenously specified discout rate $\rho$, is defined by F(x)=E\{\int_{0}^{\infty}f(x_{t})e^{-\rho t}dt|x_{0}=x\} \ \ (11)Ultimately we will be interested in controlling or regulating the motion of $x_{t}$to optimize such expected present values net of the cost of control. To this end, we begin by evaluating $F(x)$ explicitly when $f(x)$ has some particularly simple functional forms such as exponentials and polynomial. Then we can get power series expression for $F(x)$ when $f(x)$ is any analytic function. Prensent value for exponential and polynomialsFirst we conside the special case when the flow payoff has the form f(x)=\exp(\lambda x)The discounted present value $F(x)$ will be finite when $\lambda$ is in a certain range. Starting from the initial value $x_{0}=x$ , $x_{t}$ at time $t$ has a normal distribution with mean $(x+\mu t)$ and variance $\sigma^{2}t$. Then E[exp(\lambda x_{t})|x_{0}=x]=\exp[\lambda(x+\mu t)+\frac{1}{2}\lambda^{2}\sigma^{2}t]How to get the above expression, Here is the illustrative example. Suppose a random variable $x$ is normal with mean$m$and std $s$ \begin{aligned} E[\exp(\lambda x)] & =\frac{1}{s\sqrt{2\pi}}\int_{-\infty}^{\infty}\exp(\lambda x)\exp\left[-\frac{(x-m)^{2}}{2s^{2}}\right]dx\nonumber \\ & =\frac{\exp(\lambda m)}{s\sqrt{2\pi}}\int_{-\infty}^{\infty}\exp(\lambda y-\frac{y^{2}}{2s^{2}})dy\ y=x-m\nonumber \\ & =\frac{\exp(\lambda m+\frac{1}{2}\lambda^{2}s^{2})}{s\sqrt{2\pi}}\int_{-\infty}^{\infty}\exp\left[-\frac{(y-\lambda s^{2})^{2}}{2s^{2}}\right]dy\nonumber \\ & =\exp[\lambda m+\frac{1}{2}\lambda^{2}s^{2}] \ \ (12) \end{aligned}Now we have the present value of$F(x)$ \begin{aligned} F(x) & =\int{0}^{\infty}E[exp(\lambda x{t})|x_{0}=x]\exp(-\rho t)dt\nonumber \\ & =\exp(\lambda x)\int_{0}^{\infty}\exp[\lambda\mu t+\frac{1}{2}\lambda^{2}\sigma^{2}t]dt\nonumber \\ & =\exp(\lambda x)/(\rho-\lambda\mu-\frac{1}{2}\lambda^{2}\sigma^{2}) \ \ (13) \end{aligned}The intergal converges provided the denominator is positive. To describe the convergence property, it is useful to give a uniforma notation and interpretation at the outset. \phi(\xi)=\rho-\xi\mu-\frac{1}{2}\xi^{2}\sigma^{2} \ \ (14)Labed as the Fundamental Quadratic of Brownian Motion The condition for the convergence of the integral in (11) amounts to requiring that $\lambda$lies in the interval $(-\alpha,\beta)$ This includes $\lambda=0$ . We can use the formula 11 to derive an expression for $F(x)$ when $f(x)$ in any integer power $x^{n}$. For exponential flow function \begin{matrix} F(x) & =E[\int_{0}^{\infty}\sum_{n=0}^{\infty}\frac{1}{n!}(\lambda x{t})^{n}e^{-\rho t}dt|x_{0}=x]\\ & =\sum_{n=0}^{\infty}\frac{\lambda^{n}}{n!}E[\int_{0}^{\infty}x{t}^{n}e^{-\rho t}dt|x_{0}=x] \end{matrix} \begin{aligned} \exp(\lambda x) & =\sum_{k=0}^{\infty}\frac{1}{n!}\lambda^{n}x^{n}\\ [\rho-\mu\lambda-\frac{1}{2}\sigma^{2}\lambda^{2}]^{-1} & =\frac{1}{\rho}\sum_{n=0}^{\infty}\rho^{-n}(\mu\lambda+\frac{1}{2}\sigma^{2}\lambda^{2})^{n} \end{aligned}For each power of $x​$ we write out first two, and rest follows same tedious calcualtion \begin{aligned} E\{\int_{0}^{\infty}x{t}e^{-\rho t}dt \} & =\frac{\mu}{\rho^{2}}+\frac{x}{\rho} \ \ (15) \\ E\{ \int_{0}^{\infty}x{t}^{2}e^{-\rho t}dt \} & =[\frac{\sigma^{2}}{\rho^{2}}+\frac{2\mu^{2}}{\rho^{3}}]+\frac{2\mu x}{\rho^{2}}+\frac{x^{2}}{\rho} \ \ (16) \end{aligned}Finally consider any analytic $f(x)$ with the power series representation f(x)=\sum_{n=0}^{\infty}f_{n}(x^{n})assumed uniformly convergent for all $x$. Having found the expected present values of all integer powers, we can integrate term by term to find the $F(x)$ corresponding to this analytic $f(x)$. Thus we can in-principle complete the calculation of present values for most common functions of Brownian motion. Present values for powers of geometric Brownian motionNext suppose $X$ follows the geometric brownian motion, dX/X=vdt+\sigma dwWe want to find the expected present value when flow payoff is $g(X)=X^{\lambda}$. Note that $x=\ln(X)$ follows the Brownian motion dx=(v-\frac{1}{2}\sigma^{2})dt+\sigma dwand $X^{\lambda}=\exp(\lambda x)$ , we have \begin{aligned} E{\int_{0}^{\infty}X_{t}^{\lambda}e^{-\rho t}dt} & =\exp(\lambda x)/[\rho-(v-\frac{1}{2}\sigma^{2})\lambda-\frac{1}{2}\sigma^{2}\lambda^{2}]\\ & =X^{\lambda}/[\rho-v\lambda-\frac{1}{2}\sigma^{2}\lambda(\lambda-1)] \ \ (17) \end{aligned}The above will converge as long as denominator is positive. So the convergence condition is now defined as \psi(\xi)=\rho-v\xi-\frac{1}{2}\sigma^{2}\xi(\xi-1) \ \ (18)two economically natural assumption that $\rho&gt;0$ and $\rho&gt;v$ guarantee the roots of above equation $-\gamma1$ . Similarly, $\lambda$ should lie in the interval $(-\gamma,\delta)$. For $Y=X^{\lambda}$, we have dY/Y=[\lambda v+\frac{1}{2}\lambda(\lambda-1)\sigma^{2}]dt+\lambda\sigma dwFor the convergence of the expected present value of $Y$, the discount rate must exceed the trend growth rate of $Y$ which is $\psi(\lambda)&gt;0$ And for the relation between absolute and geometric Brownian motion, if $x=\ln X$, we have that $\mu=v-\frac{1}{2}\sigma^{2}$ \begin{aligned} \psi(\xi) & =\rho-(\mu+\frac{1}{2}\sigma^{2})\xi-\frac{1}{2}\sigma^{2}\xi(\xi-1)\\ & =\rho-\mu\xi-\frac{1}{2}\sigma^{2}\xi^{2} \\ & =\phi(\xi)\end{aligned}And then the roots will also correspond ,with $\alpha=\gamma$ and $\beta=\delta$ A basic differential equation for present valueNow let us return to absolute Brownian motion $x$ and the flow payoff $f(x)$ , and consider an alternative characterization of the expected present value $F(x)$. Here we just want to use an alternative method to split the $F(x)$ integral into the contribution over the initial infinitesimal time interval from $0$ to $dt$ and the integral from $dt$ to $\infty$. Why we want to do this? Because through this we can get basic differential equation for $F(x)$ Note that start from $x+dx$ we need take discount value back to time $t=0$ . So we have F(x)=f(x)dt+e^{-\rho dt}E[F(x+dx)]Note that this is already an approximation in regarding $f(x)$ as constant over the small interval $dt$. the resulting error in $f(x)dt$ is of order $dt^{2}$. We further simplify the expression and get \begin{aligned} F(x) & =f(x)dt+(1-\rho dt)(F(x)+[E[F(x+dx)]-F(x)])\\ & =f(x)dt+F(x)-\rho F(x)dt+[E[F(x+dx)]-F(x)] \end{aligned}Therefore \begin{aligned} \rho F(x)dt & =f(x)dt+[E[F(x+dx)]-F(x)]\nonumber \\ & =f(x)dt+E[dF] \ \ (19) \end{aligned}The above is the arbitrary equation. Think of the entitlement to the flow payoffs as a capital asset; $F(x)$ is its value. Contemplate holding this asset over the period $(t,t+dt)$. This yields a dividend $f(x)dt$, and an expected capital gain $E[dF]$. The sum of these two should equal the normal return $\rho F(x)dt$ By Ito Lemma, (previous) E[dF]=\mu F^{'}(x)dt+\frac{1}{2}\sigma^{2}F^{''}(x)dt Substituting into (15) and dividing by $dt$, we get \frac{1}{2}\sigma^{2}F^{''}(x)+\mu F^{'}(x)-\rho F(x)+f(x)=0 \ \ (20)Derivation by discrete approximationWe regarded Brownian motion as the limit of a discrete random walk, and we can also derive the differential equation (16) by that approach. Label the discrete points in the $x$ space by $i$ , and the discrete time periods by $j$. Let $i_{j}$ denote the position of the particle at time $j$; future positions are of course random variables given our initial information at $j=0$. Then the expected present value can be written as F(i)=E\left\{ \sum_{j=0}^{\infty}f(i_{j})\bigtriangleup te^{-j\rho\bigtriangleup t}|i_{0}=i\right\}After the first step, the same problem restarts with a new initial state $i_{1}$, which from the time $0$ perspective can be either $(i+1)$ with probability $p$ or $(i-1)$ with probability $q$. Thus the expectation on the right hand side becomes F(i)=f(i)\bigtriangleup t+e^{-\rho\bigtriangleup t}[pF(i+1)+qF(i-1)]Now expand the right hand side, ignoring terms of higher order than $\bigtriangleup t.$ Note that e^{-\rho\bigtriangleup t}=1-\rho\bigtriangleup t+...Next , we reuse the definite of (5) of $p$ and $q$ and the relation (4) between the stepsize $\bigtriangleup h$ and the time interval $\bigtriangleup t$ we can get \begin{aligned} pF(i+1)+qF(i-1) & =\frac{1}{2}[1+(\mu/\sigma)\sqrt{\bigtriangleup t}]F(x+\bigtriangleup h)\\ & \ \ +\frac{1}{2}[1-(\mu/\sigma)\sqrt{\bigtriangleup t}]F(x-\bigtriangleup h)\\ & =\frac{1}{2}[1+(\mu/\sigma)\sqrt{\bigtriangleup t}][F(x)+F^{'}(x)\bigtriangleup h\\ & \ \ +F^{''}(x)(\bigtriangleup h)^{2}+... ] \\ & =\frac{1}{2}[1+(\mu/\sigma)\sqrt{\bigtriangleup t}][F(x)-F^{'}(x)\bigtriangleup h+\\ & \ \ +F^{''}(x)(\bigtriangleup h)^{2}+... ] \\ & =F(x)+\mu F^{'}(x)\bigtriangleup t+\frac{1}{2}\sigma^{2}F^{''}(x)\bigtriangleup t+...\end{aligned}Substituting and simplifying yield the same equation (the above equation) as (16) F(i)=f(i)\bigtriangleup t+e^{-\rho\bigtriangleup t}+[1-\rho\bigtriangleup t][...]The general solutionThis part tells how to solve the second order differential equation. The differential equation (16) is linear. Therefore its general solution is the sum of two parts: any solution of the equation as a whole (particular integral) and the general solution of the homogeneous part of the equation with the term $f(x)$ omitted (the complementary function)1. To find the complementary function, write the homogeneous part of the equation: \frac{1}{2}\sigma^{2}F^{''}(x)+\mu F^{'}(x)-\rho F(x)=0Its general solution can be expressed as a linear combination of two independent solution. For instance, if we try solutions ofthe form $\exp(\xi x),$we get \exp(\xi x)[\frac{1}{2}\sigma^{2}\xi^{2}+\mu\xi-\rho]=0 Since the exponential is always positive, this holds if and only if \frac{1}{2}\sigma^{2}\xi^{2}+\mu\xi-\rho=0This is just $\phi(\xi)$ introduced above. So $\xi$ must equal $-\alpha$ or $\beta$ the two roots. The two roots are distinct. They have opposite signs. The two solutions $e^{-\alpha x}$and $e^{\beta x}$ are independent and the general solution is Ae^{-\alpha x}+Be^{\beta x} \ \ (21)where $A$ and $B$ are undetermined constants. Finding a particular solution to the full equation (16) is often an art (with lucky) , but for the exponential and polynomial forms we tried before, there are obvious choice. Begin with the exponential case. When $f(x)=exp(\lambda x)$ the form $F(x)=K\exp(\lambda x)$ suggest itself K(\frac{1}{2}\sigma^{2}\lambda^{2}+\mu\lambda-\rho)\exp(\lambda x)+\exp(\lambda x)=0we get K=1/\phi(\lambda) Combining this particular solution and the earlier complementary function, the general solution for the expected present value in the exponential case becomes F(x)=\frac{1}{\phi(\lambda)}e^{\lambda x}+Ae^{-\alpha x}+Be^{\beta x} \ \ (22)In fact, a simple argument shows that when the flow $f(x)$ is the exponential $exp(\lambda x)$ the expected present values $F(x)$ must be a multiple of $\exp(\lambda x)$. To give a formal argument, define $y_{t}=x_{t}-h$ and consider the stochastic process $y_{t}$ dy=dx=\mu dt+\sigma dwand the initial position $y_{0}=(x+h)-h=x$. The flow benefit can be written F(x+h)=e^{\lambda x_{t}}=e^{\lambda h}e^{\lambda y_{t}}=e^{\lambda h}f(y_{t})Integrating over time and taking expectation, we get F(x+h)=e^{\lambda h}E\left\{ \int_{0}^{\infty}f(y_{t})e^{-\rho t}dt|y_{0}=x\right\}Subtracting $F(x)$ from both side, dividing by $h$, and letting $h$ go to zero, we get \lim_{h\rightarrow0}\frac{F(x+h)-F(x)}{h}=\lim_{h\rightarrow0}\frac{\exp(\lambda h)-1}{h}F(x)or F^{'}(x)=\lambda F(x)As we can see the genral solution above was a combination of three terms, of which only the first, corresponding to the particular solution we guessed initially, had the right exponential form. Both $A$ and $B$ are zero. In the same way we can guess similar form of functions. Now consider the polynomial case. When $f(x)=x^{n}$ for a positive integer $n$ , a natural guess for the particular integral is F(x)=\sum_{m=0}^{n}a_{m}x^{m} \ \ (23)Substituting this in $16$ we can get \begin{aligned} \frac{1}{2}\sigma^{2}\sum_{m=2}^{n}m(m-1)a_{m}x^{m-2}+\mu\sum_{m=1}^{n}ma_{m}x^{m-1} \\ -\rho\sum_{m=0}^{n}a_{m}x^{m}+x^{m} & =0\end{aligned}Collecting like powers of $x​$ together and equating the coefficient of each separately to zero since the equation must hold as an identity in $x​$ , we find a_{n}=\frac{1}{\rho},a_{n-1}=n\mu/\rho^{2}and for $m=0,1,2,…(n-2)$ the recursive relation \rho a_{m}=(m+1)\mu a_{m+1}+\frac{1}{2}(m+1)(m+2)\sigma^{2}a_{m+2} \ \ (24)This determines all the coefficients $a_{m}​$. Once again we can verify that the expected present value cannot have any contribution from the exponential of the complementary function. So (18) is the full solution. Differential equation for the geometric Brownian motionNow we turn again to geometric Brownian motion. Given a flow cost function $g(X)$, we want to find G(X)=E\left\{ \int_{0}^{\infty}g(X_{t})e^{-\rho t}dt|X_{0}=X\right\} \ \ (25)Proceeding exactly as before, we get the arbitrage equation pG(X)dt=g(X)dt+E[dG]and Ito Lemma gives E[dG]=vXG'(X)dt+\frac{1}{2}(\sigma X)^{2}G^{''}(X)dtTherefore the basic differential equation for the case of geometric Brownian motion is \frac{1}{2}\sigma^{2}X^{2}G^{''}(X)+vXG^{'}(X)-\rho G(X)+g(X)=0 \ \ (26)The complementary function is easily seen to be CX^{-\gamma}+DX^{\delta} \ \ (27)The same argument as before. Once agian the particular integral must be guessed. Previously we considered $g(X)=X^{\lambda}$ and natural guess for this is $G(X)=KX^{\lambda}$. Substituting into (20) we find K=1/\psi(\lambda)where $\psi(\lambda)$ is the fundamental quadratic for the geometric case. An argument similar to what we made above, the full solution is jsut the particular integral we guessed ; $C$ and $D$ are both zero. We can change variables to transform geometric Brownian motion into an absolute one, and this gives an alternative way to find expected present values. Define $x=\ln X$ and $\mu=v-\frac{1}{2}\sigma^{2}$ . Let $f(x)=g(e^{x})$ $G(X)=F(\ln(X))$ . We will F'(x)=e^{x}G'(e^{x})=XG'(X)and F^{''}(x)=(e^{x})^{2}G^{''}(e^{x})+e^{x}G^{'}(e^{x})=X^{2}G^{''}(X)+XG^{'}(X)Substitution in (16) we find 0=\frac{1}{2}\sigma^{2}X^{2}G^{''}(X)+vXG^{'}(X)-\rho G(X)+g(X)which is just (20). The two approaches are mutually consistent. General diffusion processIf X follows the general diffusion process (9) rather than the simple Brownian motion (1), the expected value function $F(x)$ and the flow function $f(x)$ are linked by a differential equation that is a natural generalization of (16) namely \frac{1}{2}\sigma(x)^{2}F^{''}(x)+\mu(x)F^{'}(x)-\rho F(x)+f(x)=0 \ \ (28)Unfortunately, this time the solution is not the corresponding simple. The complementary function is specific to each case depending on the functional form of $\mu(x)$ and $\sigma(x)$. Analytical solution is possible only in very special cases. The geometric case was discussed above. for linearly mean-reverting motion (10) a power series solution related to the Confluent Hypergeometric function is avaibale; an example is developed in 5.1 . If $x$ follows the Ito process (9) whose parameters depend on time as well as the state $x$, or if the flow payoff is a function $f(x,t)$ like wise, or the process ends at a given time $T$ so the time remaining to the end of the horizon matters, then we must allow the expected present value to depend on time too. The basic equation becomes a partial differential equation \frac{1}{2}\sigma(x,t)^{2}F_{xx}(x,t)+\mu(x,t)F_{x}(x,t)-\rho F(x,t)+F_{t}(x,t)+f(x,t) \ \ (29)The solution of this is much hard, and typically needs numerical methods. BarriersThe above discussion assumed that the Brownian motion particle $x$ (absolute case) was free to range over the entire real line $(-\infty,+\infty)$. Similarly, in the case of geometric Brownian motion $X$ could range over $(0,\infty)$. In practice there are restrictions on the range. For example the output price facing one firm in a competitive industry is bounded above, because new firm will enter if the price rises beyond a certain point. Other cases the restrictions are exogenously imposed, as in the case of a government-imposed agricultural price floor. Finally, and most important for our purpose here, the restrictions arise endogenously through purposive optimal control of the stochastic propose. In this section, the book will start with some specified restrictions on the process, and show how their effects on expected present values can be computed. The restrictions are called barriers. They can constrain upward or downward movement of $x$ , and are of two types, absorbing and reflecting. at $b$ means that the process $(1)$ is allowed to proceed unhindered so long as $x_{t}&lt;b$, but if ever $x_{t}=b$, the process is terminated. That might be the end of our planning horizon, or merely the end of the movement of $x$, so that it remains at $b$ for ever after. Sometimes such an absorbed process might be immediately restarted at a point $c&lt;b$. Lower and two sided absorbing barriers are defined in obvious ways. at $b$ means that the process $(1)$ is allowed to proceed unhindered so long as $x_{t}&lt;b$, but if ever $x_{t}=b$, and the next increment $dx$ is positive, then the sign of this increment is reversed, as if the particle were reflected in a mirror placed at $b$. Once again, lower and two-sided barriers are defined analogously. We can even have an absorbing barrier on one side and a reflecting barrier on the other side. Now consider a process with barriers for $x$, take a flow payoff function $f(x)$, and define the expected present value function $F(x)$ as in (20). When there wre no barriers, we took two approaches to finding $F(x)$. The first approach was direct. The distribution of $x_{t}$ given $x_{0}$ was known and simple, namely normal, so expected values of functions of $x_{t}$ could be found relatively easily. When there are barriers, between the starting time $0$ and the instant in question $t$ the particle might have been reflected at barriers any numer of times, or been absorbed with positive probability. The distribution of $x_{t}$ conditional on $x_{0}$ is much more complicated. This would be discussed in section 6.3. The second and indirect approach proves simpler. We show that $F(x)$ satisfies the same basic differential equation in the interior of the region of variation of $x$, and certain end-point conditions at the barriers. Finding $F(x)$ then amounts to solving the differential equations subject to the end-point conditions. Basic differential equationSuppose the process moves between barriers located at $a$ and $b$. For the moment it makes no difference whether the barriers are reflecting or absorbing, and $a=-\infty,b=\infty$ are permissible. Choosing the initial point $x_{0}=x$ in the interior of the range $[a,b]$. Over an infinitesimal time interval $dt$ , the probability of $x_{t}$, reaching either barrier is negligible. Therefore the arbitrage argument of section 2.3 remains valid, and the basic differential equation (20) holds. The derivation of section 2.4, based on discretization of the $x$ space, is also valid so long as the length of each step $\bigtriangleup h$ is chosen sufficiently small. Even when the process has barriers, the flow function $f(x)$ is genrally defined over the full unrestricted range $(-\infty,+\infty)$ of the $x$ space. Even if it is not, we can extend its definition in some simpleway over the full range. Let $F_{0}(x)$ denote the expected present value of $f(x_{t})$ as defined in 2.1 above, and computed ignoring the barriers. Then we can follow previous procedure to calculate $F_{0}(x)$ for any analytic functions $f(x)$. Our choice of $F_{0}(x)$, the expected present value ignoring barriers, as the particular integral gives a very nnice economic interpretation to the solution. Since the particular integral is what the expected present value would have been in the absence of barriers, the remaining part, namely the complementary function, must equal the effect of the barriers. For example, a price ceiling cuts off the upside profit potential of a firm; its effect is simply captured by the appropriate term in the complementary function. This actually tells us that it is the complementary function that exercise such control. Recalling the form (21) of the complementary function. F(x)=F_{0}(x)+Ae^{-\alpha x}+Be^{\beta x} \ \ (30)The constants $A$ and $B$ must be determined using some other conditions on the problem. This is where the barriers come into play. If $x$ is free to range over the entire line $(-\infty,+\infty)$ , we already know that $F(x)=F_{0}(x)$. Then $A$ and $B$ must both be zero. If $x$ is not restricted on the lower side, but has an upper barrier at $b$, then we can get some information by considering what happens for very large negative values of $x$. Starting from such a value, the particle is unlikely to reach $b$ in any reasonable future time. Then the unrestricted expected present value $F_{0}(x)$ should be a good approximation for $F(X)$. But with $\alpha&gt;0,\ e^{-\alpha x}$ goes to $\infty$ as x goes to $-\infty$. This would spoil the desired approximation unless $A=0$. Thus we have determined one of the constants. The other, $B$, is fixed using end-point conditions at the barrier $b$, which we will examine shortly. Similarly, if $x$ has a only lower barrier at $a$, then the inspection of $F(x)$ as $x$ goes to $\infty$gives $B=0$, while $A$ is fixed by conditions at the barrier. If there are barriers on both sides, then both $A$ and $B$ are fixed by the end-point conditions at the barriers. Geometric Brownian motionNow suppose the underling variable is $X$, and it follows the proportional or geometric Brownian motion 1.8, with barriers at $c$ and $d$. Let the flow function be $g(X)$. Extending its definition outside the bariers to the full range $(0,\infty)$ of $X$ if necessary, following the similar setup as above 3.1, let $G_{0}(X)$ satisfies the basic differential equation 2.16 for geometic Brownian motion, and serves as a particular integral over the restricted range $(c,d)$ Then general solution is G(X)=G_{0}(X)+CX^{-\gamma}+DX^{\delta} \ \ (31)$-\gamma$ and $\delta$ are the roots of the fundamental quadratic (27), and $C,D$ are constants to be determined by conditions that apply at the barriers . StoppingNow let us return to the case of absolute Brownian motion, and begin the analysis of barriers, starting with an upper absorbing barrier.Suppose the barrier is placed at $b$. To set the stage for subsequent analysis of control, allow an exogenous terminal payoff $W(b)$. If x stays for ever at b after absorption, $W(b)$ may simply be the capitalized value of the constant flow payoff, $f(b)/\rho$. But other interpretations are also possible. If a cost $k$ must be paid at the instant of absorption, we simply subtract if from $W(b)$ to make it $W(b)-k$. The expression (2.1) for the expected present value must be modified to take account of the barrier and the terminal payoff. F(X)=E\left\{ \int_{0}^{t(b)}f(x_{t})e^{-\rho t}dt+e^{-\rho(b)}W(b)|x_{0}=x\right\} \ \ (32)where $t(b)$ is the first time the process reaches $b$ starting at $x$, and of course it is a random variable given the initial information. To find the conditions that hold at the barrier, we repeat the arbitrage calculation, but now starting at or near the barrier rather than at an interior point. Revert to the discrete approximation to Brownian motion, with time intervals $\bigtriangleup t$ and steps of length $\bigtriangleup h$. Starting at $(b-\bigtriangleup h)$, we have F(b-\bigtriangleup h)=f(b-\bigtriangleup h)\bigtriangleup t+(1-\rho\bigtriangleup t)[pW(b)+qF(b-2\bigtriangleup h)]Expanding the Taylor series, we get \begin{aligned} F(b)-F'(b)\bigtriangleup h+... & =f(b)\bigtriangleup t+...+\frac{1}{2}W(b)[1-\rho\bigtriangleup t+...]\\ & +\frac{1}{2}(1-\rho\bigtriangleup t)[F(b)-2F'(b)\bigtriangleup h+...]\end{aligned}Collecting terms, dividing by $\bigtriangleup h$ and taking limits, we find F(b)=W(b) \ \ (33)This is sometimes called the Value Matching condition. ResettingHere the process is allowed to follow (1.1) so long as $x&lt;b$, but the instant $x$ hits $b$, it is reset at $x=c&lt;b$, and the process is restarted. The calculation is as above, except that $F(c)$ replace $W(b)$, and we get a value matching condition $F(c)$ replaces $W(b)$, and we get a value matching condition $F(b)=F(c)$ . If the resetting entailed a cost $k$, this would become $F(b)=F(c)-k$ ReflectionFinally, suppose the process is reflected at an upper barrier $b$. Now starting at $b$ we are sure to go to $(b-\bigtriangleup h)$, so \begin{aligned} F(b) & =f(b)\bigtriangleup t+(1-\rho\bigtriangleup t)F(b-\bigtriangleup h)\\ & =f(b)\bigtriangleup t+(1-\rho\bigtriangleup t)[F(b)-F'(b)\bigtriangleup h+...]\end{aligned}Cancelling $F(b)$ from both sides, the leading term on the right hand side becomes $-F’(b)\bigtriangleup h$; recall that it is of order $\sqrt{\bigtriangleup t}\geq\bigtriangleup t$. Dividing $\bigtriangleup h$ and taking limits gives F'(b)=0 \ \ (34)To repeat the point in a slightly different way, note that starting at $b$, the next step is sure to be downward through the distance $\bigtriangleup h=\sigma\sqrt{\bigtriangleup t}$. If $F’(b)$ were non-zero, this would make $dF=-\sigma\sqrt{\bigtriangleup t}$. The capital gain term in the arbitrage equation (2.9) would be of order $\sqrt{\bigtriangleup t}$. But the normal return and dividend terms are of order $\bigtriangleup t$, and therefore relatively much smaller. Then the arbitrage equation would not hold. This contradiction proves that $F’(b)\neq0$ is impossible. Some people call any condiiton that pertains to the first-order derivatives of an expected present value of a function of Brownian motion a Smooth Pasting Condition. If reflection is costly, with cost $m$ per unit distance through which the particle is reflected, we substrct $m\bigtriangleup h$ from the right hand side, and the conditon becomes $F’(b)+m=0$ At a lower reflecting barrier, a, similar analysis gives $F’(a)-m=0$. Similar conditions for geometric Brownian motion follows. Example: price ceilingConsider a firm that produces a unit of output per unit time. The cost of production is $W$. The price $P$ follows a geometric Brownian motion with parameters $v$ and $\sigma$. In this section I suppose that even when $P$ falls below $W$ so that the operating profit $(P-W)$ is negative, the firm must continue operation. This is sometimes required of public utilities or transport services. Begin by supposing that there are no barriers on the price process. Then we can use the formula (26) for $\lambda=0$ and $1$ to get the expected present value of profits. G_{0}(P)=P/(p-v)-W/\rho \ \ (35)Next suppose there is an upper reflecting barrier on the price process at $P=b$. This could be a ceiling imposed by the government, or the method laid out in Section 3., the expression for the expected present value of profit is easily seen to be G(P)=G_{0}(P)+DP^{\delta} \ \ (36)Note that the other term in the complementary function, $CP^{-\gamma}$, must be zero to ensure a finite expected value as $P$ goes to zero. To determine the constant $D$, we use the condition (34), so G'(b)=\delta Db^{\delta-1}+1/(\rho-v)=0Then D=-b^{1-\delta}/[\delta(\rho-v)]]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>continuous time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python下微信公共号开发如何从网络url上传素材]]></title>
    <url>%2F2017%2F01%2F29%2Fpython-upload%20pic%20to%20wechat%2F</url>
    <content type="text"><![CDATA[微信上传图片的接口定义非常不友好，一般情况只能从local上传图片。当我们想通过云端存储介质上传或者url链接上传时，很难有有效的方法来处理。网上寥寥无几提出了一些利用php语言修改文件参数什么的来上传。但对于python来说，一直没有有效的办法。这里我找到一种极为简单方便的方法来上传文件 为了实现自己微信公众号自动回复图片消息，我被微信蛋疼无比的素材接口折磨疯掉了。但老天眷顾，一次偶然的机会我发现了可以利用request package 极为简单的实现上传功能。12345678access_token=&quot;your access_token&quot;cc = &quot;f(x)+1&quot; # the string pp=requests.get(&quot;http://latex.codecogs.com/png.latex?\dpi&#123;300&#125; \huge %s&quot; % cc).content # get the online png data (binary data)files = &#123;&apos;media&apos;: (&apos;temp2.png&apos;,pp)&#125; # the first item &quot;temp2.png&quot; is the file name, the second one is the file dataupload_url=&quot;https://api.weixin.qq.com/cgi-bin/media/upload?access_token=&quot;+access_token+&quot;&amp;type=image&quot; # set your access_tokenr =requests.post(upload_url, files=files) # upload media_id=json.loads(r.content)[&apos;media_id&apos;] # if it is success, you get media id 当然这里可以将iles = {&#39;media&#39;: (&#39;temp2.png&#39;,pp)}pp 换成 新浪sae storage 中的 get_object(“filename”) 将storage 存储的图片上传到微信上]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>wechat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Academic Assistance From Apple Device]]></title>
    <url>%2F2017%2F01%2F19%2Facademic-assistant-from-Apple%2F</url>
    <content type="text"><![CDATA[这篇文章主要介绍和记载如何更好利用各种电脑、电子设备以及软件、应用等帮助更好的工作、学习和研究 ipad iphone 照片互传功能开始我并不知道apple设备存在着icloud share 可以把照片在各个设备共享互传。现在发现了这一功能，并且大概摸清楚了怎么个情况。我发现可以利用这个功能来进行高效率的娱乐分享，学习以及工作。苹果设备上的照片、相册存在如下几个默认文件夹：Camera RollMy Photo StreamFavoriteSelfiesScreenshotsRecently Deleted你自己定义的 从名字中可以大致判断出后面各个文件夹的功能。这里介绍第一个和第二个文件夹。第一个Camera Roll 存储了在本地设备上所有的拍照照片和截图啥的，但凡你在本设备上的照片截图下载图片啥的都在这个文件夹内。如果这个文件夹的照片被删掉了，那么除了My Photo Stream 中的照片，所有的文件夹都会受到影响。比如照片A在我自己定义的temp中，我从Camera Roll中删掉A 那么temp中也没有A了。 在有wifi情况下，各个apple设备会互传最新拍摄的照片，并且储存在My Photo Stream 下。My Photo Stream 独立于Camera Roll 。 删掉My Photo Stream 不会影响Camera Roll 中的照片，反之亦然。 而若想把My Photo Stream 再转存到本地，select照片后 点向上的方框那个东东，选择save image 即可。 因此，我可以通过ipad 画图，然后保存在相册中，通过My Photo Stream 同步啥的同步到其他设备上，假如我买了mac 那么可以打开icould phoe share 功能 把照片都同步起来。 旧的照片给备份好。 画图的同步一般来说，我做笔记需要画图啥的，用电脑画图太蛋疼了，可以利用ipad 画图软件（找一个好点的）来画图。然后通过dropbox 还是啥的同步到电脑上作为图片插入lyx或者其他书写软件中！]]></content>
      <categories>
        <category>Skills</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Turn Latex Formula to Word]]></title>
    <url>%2F2017%2F01%2F16%2Fhow%20to%20turn%20latex%20formula%20to%20word%2F</url>
    <content type="text"><![CDATA[This article introduce a little technic to assist research work. sometimes it is really frustrated to use word type math formula. by the converience and beautiful display of formula, latex is usually our first choice to write formula. But we usually encounter the case that we need to translate the formula from latex file into word document. Here I introduce a technic to help us save time and translate the formula smoothly from latex to word. First of all, we need to download the MathType software, (6.00 later). Then open the software and find the options for copy and paste. And then, click that option find Math_format of MathML 2.0 (namespace attr). Then, we can just open our latex file or lyx file, to copy the formula into the MathType, and then copy it again into the Word. When copy into the word, we do not need to insert a new math formula area. Just Copy Directly is fine!]]></content>
      <categories>
        <category>Skills</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django Learning -1]]></title>
    <url>%2F2016%2F12%2F25%2Fdjango-learning-1%2F</url>
    <content type="text"><![CDATA[These series of article record my learning process of Django Web Development Introduction to DjangoDjango loosely follows a model-viewcontroller design pattern,which greatly helps in building clean and maintainable web applications MVC pattern in Web DevelopmentA few years ago, the model-view-controller pattern came for web-based applications was introduced. This software engineering pattern separates data (model), user interface (view), and data handling logic (controller), so that one can be changed without affecting the others. advantages of DjangoThe standard features expected of a web framework are all available in Django, which include: supporting MVC A template and text filtering engine with simple but extensible syntax. A form generation and validation API. An extensible authentication system. A caching system for speeding up the performance of applications. A feed framework for generating RSS feeds. Apart from those above features, Django also have the following advantages: Tight Integration between Components First of all, Django provides a set of tightly integrated components; all of these components have been developed by the Django team themselves. Django was originally developed as an in-house framework for managing a series of news-oriented websites. Later its code was released on the Internet and the Django team continued its development using the Open Source model. Because of its roots, Django’s components were designed for integration, reusability and speed from the start. Object-Relational Mapper Django’s database component, the Object-Relational Mapper (ORM), provides a bridge between the data model and the database engine. It supports a large set of database systems, and switching from one engine to another is a matter of changing a configuration file. This gives the developer great flexibility if a decision is made to change from one database engine to another. Clean URL Design The URL system in Django is very flexible and powerful; it lets you define patterns for the URLs in your application, and define Python functions to handle each pattern. This enables developers to create URLs that are both user and search engine friendly. Automatic Administration Interface Django comes with an administration interface that is ready to be used. This interface makes the management of your application’s data a breeze. It is also highly flexible and customizable. Advanced Development Environment In addition, Django provides a very nice development environment. It comes with a lightweight web server for development and testing. When the debugging mode is enabled, Django provides very thorough and detailed error messages with a lot of debugging information. All of this makes isolating and fixing bugs very easy. Multi-Lingual Support Django supports multi-lingual websites through its built-in internationalization system. This can be very valuable for those working on websites with more than one language. The system makes translating the interface a very simple task start a new Django Projectopen a terminal (or command prompt for Windows users), type the following command, and hit enter:django-admin.py startproject new_project This command will make a folder named django_bookmarksin the current directory, and create the initial directory structure inside it. Let’s see what kinds of files are created: new_project/ __init__.py manage.py setting.py urls.py The explanation of the four main files are : init.py Django projects are Python packages, and this file is required to tellPython that the folder is to be treated as a package. A package in Python’s terminology is a collection of modules, and they are used to group similar files together and prevent naming conflicts. manage.py This is another utility script used to manage your project. You can think of it as your project’s version of django-admin.py. Actually, both django-admin.pyand manage.pyshare the same back-end code. manage.py is used to control the demand of Django. settings.py This is the main configuration file for your Django project. In this file you can specify a variety of options, including the database settings, site languages, which Django features are to be enabled, and so on. Various sections of this file will be explained as we build our application during the next chapters, but in this chapter, we will only see how to enter the database settings. url.py This is another configuration file. You can think of it as a mapping between URLs and Python functions that handle them. This file is one of Django’s powerful features, and we will see how to utilize it in the next chapter. When we start writing code for our application, we will create new files inside the project’s folder. So the folder also serves as a container for our code. In the following case, I try to use the soical bookmark to illustrate how to ceate a simple webapp from scratch. This include the setup of database(qulite3), setup MVC framework, establishment of Registration, tag Voting,Commenting functions, be familar with the User Interface with Ajax, the setup of Administration Interface, searching, user network and finally Deploying the web app. setup the databaseopen the settings.py file, we can see that the following information for the setup of database: 1234567891011121314151617181920DEBUG = TrueTEMPLATE_DEBUG = DEBUGADMINS = ( # ('Your Name', 'your_email@example.com'),)MANAGERS = ADMINSDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'. 'NAME': '', # Or path to database file if using sqlite3. # The following settings are not used with sqlite3: 'USER': '', 'PASSWORD': '', 'HOST': '', # Empty for localhost through domain sockets or '127.0.0.1' for localhost through TCP. 'PORT': '', # Set to empty string for default. &#125;&#125; Here we use the sqlite3 as the default database to implement the app. So it is simple to just setup the “ENGINE” and “NAME” : 123456789101112131415161718DEBUG = TrueTEMPLATE_DEBUG = DEBUGMANAGERS = ADMINSDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'. 'NAME': 'book_markdb', # Or path to database file if using sqlite3. # The following settings are not used with sqlite3: 'USER': '', 'PASSWORD': '', 'HOST': '', # Empty for localhost through domain sockets or '127.0.0.1' for localhost through TCP. 'PORT': '', # Set to empty string for default. &#125;&#125; To create the database and initialize the tables in the database, we can use the following command:$ python manage.py syncdb Note: there maybe an error **No module named &#39;sqlite3.base&#39;** Launching the Development ServerLaunching the Development ServerAs discussed before, Django comes with a lightweight web server for developing and testing applications. This server is pre-configured to work with Django, and more importantly, it restarts whenever you modify the code.To start the server, run the following command:$ python manage.py runserveraccording to the instruction of the server, we can open the browser to see our web app. build the basic web applicationIn this part, I am intended to learn MVC framework and templates. First of all, we need to create our own main page(index) URLs and Views: Creating the Main PageTo create our own welcome page, we need to define an entry point to our application in the form of a URL (URL), and tell Django to call a particular Python function when a visitor accesses this URL. (VIEW) VIEWA view in Django terminology is a regular Python function that responds to a page request by generating the corresponding page. To write the main page (any functions of Django), we we first need to create a Django application inside our project. To create it, issue the following command within our folder: $ python manage.py startapp bookmarks After running this command, Django will create a folder named bookmarksinside the project folder with these three files: init.py: This file tells Python that bookmarksis a Python package. views.py: This file will contain our views. models.py: This file will contain our data models. Open the file bookmarks/views.pyin your code editor and enter the following: 1234567891011121314151617from django.http import HttpResponsedef main_page(request): output = ''' &lt;html&gt; &lt;head&gt;&lt;title&gt;%s&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;h1&gt;%s&lt;/h1&gt;&lt;p&gt;%s&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; ''' % ( 'Django Bookmarks', 'Welcome to Django Bookmarks', 'Where you can store and share bookmarks!' ) return HttpResponse(output) We import the class HttpResponsefrom django.http. We need this class in order to generate our response page. We define a Python function that takes one parameter named request; this parameter contains user input and other information. For example, request. GET, request.POSTand request.COOKIESare dictionaries that contain get, post and cookie data respectively. We build the HTML code of the response page, wrap it within an HttpResponseobject and return it. relation about request and response httprequest 表示来自某客户端的一个单独的HTTP请求。HttpRequest对象是Django自动创建的。 URLAs you may recall from the previous chapter, a file named urls.pywas created when we started our project. This file contains valid URLs for our application, and maps each URL to a view that is a Python function.As you can probably tell, the file contains a table of URLs and their corresponding Python functions (or views). The table is called urlpatterns, and it initially contains example entries that are commented out. Each entry is a Python tuple that consists of a URL and its view. The Django use URL to allocate each url address to each view and do different tasks. Let’s start by removing the comments and adding an entry for the main page:12345from django.conf.urls import patterns, include, urlfrom bookmarks.views import *urlpatterns = patterns('',(r'^$', main_page),) The file imports patterns, include, url from the module django.conf.urls. This module provides the necessary * functions to define URLs. We import everything from bookmarks.views. This is necessary to access our views, and connect them to URLs.The patterns function is used to define the URL table. It contains only one mapping for now — from r’^$’to our view main_page. the synax in URL expression need some knowledge from regix. For more detail, please see the documentation online at: http://docs.python.org/lib/module-re.html Symbol / Expression Matched String . (Dot) Any character. ^ (Caret) Start of string. $ End of string. * 0 or more repetitions. + 1 or more repetitions. ? 0 or 1 repetitions. A \ B means A or B. [a-z] Any lowercase character. \w Any alphanumeric character or _. \d Any digit. the Database and ModelsIf you are used to dealing with the database directly through SQL queries, then you may find Django’s approach to database access a bit different. Loosely speaking, Django abstracts access to database tables through Python classes. To store, manipulate and retrieve objects from the database, the developer uses a Python-basedAPI. In order to do this, SQL knowledge is useful but not required. This technique is best explained by example. For our bookmarking application, we need to store three types of data in the database: Users (ID, username, password, email) Links (ID, URL) Bookmarks (ID, title, user_id, link_id) Each user will have their own entry in the Users table. This entry stores the username, password and email. Similarly, each link will have a corresponding entry in the links table. We will only store the link’s URL for now.As for the Bookmarks table, you can think of it as the joining table between Users and Links. When a user adds a bookmark, an entry for the bookmark’s URL is added to the links table if it doesn’t already exist, and then a joining entry is added to the Bookmarks table. This entry connects the user with the link, and stores the title that the user entered for their bookmark. To use and define the model, we first need to activate it in our Django project. This is done byediting settings.py, looking for the INSTALLED_APPSvariable, and adding ourapplication name (django_bookmarks.bookmarks) to it: 1234567INSTALLED_APPS = ('django.contrib.auth','django.contrib.contenttypes','django.contrib.sessions','django.contrib.sites','django_bookmarks.bookmarks',) For all three database: Link Data Model 123from django.db import modelsclass Link(models.Model): url = models.URLField(unique=True) User Data Model from django.contrib.auth.models import User use the default user model in django Bookmark Data Model 12345from django.contrib.auth.models import Userclass Bookmark(models.Model): title = models.CharField(max_length=200) user = models.ForeignKey(User) link = models.ForeignKey(Link) **many to many relation** A bookmark belongs to one user and one link. However, one user may have many bookmarks, and one link may be bookmarked by many users. In database language we say there is a many-to-many relationship between users and links. However, there is no way to actually represent a many-to-many relationship such as this one using a standard database system. In our particular case, we will invent the concept of a bookmark to break up this many-to-many relationship into its constituent one-to-many relationships. The first of these is the one-to-many relationship between the user and their bookmarks. One user can have many bookmarks, but each bookmark is associated with only one user. That is to say, each user can bookmark a particular link once. The second of these is the one-to-many relationship between a link and its bookmarks. One link can have many bookmarks associated with it if multiple users have bookmarked it, but each bookmark is associated with only one link. as we set up the Model.py and then, run the following command manage.py syncdb in order to create its corresponding table. Let’s examine the SQL query generated by Django to see how it automaticallyhandles foreign keys. Again, issue the following command: $ python manage.py sql bookmarks And the result is : 123456789101112131415BEGIN;CREATE TABLE &quot;bookmarks_link&quot; ( &quot;id&quot; integer NOT NULL PRIMARY KEY, &quot;url&quot; varchar(200) NOT NULL UNIQUE);CREATE TABLE &quot;bookmarks_bookmark&quot; ( &quot;id&quot; integer NOT NULL PRIMARY KEY, &quot;title&quot; varchar(200) NOT NULL, &quot;user_id&quot; integer NOT NULL REFERENCES &quot;auth_user&quot; (&quot;id&quot;), &quot;link_id&quot; integer NOT NULL REFERENCES &quot;bookmarks_link&quot; (&quot;id&quot;));COMMIT; Now that the data models are ready, we have the facilities to store and manage our data. Django offers an elegant and straightforward Python API to store Python objects in the database, thus sparing the developer the burden of working with SQL and converting between SQL and Python types and idioms. TemplateThe next thing we need concern is the templates. It is a very important part in Django and all web app for python. Because handling the html code directly in python may have many disadvantages: Good software engineering practices always emphasize the separation between UI and business logic, because it enhances reusability. However, embedding HTML within Python code clearly violates this rule. Editing HTML embedded within Python requires Python knowledge, but this is impractical for many development teams whose web designers do not know Python. Handling HTML code within Python code is a tedious and error-prone task. For example, quotation marks in HTML may need to be escaped in Python string literals, and the overall result may be unclean and unreadable code. some synax for the templatesChecking whether a variable is empty or not in a template is done using the following syntax: 12345&#123;% if variable %&#125;&lt;p&gt;variable contains data.&lt;/p&gt;&#123;% else %&#125;&lt;p&gt;variable is empty&lt;/p&gt;&#123;% endif %&#125; This ‘if’ condition works as expected. If the variable contains data, only the first line is printed to the browser. On the other hand, if the variable is indeed empty, only the second line is printed. To iterate through a list and print its items, we use the following syntax: 123&#123;% for item in list %&#125;&#123;&#123; item &#125;&#125;&#123;% endfor %&#125; Finally, if a variable has attributes, you can access them in a way similar to Python: Combining Templates and ViewIf we want tansfer some variable or character from URL to the web server (eg: into View), how do we do it? For example, an URL of a User page will have such form : user/username，where usernameis the owner of the bookmarks that we want to see.where usernameis the owner of the bookmarks that we want to see. 1234urlpatterns = patterns('',(r'^$', main_page),(r'^user/(\w+)/$', user_page),) We have surrounded this portion of the regular expression with parentheses; this will cause Django to capture the string that matches this portion, and pass it to the view, as we will see later. and we modify the View.py for user_page 123456789101112131415from django.http import HttpResponse, Http404from django.contrib.auth.models import Userdef user_page(request, username):try:user = User.objects.get(username=username)except:raise Http404('Requested user not found.')bookmarks = user.bookmark_set.all()template = get_template('user_page.html')variables = Context(&#123;'username': username,'bookmarks': bookmarks&#125;)output = template.render(variables)return HttpResponse(output) There are several features need to be mentioned here: Unlike our first view, user_pagetakes an extra parameter in addition to the familiar request object. Remember that the pattern for this URL contains capturing parentheses? Strings captured by URL patterns are passed as parameters to views. The captured string in this URL is passed as the username parameter. We used User.objects.get to obtain the user object whose username is requested. We can use a similar technique to query any table by a unique column. This method throws an exception if there are no records that match the query, or if the matched record is not unique. The reason we can use User.objects.get is that we directly use default User Model to define our database table If the requested username is not available in the database, we generate a 404 “Page Not Found” error by raising an exception of the type Http404. To obtain the list of bookmarks for a particular user object, we can conveniently use the bookmark_set bookmarks = user.bookmark_set.all() attribute available in the user object. Django detects relations between data models and automatically generates such attributes. There is no need to worry about constructing SQL JOIN queries ourselves to obtain user bookmarks for example. See more detail in stack overflow user-object-has-no-attribute-bookmark-set, And Django reference under “Following relationships “backward”” User Registration and Management Creating a login page. Enabling logout functionality. Creating a registration form. Enabling users to update their account information. Django has its own Authentication module. The Django authentication system is available in the django.contrib.authpackage. It is installed by default as part of Django, and projects created withthe django-admin.pyutility have it enabled by default. Before we start using the authentication system, let’s have a quick look at the featuresthat it provides: Users: A comprehensive User data model with fields commonly required by web applications. Permissions: Yes/No flags that indicate whether a user may access a certain feature or not. Groups: A data model for grouping more than one user together and applying the same set of permissions to them. Messages: Provides the functionality for displaying information and error messages to the user. Creating the Login PageOpen the file in your editor and change it so that the URL table looks like the following snippet: 12345urlpatterns = patterns(&apos;&apos;,(r&apos;^$&apos;, main_page),(r&apos;^user/(\w+)/$&apos;, user_page),(r&apos;^login/$&apos;, &apos;django.contrib.auth.views.login&apos;),) The login view requires the availability of a template called registration/login.html. It loads this template and passes an object that represents the login form toit. We will learn about form objects in detail when we create a user registrationform, but for now, we only need to know that this object is called formand has thefollowing attributes: form.username, form.passwordand form.has_errors. Whenprinted, the first two attributes generate HTML code for the username and passwordtext fields, whereas form.has_errorsis a Boolean attribute that is set to true iflogging-in fails after submitting the form. Improving Template StructureWe see lots of templates have similar structure. Wouldn’t it be great if we could factor out the shared sections into a single file so that, if we want to modify all the pages in future, we need only edit one file? Fortunately, the Django template system already provides such a feature-template inheritance. The idea is simple; we create a base template that contains the structure shared by all templates in the system. We also declare certain blocksof the base template to be modifiable by child templates. Next, we create a template thatextendsthe base template and modifies its blocks. The idea is very similar to class inheritance in object-oriented programming. the base file:1234567891011&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot;&quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Django Bookmarks | &#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;&#123;% block head %&#125;&#123;% endblock %&#125;&lt;/h1&gt;&#123;% block content %&#125;&#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt; The template utilizes a new template tag called block. This tag is used to define sections that are modifiable by child templates. Our base template contains three blocks, one for the title, one for the page heading and one for the body. To be continued how to apply crob job to do some thing]]></content>
      <categories>
        <category>develop</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PhantomJS and Selenium -- Headless Browser Spider]]></title>
    <url>%2F2016%2F12%2F19%2FPhantomJS-and-Selenium-headless-brower-spider%2F</url>
    <content type="text"><![CDATA[This article introduce how to use PhantomJS and Selenium to do headless Browser Testing and web spider For the webspider, many times we encounter some annoying websites and found it is hard to crawler the data easily. So we need to simulate browser to do it. Selenium is a very powerful tool to help us crawling data. But Selenium also have some shortcomings, for example in linux and other cloud system, it is not easy to install a browser to do it. Another thing is that usually starting a browser is much less efficient to do the scrapy things. This article is intended to introduce PhantomJS and Selenium , which will help developer to do the browser testing quickly and web spider efficiently. intall the required softwarefirst is to install seleniumpip intall selenium for phantomJS, we can use brew or use npm (Node.js) to install:npm -g install phantomjs-prebuilt Note my node module is in “C:\Users\username\AppData\Roaming\npm\node_modules” Once we are done with this, we can use PhantomJS freely in selenium 123456789## python 3.5from selenium import webdriverdriver = webdriver.PhantomJS() ## put PhantomJS.exe in the same directory driver.set_window_size(1120, 550)driver.get("https://realpython.com/blog/python/headless-selenium-testing-with-python-and-phantomjs/")temp=driver.find_element_by_xpath("//h3/following-sibling::p") ## find the following sibling hahadriver.save_screenshot('screen.png') # save a screenshot to diskprint(temp.text)driver.quit() We can see it is headless browser and give us results directly. (personally speaking, I do not think it is very fast. It seems still spend long time) reference selenium find nex element selenium + PhantomJS source code]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>webspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Extract Text From Sanned PDF With Python]]></title>
    <url>%2F2016%2F12%2F19%2Fextract-text-from-sanned-pdf%2F</url>
    <content type="text"><![CDATA[This article introduces how to setup the denpendicies and environment for using OCR technic to extract data from scanned PDF or image. extracting normal pdf is easy and convinent, we can just use pdfminer and pdfminer.six (for python2 and python3 respectively) and follow the instruction to get text content. But for those scanned pdf, it is actually the image in essence. To extract the text from it, we need a little bit more complicated setup. In addition, it is easy for linux system but hard for windows system. Basic package and software neededWe want to use pyocr to extract what we need. And in order to use if correctly, we need the following important denpendencies Python Imaging Library (PIL) Wand tesseract-ocr ghostscript ImageMagick Note that PIL could use conda install pil. And also we need to setup the environment and path.First of all, do not change the default name of the folder, you can change the directory. But if you change the directory, you need to change some path setup from tesseract.py.py in pyocr package. For the system path and environment, you need to add the directory of ghostscript, ImageMagick, tesseract-ocr into system path: create a new name MAGICK_HOME and set ImageMagick,ghostscript as E:\system\ImageMagick-6.9.7-Q8; E:\system\gs9.20\bin add them into the path E:\system\ImageMagick-6.9.7-Q8; E:\system\gs9.20\bin create a new name TESSDATA_PREFIX and set tesseract directory E:\system\Tesseract-OCR change the tesseract.py as 123456# CHANGE THIS IF TESSERACT IS NOT IN YOUR PATH, OR IS NAMED DIFFERENTLYTESSERACT_CMD = os.environ[&quot;TESSDATA_PREFIX&quot;]+ os.sep +&apos;tesseract.exe&apos; if os.name == &apos;nt&apos; else &apos;tesseract&apos;TESSDATA_EXTENSION = &quot;.traineddata&quot;logger = logging.getLogger(__name__) when you successfully setup, you can open the cmd, and input :convert filename.pdf filename.jpgto see whether it can operate correctly. python OCR striptWhen all those are done. We are able to write the python script : importing the required libraries: 12345from wand.image import Imagefrom PIL import Image as PIimport pyocrimport pyocr.buildersimport io get the handle of the OCR library (tesseract) 12tool = pyocr.get_available_tools()[0]lang = tool.get_available_languages()[0] # you need to check what the language is in the list, in my computer it is eng for [0] If your tesseract does not setup correctly, you will encount null value in this part, please carefully check the environment path setup. setup two lists to store the images and final_text 12req_image = []final_text = [] open the PDF file using wand and convert it to jpeg 12image_pdf = Image(filename="path/filename.pdf", resolution=300)image_jpeg = image_pdf.convert('jpeg') If the ghostscript does not setup correctly, this part will raise the error, usually I encounter 798 : the system could not find the file. Here you need not only check the environment path but also do not change the folder’s name, because I change the folder’s name at the beginning, It tooks me a long time to fix this problem. wand has converted all the separate pages in the PDF into separate image blobs. We can loop over them and append them as a blob into the req_image list. 123for img in image_jpeg.sequence: img_page = Image(image=img) req_image.append(img_page.make_blob('jpeg')) run OCR to get the text 1234567for img in req_image: txt = tool.image_to_string( PI.open(io.BytesIO(img)), lang=lang, builder=pyocr.builders.TextBuilder() ) final_text.append(txt) It will take a few minuite to finsih the converting. Full codeThe Full code is 1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-python 27required package pyocr, PIL, wandfrom wand.image import Imagefrom PIL import Image as PIimport pyocrimport pyocr.buildersimport iopath = "your path directory\demo.pdf"tool = pyocr.get_available_tools()[0]lang = tool.get_available_languages()[0] // 0 is engreq_image = []final_text = []image_pdf = Image(filename=path, resolution=300)image_jpeg = image_pdf.convert('jpeg')for img in image_jpeg.sequence: img_page = Image(image=img) req_image.append(img_page.make_blob('jpeg'))for img in req_image: txt = tool.image_to_string( PI.open(io.BytesIO(img)), lang=lang, builder=pyocr.builders.TextBuilder() ) final_text.append(txt) reference OCR on PDF files using Python extracting normal PDF using pdfminer]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Visualization For GIS Info With Python]]></title>
    <url>%2F2016%2F07%2F28%2Fdata-visualization-for-GIS-info-with-python%2F</url>
    <content type="text"><![CDATA[This post breifly records my learning on how to visualiza the GIS info on the map by python. I attempted different ways to visualize my gis data by python. I was very luck to find two powerful package to deal with data visualization: vincent link folium link The two package offers different ways to visualize gis info on the map. And most importantly, it is easy to get started! what is foliumfolium is a package aimed at making beautiful maps with Leaflet.js &amp; Python. Just with a few codes, you can generate a beautiful map to visualize your data and help you doing analysis. folium supports python2 and python3. If you install the anaconda IDE environment, you won’t face any trouble to install it. the installation is simple and easy with pip: pip install folium get started to create the first simple mapwith a few code, we can get the traditional map immediately: 12345import foliummap_osm = folium.Map(location=[45.5236, -122.6750])## output to a html filemap_osm.create_map(path='osm.html') Different types of mapAs far as I know, folium has at least three types of map form (tiles), which are “default”,”Stamen Toner”,”Stamen Terrain”. In the following are the examples: 12345map_demo1 = folium.Map(location=[45.5236, -122.6750])map_demo2 = folium.Map(location=[45.5236, -122.6750], tiles='Stamen Toner', zoom_start=13)map_demo3 = folium.Map(location=[45.372, -121.6972], tiles='Stamen Terrain', zoom_start=12) custom tiles: tiles: str, default &#39;OpenStreetMap&#39; Map tileset to use. Can choose from this list of built-in tiles: - &quot;OpenStreetMap&quot; - &quot;MapQuest Open&quot; - &quot;MapQuest Open Aerial&quot; - &quot;Mapbox Bright&quot; (Limited levels of zoom for free tiles) - &quot;Mapbox Control Room&quot; (Limited levels of zoom for free tiles) - &quot;Stamen&quot; (Terrain, Toner, and Watercolor) - &quot;Cloudmade&quot; (Must pass API key) - &quot;Mapbox&quot; (Must pass API key) - &quot;CartoDB&quot; (positron and dark_matter) Of course, you can use mapbox and Leaflet.js to design and create many other types of map style (tile)Of course, you can use mapbox and Leaflet.js to design and create many other types of map style (tile)For Mapbox, it is very simple : all you need to do is set the tiles as ‘Mapbox’, and include API_key.(I don not know it quite well) Mapbox is an open source mapping platform for custom designed maps. it is very easy to use and can help designers and gis professionars to create beautiful maps instantly 1234567custom = folium.Map(location=[45.5236, -122.6750], tiles='Mapbox', API_key='wrobstory.map-12345678')tileset = r'https://api.mapbox.com/v4/xiaofeima.p37fkfk1/&#123;z&#125;/&#123;x&#125;/&#123;y&#125;.png?access_token=pk.eyJ1IjoieGlhb2ZlaW1hIiwiYSI6ImNpa2I2dTRudDBtNnV2dWtwbXh6Njg2NmcifQ.7E2eCTrl2aY5dCtTJJwXtw'custom2 = folium.Map(location=[45.372, -121.6972], zoom_start=2, tiles=tileset, attr='My Data Attribution') and with the above style, we get the following map style markers and popupsWhen we create the map style, the next thing we want to do is to mark the information on the map, folium offers us multiple methods to make marker icon and popups: 12345678910111213141516171819202122232425map_demo2 = folium.Map(location=[45.5236, -122.6750], tiles='Stamen Toner', zoom_start=13)# custom simple map_demo2.simple_marker(location=[45.5244, -122.6699], popup='The Waterfront')# draw a circlemap_demo2.circle_marker(location=[45.5215, -122.6261], radius=500, popup='Laurelhurst Park', line_color='#3186cc', fill_color='#3186cc')# Polygon markermap_demo4 = folium.Map(location=[45.5236, -122.6750], zoom_start=13)map_demo4.polygon_marker(location=[45.5012, -122.6655], popup='Ross Island Bridge', fill_color='#132b5e', num_sides=3, radius=10)map_demo4.polygon_marker(location=[45.5132, -122.6708], popup='Hawthorne Bridge', fill_color='#45647d', num_sides=4, radius=10)map_demo4.polygon_marker(location=[45.5275, -122.6692], popup='Steel Bridge', fill_color='#769d96', num_sides=6, radius=10)map_demo4.polygon_marker(location=[45.5318, -122.6745], popup='Broadway Bridge', fill_color='#769d96', num_sides=8, radius=10)# lively icon map_demo1.simple_marker([45.3288, -121.6625], popup='Mt. Hood Meadows',marker_icon='cloud')map_demo1.simple_marker([45.3311, -121.7113], popup='Timberline Lodge',marker_color='green')map_demo1.simple_marker([45.3300, -121.6823], popup='Some Other Location',marker_color='red',marker_icon='info-sign') Moreover, folium support the Vincent/Vega markers, which means we can put data and graphs into the marker 12345678910111213141516171819202122232425262728293031323334353637383940import vincent, jsonimport numpy as npscatter_points = &#123; 'x' : np.random.uniform(size=(100,)), 'y' : np.random.uniform(size=(100,)), &#125;# Let's create the vincent chart.scatter_chart = vincent.Scatter(scatter_points, iter_idx='x', width=600, height=300)# Let's convert it to JSON.scatter_json = scatter_chart.to_json()# Let's convert it to dict.scatter_dict = json.loads(scatter_json)map_vin = folium.Map([43,-100], zoom_start=4)# Let's create a Vega popup based on scatter_chart.popup = folium.Popup(max_width=800)folium.Vega(scatter_chart, height=350, width=650).add_to(popup)folium.Marker([30,-120], popup=popup).add_to(map_vin)# Let's create a Vega popup based on scatter_json.popup = folium.Popup(max_width=800)folium.Vega(scatter_json, height=350, width=650).add_to(popup)folium.Marker([30,-100], popup=popup).add_to(map_vin)# Let's create a Vega popup based on scatter_dict.popup = folium.Popup(max_width=800)folium.Vega(scatter_dict, height=350, width=650).add_to(popup) folium.Marker([30,-80], popup=popup).add_to(map_vin)map_vin In more general case, we can put html in to the popup so that we can represet anything we want : 12345678910111213141516171819m = folium.Map([43,-100], zoom_start=4)html=""" &lt;h1&gt; This is a big popup&lt;/h1&gt;&lt;br&gt; With a few lines of code... &lt;p&gt; &lt;code&gt; from numpy import *&lt;br&gt; exp(-2*pi) &lt;/code&gt; &lt;/p&gt; """iframe = folium.element.IFrame(html=html, width=500, height=300)popup = folium.Popup(iframe, max_width=2650)folium.Marker([30,-100], popup=popup).add_to(m)m the folium also supports the Choropleth Maps. But we need to offer the Geojson files to draw the map. The folium command is simple, but how to get Geojson is difficult. we need to find GIS info and switch it into geojson format. Fortunately,We find ways to get geoJson format file: source Install the Quantum GIS framework http://www.qgis.org/e/qgis. If you are on Mac OS X, you can use this version http://www.kyngchaos.com/software This will give you the ogr2ogr utility used for converting shapefiles to geoJSON Download the shapefiles for your country from here http://www.gadm.org/country and unzip For Canada, and possibly other countries, the shapefile with suffix 0 is for the country boundary and the suffix 1 is for the internal regions. Not sure if this naming is consistent across countries. Upload the region level shapefile to MapShaper http://mapshaper.com/test/MapShaper.swf or http://www.mapshaper.org. You can skip this step if you don’t care to optimize the size of your resulting geoJSON Set the ‘simplification level’ slider in MapShaper to the desired level and export the simplified shapefile as ‘Shapefile - Polygons’ Download .shp and .shx file to the local directory where you unzipped the original shapefiiles, replace the original files with the simplified ones. Navigate to the local directory and run the command below, replacing with the actual name of the shapefile you want to convert.ogr2ogr -f geoJSON regions.json &lt;shapefile&gt;.shp You should now have the regions for your country in geoJSON format. Check to make sure there are paths defined in regions.json and that property fields were maintained (ex. region name). Then we should have the Choropleth maps: 12345678910111213china_geo=r'geojson/chn_adm1_2.json'csv_data = r'data/CN-gdp-2014.csv'state_data = pd.read_csv(csv_data,encoding='gbk')map_ch = folium.Map(location=[50, 120], zoom_start=3)map_ch.geo_json(geo_path=china_geo, data=state_data, columns=['province', 'GDP-2014'], #threshold_scale=[6, 7, 8, 9, 10,11,12,13], key_on='feature.properties.HASC_1', fill_color='BuPu', fill_opacity=0.7, line_opacity=0.5, legend_name='GDP Rate (%)',reset=True)map_ch.create_map('cn_states.html') download the source code word_cloudipynb to md]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>GIS</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas Techniques]]></title>
    <url>%2F2016%2F05%2F07%2Fpandas-snippets%2F</url>
    <content type="text"><![CDATA[Python Pandas Data Analysis Sourcesource from pandas snippets the following is cited from online sources123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#List unique values in a DataFrame columnpd.unique(df.column_name.ravel())#Convert Series datatype to numeric, getting rid of any non-numeric valuesdf['col'] = df['col'].astype(str).convert_objects(convert_numeric=True)#Grab DataFrame rows where column has certain valuesvaluelist = ['value1', 'value2', 'value3']df = df[df.column.isin(value_list)]#Grab DataFrame rows where column doesn't have certain valuesvaluelist = ['value1', 'value2', 'value3']df = df[~df.column.isin(value_list)]#Delete column from DataFramedel df['column']#Select from DataFrame using criteria from multiple columnsnewdf = df[(df['column_one']&gt;2004) &amp; (df['column_two']==9)]#Rename several DataFrame columnsdf = df.rename(columns = &#123; 'col1 old name':'col1 new name', 'col2 old name':'col2 new name', 'col3 old name':'col3 new name',&#125;)#lower-case all DataFrame column namesdf.columns = map(str.lower, df.columns)#even more fancy DataFrame column re-naming#lower-case all DataFrame column names (for example)df.rename(columns=lambda x: x.split('.')[-1], inplace=True)#Loop through rows in a DataFrame#(if you must)for index, row in df.iterrows(): print index, row['some column'] #Next few examples show how to work with text data in Pandas.#Full list of .str functions: http://pandas.pydata.org/pandas-docs/stable/text.html#Slice values in a DataFrame column (aka Series)df.column.str[0:2]#Lower-case everything in a DataFrame columndf.column_name = df.column_name.str.lower()#Get length of data in a DataFrame columndf.column_name.str.len()#Sort dataframe by multiple columnsdf = df.sort(['col1','col2','col3'],ascending=[1,1,0])#get top n for each group of columns in a sorted dataframe#(make sure dataframe is sorted first)top5 = df.groupby(['groupingcol1', 'groupingcol2']).head(5)#Grab DataFrame rows where specific column is null/notnullnewdf = df[df['column'].isnull()]#select from DataFrame using multiple keys of a hierarchical indexdf.xs(('index level 1 value','index level 2 value'), level=('level 1','level 2'))#Change all NaNs to None (useful before#loading to a db)df = df.where((pd.notnull(df)), None)#Get quick count of rows in a DataFramelen(df.index)#Pivot data (with flexibility about what what#becomes a column and what stays a row).#Syntax works on Pandas &gt;= .14pd.pivot_table( df,values='cell_value', index=['col1', 'col2', 'col3'], #these stay as columns columns=['col4']) #data values in this column become their own column#change data type of DataFrame columndf.column_name = df.column_name.astype(np.int64)# Get rid of non-numeric values throughout a DataFrame:for col in refunds.columns.values: refunds[col] = refunds[col].replace('[^0-9]+.-', '', regex=True)#Set DataFrame column values based on other column valuesdf['column_to_change'][(df['column1'] == some_value) &amp; (df['column2'] == some_other_value)] = new_value#Clean up missing values in multiple DataFrame columnsdf = df.fillna(&#123; 'col1': 'missing', 'col2': '99.999', 'col3': '999', 'col4': 'missing', 'col5': 'missing', 'col6': '99'&#125;)#Concatenate two DataFrame columns into a new, single column#(useful when dealing with composite keys, for example)df['newcol'] = df['col1'].map(str) + df['col2'].map(str)#Doing calculations with DataFrame columns that have missing values#In example below, swap in 0 for df['col1'] cells that contain nulldf['new_col'] = np.where(pd.isnull(df['col1']),0,df['col1']) + df['col2']# Split delimited values in a DataFrame column into two new columnsdf['new_col1'], df['new_col2'] = zip(*df['original_col'].apply(lambda x: x.split(': ', 1)))# Collapse hierarchical column indexesdf.columns = df.columns.get_level_values(0)#Convert Django queryset to DataFrameqs = DjangoModelName.objects.all()q = qs.values()df = pd.DataFrame.from_records(q)#Create a DataFrame from a Python dictionarydf = pd.DataFrame(list(a_dictionary.items()), columns = ['column1', 'column2'])# create panel data concated = pd.concat(list_of_frames)items = ['age', 'weight', 'score']pd.Panel(dict(zip(items, [concated.pivot(index='date', columns='id', values=i) for i in items])))]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Marginal Treatment Effects Introduction]]></title>
    <url>%2F2016%2F04%2F27%2FMTE%20intro%2F</url>
    <content type="text"><![CDATA[Motivationtraditional different evaluation parameters are an average over parts of the distribution of impacts: The ATE averages over the entire distribution The ATT averages over the distribution of impacts for those who are somehow allocated into treatment LATE averages over the distribution of impacts for those who switch into treatment as a result of a reform or more precisely, as a result of a change of the value of some instrument a§ecting decisions to participate. shortcomings: represent an aggregation over di§erent margins not comparable and they are di¢ cult to interpret from the perspective of general Heckman and Vytlacil (2005) defined the MARGINAL TREATMENT EFFECT MTE unifying those treatment parameters MTE is the effect of a treatment on the marginal individual entering treatment, focusing on the specified individual who is indifferent between entering the policy(treatment) or not. (in average measure) DefinitionConsider a discrete treatment T. The rule allocating to treatment may be written as: T = 1({v_i} \le {Z_i}^{'}\gamma )For a particular value of ${Z_i}^{‘}\gamma$ the marginal individual is the one with {v_i} = {Z_i}^{'}\gammaNow consider the e§ect of treatment for the ith individual (of course the ith individual can not have both $Y_i^1$ and $Y_i^0$ the discussion see the next subsection) \beta_{i}=Y_{i}^{1}-Y_{i}^{0}Then the marginal treatment effect can be defined by : MTE(Z_{i}^{'}\gamma)=E(\beta_{i}|v_{i}=Z_{i}^{'}\gamma)MTE has a very important property that all treatment parameters, such as ATE, ATT, AUT, can be written as weighted averages of MTE Heckman definationIt is easy to understand if we can start from basic notation. For policy analysis, The generalized Roy Model is a basic choice-theoretic framework. Take the attending college for example. Let $Y_1$ be the potential return (log wage) if the individual were in the treatment group( were to attend college) Let $Y_0$ be the potential return (log wage) if the individual were in the control group( were not to attend college) We have potential outcomes as : \begin{matrix} Y_{1} &= & \mu_{1}(X)+U_{1} \\ Y_{0} &= & \mu_{0}(X)+U_{0} \\ \end{matrix}where $\mu_{1}(X)=E(Y_{1}|X=x)$ and $\mu_{0}(X)=E(Y_{0}|X=x)$. The policy effect (return to schooling ) is $Y_{1} -Y_{0} = \beta = \mu_{1}(X)+U_{1} - \mu_{0}(X)+U_{0}$. Then average treatment effect conditional on $X=x$ is \bar{\beta}=E(\beta|X=x)=\mu_{1}(x)-\mu_{0}(x)and average effect of treatment (those who choose to attend college conditional on $X=x$) is E(\beta|X=x,\ S=1)=\bar{\beta}+E(U_{1}-U_{0}|X=x,\ S=1)Next, we utlize the latent variable discrete choice model to represents the individual’s decision (such as attending school) Causal Inference ProblemSuppose a policy $D$ (which is dichotomous in simplicity) affect the population group $U$. $D=1$ if a memebr is treated $D=0$ if a member is not treated for ith member in U, we denote $Y_{i}^{1}$ as ith member potential outcome if treated ($d_i=1$). similiarly, $Y_{i}^{0}$ as ith member potential outcome if untreated ($d_i=0$) \beta_{i}=Y_{i}^{1}-Y_{i}^{0}$\beta_{i}$ represent hypothetical treatment effect for ith member. But the problem is that for a given individual i, we can only observe either $Y_{i}^{1}$ ($d_i=1$) or $Y_{i}^{0}$ ($d_i=0$), but not both. 如果个体i参加了项目($d_i=1$)，观察到$Y_{i}^{1}$，但看不到这个人的$Y_{i}^{0}$。除非可以把这个人送回“过去”，改写历史不让他参加项目($d_i=0$)，记录其$Y_{i}^{0}$。简单来说个体只能处于一种状态 Y_{i}=(1-d_i) Y_{i}^{0}+d_i Y_{i}^{1}= Y_{i}^{0}+ ( Y_{i}^{1}-Y_{i}^{0})d_iregarding the fundamental problem, how can we estimate treatment effect? Holland describes two possible solutions: the “scientific solution” and the “statistical solution.” “scientific solution” : capitalizes on homogeneity in assuming that all members in U are the same, in either the treated state or the control state. So to estimate the effect we can just use two members (one in treatment group and the other one in untreatment group). this solution has little practical values “statistical solution” : compute quantities of interest that reveal treatment effects only at the group level, which induce the ATE ATT ATUT method. $ATE=E(Y_{i}^{1}-Y_{i}^{0})$ $ATT=E(Y_{i}^{1}-Y_{i}^{0} | D=1)$ $ATUT=E(Y_{i}^{1}-Y_{i}^{0} | D=0)$ Building MTE with more detailed justificationHere I borrow the case from Costas Meghir notes to illusatrate how to build MTE Reference and ResourcesCostas Meghir lecture5MTE estimation guide]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>econometrics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python For @static Method]]></title>
    <url>%2F2016%2F04%2F11%2Fpython-function-decorated%2F</url>
    <content type="text"><![CDATA[What is the difference between a function decorated with @staticmethod and one decorated with @classmethod? the answer is copyed from stack overflow Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:12345678910111213class A(object): def foo(self,x): print &quot;executing foo(%s,%s)&quot;%(self,x) @classmethod def class_foo(cls,x): print &quot;executing class_foo(%s,%s)&quot;%(cls,x) @staticmethod def static_foo(x): print &quot;executing static_foo(%s)&quot;%x a=A() Below is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.12a.foo(1)# executing foo(&lt;__main__.A object at 0xb7dbef0c&gt;,1) With classmethods, the class of the object instance is implicitly passed as the first argument instead of self.12a.class_foo(1)# executing class_foo(&lt;class &apos;__main__.A&apos;&gt;,1) You can also call class_foo using the class. In fact, if you define something to be a classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine: 12A.class_foo(1)# executing class_foo(&lt;class &apos;__main__.A&apos;&gt;,1) @proerty 用法： 在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，导致可以把成绩随便改：12s = Student()s.score = 9999 这显然不合逻辑。为了限制score的范围，可以通过一个set_score()方法来设置成绩，再通过一个get_score()来获取成绩，这样，在set_score()方法里，就可以检查参数：1234567891011class Student(object): def get_score(self): return self._score def set_score(self, value): if not isinstance(value, int): raise ValueError(&apos;score must be an integer!&apos;) if value &lt; 0 or value &gt; 100: raise ValueError(&apos;score must between 0 ~ 100!&apos;) self._score = value\ 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.set_score(60) # ok!&gt;&gt;&gt; s.get_score()60&gt;&gt;&gt; s.set_score(9999)Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？对于追求完美的Python程序员来说，这是必须要做到的！ 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的：12345678910111213class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(&apos;score must be an integer!&apos;) if value &lt; 0 or value &gt; 100: raise ValueError(&apos;score must between 0 ~ 100!&apos;) self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60&gt;&gt;&gt; s.score = 9999Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： 12345678910111213class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2014 - self._birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas Data Analysis]]></title>
    <url>%2F2016%2F03%2F21%2Fpython%20data%20analysis%2F</url>
    <content type="text"><![CDATA[安装 pip123456later use utf-8-sig instead of utf-8pip install --target=d:\somewhere\other\than\the\default package_name带空格处理pip install --target=X:\My~Documents\py_pkg\ selenium 进行分组统计123456789101112131415161718192021222324252627import pandas as pddf = pd.DataFrame(&#123;'key1':['a', 'a', 'b', 'b', 'a'], 'key2':['one', 'two', 'one', 'two', 'one'], 'data1':np.random.randn(5), 'data2':np.random.randn(5)&#125;)df data1 data2 key1 key20 -0.410673 0.519378 a one1 -2.120793 0.199074 a two2 0.642216 -0.143671 b one3 0.975133 -0.592994 b two4 -1.017495 -0.530459 a onegrouped = df['data1'].groupby(df['key1'])grouped.mean()key1a -1.182987b 0.808674dtype: float64means = df['data1'].groupby([df['key1'], df['key2']]).mean()means.unstack()key2 one twokey1 a -0.714084 -2.120793b 0.642216 0.975133 选取两个dataframe中列公共的元素有df1.ID, df2.ID 两列，我需要选取两列中都有的元素。可以用如下方法： 1234567import pandas as pdm_df=pd.merge(df1,df2,how="inner",on="ID")ind=m_df.IDdf1.set_index('ID',inplace=True)df2.set_index('ID',inplace=True)common_df1=df1.loc[ind,:]common_df2=df2.loc[ind,:] 改变pandas 列的类型把数值型的改变成string 1234567import pandas as pddf = pd.DataFrame(&#123;'key1':['a', 'a', 'b', 'b', 'a'], 'key2':['one', 'two', 'one', 'two', 'one'], 'data1':np.random.randn(5), 'data2':np.random.randn(5)&#125;) df["data1"]=df["data1"].astype(str) 寻找缺失值或者NaN值所带的位置12345678910111213141516171819202122 a b2011-01-01 00:00:00 1.883381 -0.4166292011-01-01 01:00:00 0.149948 -1.7821702011-01-01 02:00:00 -0.407604 0.3141682011-01-01 03:00:00 1.452354 NaN2011-01-01 04:00:00 -1.224869 -0.9474572011-01-01 05:00:00 0.498326 0.0704162011-01-01 06:00:00 0.401665 NaN2011-01-01 07:00:00 -0.019766 0.5336412011-01-01 08:00:00 -1.101303 -1.4085612011-01-01 09:00:00 1.671795 -0.764629import numpy as npindex = df['b'].index[df['b'].apply(np.isnan)]df['a'].ix[index[0]]&gt;&gt;&gt; 1.452354df_index = df.index.values.tolist()[df_index.index(i) for i in index]&gt;&gt;&gt; [3, 6] iloc 与 loc 区别 at 与 iat 区别 ixdataframe.iloc按下标选取，或者使用dataframe.loc按索引选取： 如果不是需要访问特定行列，而只是某个特殊位置的元素的话，dataframe.at和dataframe.iat是最快的方式，它们分别用于使用索引和下标进行访问： ataframe.ix可以混合使用索引和下标进行访问，唯一需要注意的地方是行列内部需要一致，不可以同时使用索引和标签访问行或者列，不然的话，将会得到意外的结果。 pandas 处理缺失数据原始数据的中很可能存在一些数据的缺失，就如同现在处理的这个样例数据一样，处理缺失数据有多种方式。通常使用dataframe.dropna()，dataframe.dropna()可以按行丢弃带有nan的数据；若指定how=’all’（默认是’any’），则只在整行全部是nan时丢弃数据；若指定thresh，则表示当某行数据非缺失列数超过指定数值时才保留；要指定根据某列丢弃可以通过subset完成。 12345678910111213141516171819202122print "Data size before filtering:"print df.shapeprint "Drop all rows that have any NaN values:"print "Data size after filtering:"print df.dropna().shapeprint df.dropna().head(10)print "Drop only if all columns are NaN:"print "Data size after filtering:"print df.dropna(how='all').shapeprint df.dropna(how='all').head(10)print "Drop rows who do not have at least six values that are not NaN"print "Data size after filtering:"print df.dropna(thresh=6).shapeprint df.dropna(thresh=6).head(10)print "Drop only if NaN in specific column:"print "Data size after filtering:"print df.dropna(subset=['closePrice']).shapeprint df.dropna(subset=['closePrice']).head(10)]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab Technique]]></title>
    <url>%2F2016%2F03%2F21%2Fmatlab-technique%2F</url>
    <content type="text"><![CDATA[Here I summarize some matlab trips to facilate my research 将 excel 中的日期转化为matlab中的日期(char) 在EXCEL中设置日期变量的单元格为“常规格式”； 复制此变化到EXCEL中，生成一个数组A 采用 x2mdate命令将其转化为MATLAB中的数据B 采用datestr命令将其转化为MATLAB中特有的日期格式 matlab arry tricksmatlab array tricks]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database Basic]]></title>
    <url>%2F2016%2F03%2F21%2Fdatabase-basic%2F</url>
    <content type="text"><![CDATA[With the help of Navicat for Mysql, we can easily transform different data format into mysql. And when we install MySQL Workbench, it helps us to deal with any sql operation easily and conviently. get column name from tablelite1select column_name from information_schema.COLUMNS where table_name='table name' 修改表名：alter table t_book rename to newname; 添加列：alter table 表名 add column 列名 varchar(30); 删除列：alter table 表名 drop column 列名; 修改列名MySQL： alter table bbb change oldname newname type; python and mysqlpython 3 利用 Pymysql 包来进行 123456789101112131415161718import pymysql.cursors# Connect to the databaseconnection = pymysql.connect(host='localhost', user='root', password='passcode', db='database name', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)mysql_cursor = connection.cursor()sql_col_property="show columns from table_name"mysql_cursor.execute(sql_col_property)connection.commit()connection.close() 利用列的位置选取数据库中的列首先获取想要选取位置的列名 lite1SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'your database schema' AND TABLE_NAME = 'the wanted table name' AND ORDINAL_POSITION = 2; 利用获取到的列名提取列* lite1SELECT SPECIFICED_NAME FROM TABLE_NAME 选取列名中包含特定字符lite1select * from INFORMATION_SCHEMA.COLUMNS where COLUMN_NAME like '%control%' order by TABLE_NAME 更新数据库中的数据1234567891011121314151617181920212223242526272829303132333435from mysql.connector import MySQLConnection, Errorfrom python_mysql_dbconfig import read_db_config def update_book(book_id, title): # read database configuration db_config = read_db_config() # prepare query and data query = """ UPDATE books SET title = %s WHERE id = %s """ data = (title, book_id) try: conn = MySQLConnection(**db_config) # update book title cursor = conn.cursor() cursor.execute(query, data) # accept the changes conn.commit() except Error as error: print(error) finally: cursor.close() conn.close() if __name__ == '__main__': update_book(37, 'The Giant on the Hill *** TEST ***')]]></content>
      <categories>
        <category>develop</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Data Visualization]]></title>
    <url>%2F2016%2F03%2F21%2Fpython-data-visualization%2F</url>
    <content type="text"><![CDATA[python 上面有不少数据可视化的工具包，先利用这篇文章总结如下： ggplot Seaborn Bokeh Pygal python-igraph folium Network XMayavi VisPy PyQtGraph vincent Plotly seaborn是偏向于统计作图的，尤其是线性作图，用起来比较顺手，简单。seaborn整个语法层也会简化很多，画出的图不需要修饰看起来也很好看。但是绘图方式有限，不够灵活 bokeh是使用了js。因此主打的是交互式绘图，你可以在Ipython notebook里使用到最佳！画出的图非常好看，关键是可以交互修改！ 缺点是语法有点生涩，一点也不必matplotlib简单 ggplot就算了吧，和R语言那个GGPLOT2比起来，简直是感觉在用两个包，似然都是同一个人开发的！ 而且原作者也在GITHUB上说了，不再会更新PYTHON的库！ 不过话说，ggplot2真的是绘图神器，这几乎是我还在用R语言的唯一原因。 python for map data visualization folium no doubt javascript library : http://kartograph.org/dynamic visualization : https://github.com/areski/python-nvd3https://github.com/wrobstory/bearcart 以下是folium 作者的回复Vincent is really meant for static visualization, unfortunately. If you’re interested in dynamic vis, take a look at python NVD3: https://github.com/areski/python-nvd3 or my other lib, Bearcart: https://github.com/wrobstory/bearcart I might put in a little work this weekend to integrate it more tightly with the ipython notebook. If you’re looking for interactive D3 libs that are easy to work with, I highly recommend Dimple as well: http://dimplejs.org/ matplotlib draw animate graph12345678910import numpy as npimport matplotlib.pyplot as pltplt.axis([0, 100, 0, 1])plt.ion()for i in range(100): y = np.random.random() plt.scatter(i, y) plt.pause(0.1) Source for map visualizationhow to get geoJson Datafrom shapefile to geoJsonlets make a mapgadm world shapefile download mapshapertopoJson]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Network]]></title>
    <url>%2F2016%2F03%2F21%2Fnueral_network1%2F</url>
    <content type="text"><![CDATA[Neural Network TutorialThis note introduce the basic concept of neural network in machine learning, plus the python realization. this note is intended to tell beginners the cornerstone of the neural network and help anyone who is interested in neural network can build their own application easily. The notes contains the following part: Toy example of Neural Network The Defination of Neural Network Bsic element in Neural Network and Backprogation constuct the Neural Network Toy exampleFirst CaseThe example comes from @iamtrask, link blog the toy example is neural network trained with backpropagation is attempting to use input to predict output. Inputs Inputs Inputs Output 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 we would see that the leftmost input column is perfectly correlated with the output. Backpropagation, in its simplest form. Variable Definition X Input dataset matrix where each row is a training example y Output dataset matrix where each row is a training example l0 First Layer of the Network, specified by the input data l1 Second Layer of the Network, otherwise known as the hidden layer syn0 First layer of weights, Synapse 0, connecting l0 to l1. * Elementwise multiplication, so two vectors of equal size are multiplying corresponding values 1-to-1 to generate a final vector of identical size. - Elementwise subtraction, so two vectors of equal size are subtracting corresponding values 1-to-1 to generate a final vector of identical size. x.dot(y) If x and y are vectors, this is a dot product. If both are matrices, it’s a matrix-matrix multiplication. If only one is a matrix, then it’s vector matrix multiplication. 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# sigmoid functiondef nonlin(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) # input datasetX = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) # output dataset y = np.array([[0,0,1,1]]).T# seed random numbers to make calculation# deterministic (just a good practice)np.random.seed(1)# initialize weights randomly with mean 0syn0 = 2*np.random.random((3,1)) - 1for iter in range(1,10000): # forward propagation l0 = X l1 = nonlin(np.dot(l0,syn0)) # how much did we miss? l1_error = y - l1 # multiply how much we missed by the # slope of the sigmoid at the values in l1 l1_delta = l1_error * nonlin(l1,True) # update weights syn0 += np.dot(l0.T,l1_delta)print("Output After Training:")print(l1) Output After Training: [[ 0.00966498] [ 0.00786546] [ 0.99358866] [ 0.99211917]] A sigmoid function maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks.A sigmoid function maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks. A Harder Problem XORThe problem can be described in the following table : Inputs Inputs Inputs Output 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 The realized code is in the following and the definations of each variables are : Inputs (l0) Inputs (l0) Inputs (l0) Hidden Weights (l1) Hidden Weights (l1) Hidden Weights (l1) Hidden Weights (l1) Output (l2) 0 0 1 0.1 0.2 0.5 0.2 0 0 1 1 0.2 0.6 0.7 0.1 1 1 0 1 0.3 0.2 0.3 0.9 1 1 1 1 0.2 0.1 0.3 0.8 0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npdef nonlin(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]]) y = np.array([[0], [1], [1], [0]])np.random.seed(1)# randomly initialize our weights with mean 0syn0 = 2*np.random.random((3,4)) - 1syn1 = 2*np.random.random((4,1)) - 1for j in range(1,60001): # Feed forward through layers 0, 1, and 2 l0 = X l1 = nonlin(np.dot(l0,syn0)) l2 = nonlin(np.dot(l1,syn1)) # how much did we miss the target value? l2_error = y - l2 if (j% 10000) == 0: print("Error:" + str(np.mean(np.abs(l2_error)))) # in what direction is the target value? # were we really sure? if so, don't change too much. # l2_delta means how the l2 layer weight (syn1) affect the estimated error change l2_delta = l2_error*nonlin(l2,deriv=True) # how much did each l1 value contribute to the l2 error (according to the weights)? # this is actually backprogation chain rule of l2_error . # for l1 value x to measure 1 unit change to the l2_error , we need the derivates of f(wx),right?! is f(wx)*w, # so to know the l1_error, we have 1*w*f(wx)*l2_error l1_error = l2_delta.dot(syn1.T) # in what direction is the target l1? # were we really sure? if so, don't change too much. # l1_delta means how the l1 layer weight (syn0) affect the estimated error change l1_delta = l1_error * nonlin(l1,deriv=True) syn1 += l1.T.dot(l2_delta) syn0 += l0.T.dot(l1_delta) Error:0.0085850250975 Error:0.00578962106435 Error:0.00462926115218 Error:0.0039588188244 Error:0.0035101602766 Error:0.00318353073559 decode the above program The above progam is acutally three nodes for input layer, four nodes for hidden layer (only 1 hidden layer) and 1 node output layer. we have four observational data. the number of weights (Synapses) need to calcualte is 3*4 + 1*4. For fiting the data, the code use traditional backprogation method to update the weightings and raise the prediction precise. When the code calculate the result, it needs to know the error to the training data. l2_error = y - l2 Then, based on the error distance, we can make the correction. Firstly, the last node, we want to know how to update the weights in the last layer. this is l2_delta = l2_error*nonlin(l2,deriv=True). After that, we need to know the how the error contribution contributed to the weights in the upper layer. Then, we need the backprogation method to calculate the extent of one unit change of the weights in the upper layer to final error. This can use derivates to express, error \times f(wx)'\times (wx)' So we have l1_error = l2_delta.dot(syn1.T). In addition, we have l1_delta = l1_error * nonlin(l1,deriv=True) The l1_delta and l1_delta indicate the update coefficients that weights should concern, multiplied by input value we can get the update amount of different weights in the Neural Network, which is the last two lines: syn1 += l1.T.dot(l2_delta) syn0 += l0.T.dot(l1_delta) The Defination of Neural NetworkBiological motivationthe online lecture notes have very clearly introduce to the neural network .(see note1)Here I briefly mention the basic ideas of eural network. The neural Network mimick human brain neurons performance. different neurons connect with each other by synapses, and receives the input signals from dendrites. In the nucleus, it applies activation function to produce the output singals, and then send the singal to the next neurons through its axon and synapses The above description illustrate the functioning procedures of the very basic neural network. A cartoon drawing of a biological neuron (left) and its mathematical model (right). Neural Network architecturesInstead of an amorphous blobs of connected neurons, Neural Network models are often organized into distinct layers of neurons. Left: A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. Right: A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Notice that in both cases there are connections (synapses) between neurons across layers, but not within a layer. Self UnderstandingAs far as I learned in neural network, it can be seen as weighting different analysis core to produce the final judgement. for example, in one specific node at jth layer, it can receive all or part of input signal from upper layer. when it receive those signals, there are many ways to organzie them. the simplest way is liner form, multiplying them with weightings (synapses)(eg $w1x1+w2x2+w3x3$). But we could also apply other forms such as polynomial, quardatic form, max, … And when we do like this, we need to remember that we need to output singal. So we need to apply *activation function (usually sigmoid) to swith the numerical result into the singal. By combing many nodes and many layers, we can deal with many complex issues and get the desired results. I think this is the power of Neural Network. In Big Data era, Neural Network must have wider and wider applications. reference http://karpathy.github.io/neuralnets/ http://cs231n.github.io/ https://iamtrask.github.io/2015/07/12/basic-python-network/]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summary of Economics Notes]]></title>
    <url>%2F2016%2F01%2F22%2Fsummary-of-economics-notes%2F</url>
    <content type="text"><![CDATA[This is a few summary of my economics studying notes in my graduate and undergraduate]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Selenium Webdriver Api]]></title>
    <url>%2F2016%2F01%2F19%2Fselenium-webdriver-api%2F</url>
    <content type="text"><![CDATA[In this article, I sumamrize some selenium webdriver api command to help coding webspider. locating elementselenium webdriver offer several ways to locate the elements : id name class name tag name link text partial link text xpath css selector coressponding to the following : find_element_by_id() find_element_by_name() find_element_by_class_name() find_element_by_tag_name() find_element_by_link_text() find_element_by_partial_link_text() find_element_by_xpath() find_element_by_css_selector() following the above locating element, we have four most commonly used operations: clear clear the content, if possible send_keys simulate the keyboard input operation. But if we want to input chinese chacarters, usually we declare “utf-8” format at the beginning and use send_keys(u”中文内容”) click simulate the mouse click operation, can click any text/image link, button, option … submit submit a form, but the object should be a form for example, the following is a typical case for user to login a website: 123456driver.find_element_by_id(“username").clear()driver.find_element_by_id(" username ").send_keys("username")driver.find_element_by_id(“password").clear()driver.find_element_by_id(" password ").send_keys("password")driver.find_element_by_id("loginBtn").click() please note that if we use find_elements_by… , we get a group of info which has the key word info. Then we can use loop to select a group of elements that we want. keyboard operationmost commonly used keyboard function key : 12345678910send_keys(Keys.BACK_SPACE) 删除键（BackSpace）send_keys(Keys.SPACE) 空格键(Space)send_keys(Keys.TAB) 制表键(Tab)send_keys(Keys.ESCAPE) 回退键（Esc）send_keys(Keys.ENTER) 回车键（Enter）send_keys(Keys.CONTROL,'a') 全选（Ctrl+A）send_keys(Keys.CONTROL,'c') 复制（Ctrl+C）send_keys(Keys.CONTROL,'x') 剪切（Ctrl+X）send_keys(Keys.CONTROL,'v') 粘贴（Ctrl+V） we can also use webdriver to get page information, such as title, url link, text content. For example: 1234567891011#get the titletitle = driver.titleprint(title)#get page current URLnow_url = driver.current_urlprint(now_url)# get "userid" text content now_user=driver.find_element_by_id(“userid").textprint(now_user) multiple windows operationIf we need to switch to multiple windows in a single webpage, we can use current_window_handle, window_handles,switch_to_window() command to operate. javascript operationgenerally, we can use driver.execute_script() to apply javascript command scrollbar operation, sometimes (sign up for a website ), we need to control the scrollbar1234567891011code for scroll：&lt;body onload= "document.body.scrollTop=0 "&gt;&lt;body onload= "document.body.scrollTop=100000 "&gt;#将页面滚动条拖到底部js = "var q=document.documentElement.scrollTop=10000"driver.execute_script(js)time.sleep(3)#将滚动条移动到页面的顶部js_ = "var q=document.documentElement.scrollTop=0"driver.execute_script(js_) cookie operation12345678910111213141516171819# get cookie infocookie = driver.get_cookies()# printprint(cookie)driver = webdriver.Firefox()driver.get("http://www.youdao.com")#add name and value into cookiedriver.add_cookie(&#123;'name':'key-aaaaaaa', 'value':'value-bbbb'&#125;)#loop the name and value and other info in cookiesfor cookie in driver.get_cookies():print "%s -&gt; %s" % (cookie['name'], cookie['value'])##### we have two ways to delete cookie ###### delete specific cookiedriver.delete_cookie("CookieName")# delete all cookiesdriver.delete_all_cookies()time.sleep(2)]]></content>
      <categories>
        <category>develop</category>
      </categories>
      <tags>
        <tag>webspider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Economics Research]]></title>
    <url>%2F2015%2F12%2F30%2Fecon-research-summary%2F</url>
    <content type="text"><![CDATA[ECONOMICSProduct design improvement for Long-Term Care Insurance Market PDFAbstract: Using a modified Brown and Finkelstein (2008) model of long-term care insurance purchase decisions, we evaluate catastrophic long-term care insurance policies that cover the tail risk of long-term care costs at affordable premiums. Under our baseline model, we show theoretically that introducing catastrophic policies will induce 11 percent of middle-income men and 3 percent of middle-income women to initiate insurance coverage. As a result, Medicaid costs will be reduced by 0.20 percent and 0.19 percent for men and women, respectively. Home Security or Social Security, Which one is better for Long Term Growth ?Abstract: China has long history of family-based support and security, functioning part of intergeneration welfare transfer as social security system. Considering the pros and cons in both family and social security systems, it is crucial to design the optimal security system to improve human capital accumulation and guarantee long term economics growth. On the other side, education investment and population policy also alter distribution of human capital accumulation. Through three period OLG model, this paper finds that given current education and population policy, the performance of different policies depends on the critical parameters. Applying numerical estimation simulation, I find that pay as you go system plus family-based security can obtain optimal balance growth path. But it increases risks on fiscal budget.]]></content>
      <categories>
        <category>research</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Science Studying]]></title>
    <url>%2F2015%2F12%2F30%2Fdatascience%2F</url>
    <content type="text"><![CDATA[数据科学学习指导https://www.zhihu.com/question/20176089想做数据处理尤其是大数据量处理的相关工作必须兼具 计算机科学基础和统计基础。强烈推荐：Distance Education § Harvard University Extension School 和哈佛的学生一起学习Data Science。 网络课程同样有丰富的资源：机器学习类：斯坦福大学：机器学习 courseraLearning From Datahttp://dataunion.org/19321.html python 机器学习系统数据分析类：约翰霍普金斯： Data Analysis Methods杜克： Data Analysis and Statistical Inference约翰霍普金斯： Computing for Data AnalysisMIT： The Analytics Edge 编程类：莱斯大学： Introduction to Interactive Programming in PythonMIT： Introduction to Computer Science &amp; Programming in Python相关问题： Data Science: What are some good free resources to learn data science?Where can I learn pandas or numpy for data analysis?What are some good resources for learning about statistical analysis?Data Science: How do I become a data scientist?What are some good “toy problems” in data science?What are some good resources for learning about machine learning?Harvard CS109http://cs109.github.io/2015/index.html http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml https://www.quora.com/How-can-I-become-a-data-scientist-1?redirected_qid=59455]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Public Economics Introduction]]></title>
    <url>%2F2015%2F12%2F25%2FPublic-Economics-Intro-and-Ideas%2F</url>
    <content type="text"><![CDATA[Public Economicsbroder definitionPublic economics (or economics of the public sector) is the study of government policy through the lens of economic efficiency and equity. At its most basic level, public economics provides a framework for thinking about whether or not the government should participate in economics markets and to what extent its role should be. In order to do so, microeconomic theory is utilized to assess whether the private market is likely to provide efficient outcomes in the absence of governmental interference. Inherently, this study involves the analysis of government taxation and expenditures. This subject encompasses a host of topics including market failures, externalities, and the creation and implementation of government policy. Public economics builds on the theory of welfare economics and is ultimately used as a tool to improve social welfare. Broad methods and topics include: the theory and application of public finance analysis and design of public policy distributional effects of taxation and government expenditures analysis of market failure and government failure. Emphasis is on analytical and scientific methods and normative-ethical analysis, as distinguished from ideology. Examples of topics covered are tax incidence optimal taxation and the theory of public goods What is Public Economics?Public economics focuses on answering two types of questions How do government policies a搂ect the economy? How should policies be designed to maximize welfare? Motivation 1: Practical Relevance Interest in improving economic welfare -&gt; interest in public economics Almost every economic intervention occurs through government policy Price intervention: taxes, welfare, social insurance, public goods Regulation: min wages, FDA regulations (25% of products consumed),zoning, labor laws, min education laws, environment Government directly employs one sixth of U.S. workforce Contentious debate on the appropriate role of government in society Stakes are extremely large because of broad scope of policies Motivation 2: Academic Interest Macro, development, labor, and corporate 脰nance questions often ultimately motivated by a public economics question Natural to combine public 脰nance with another field Understanding public 脰nance can help ensure that you work on relevant topics Motivation 3: Methodology Public economics is at the frontier of a methodological transformation in applied microeconomics Data-driven approach to answering important policy questions Combines a broad set of skills: applied theory, applied econometrics, simulation methods Useful skill set for many applied 脰elds in economics Theme 1: Connecting Theory to Data Modern public economics tightly integrates theory with empirical evidence to derive quantitative predictions about policy || Taditional approach: theoretical models and numerical simulations Recent work derives robust formulas that can be implemented using well-identified empirical estimates Theme 2: Quasi-Experimental Empirical Methods Theme 3: “Big Data” so-called pre-existing Survey Data and Administrative Data Theme 4: Behavioral Models Administrative data refers to information collected primarily for administrative (not research) purposes. This type of data is collected by government departments and other organisations for the purposes of registration, transaction and record keeping, usually during the delivery of a service. Event Study Designs Event studies are a powerful research design when treatments are staggered in time across individuals Use a group of treated individuals as counterfactuals for each other to account for time series trends Good for identifying sharp, short-run e§ects but not longer-term impacts Methodology Define “event time” as calendar time minus date of treatment for each treated obs. Plot means/medians, etc. of outcome variable by event time Social Insurancedefinition Transfers based on events such as unemployment, disability, or age Contrasts with welfare: means-tested transfers SI is the biggest and most rapidly growing part of government expenditure today MAIN QUESTION Why have social (as opposed to private, or any) insurance? What type of SI system maximizes social welfare? Tradeoff between two forces Benefits — reducing risk (áuctuations in consumption) Distortion — changes in incentives for workers and firms -&gt; ineficient behavior and DWL Also will generate new distortions as you fix the problem you set out to solve -&gt; second-best solution. Identify optimal policy by combining theoretical models of social insurance with empirical evidence on program effects first question: Why have social insurance Motivation for insurance: reduction in risk for risk-averse individuals Unemp Ins: risk of involuntary unemployment Workersícomp and DI: risk of injuries/disabilities Social Security annuity: risk of living too long But why is government intervention needed to provide this insurance? Possible sources of market failure here: Informational problems (adverse selection) Individual optimization failures (myopia/improper planning) Macroeconomic shocks Adverse Selectionreference: Rothschild and Stiglitz (1976); see MWG Ch. 13 for a good review Main result: can lead to market failure where no equilibrium supportsprovision of insurance]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>public economics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stata Learning]]></title>
    <url>%2F2015%2F12%2F20%2Fstata-learning%2F</url>
    <content type="text"><![CDATA[基本操作导入数据If your data are in free format, with variables separated by blanks, commas, or tabs, you can use the infile command. infile str14 country setting effort change using http://data.princeton.edu/wws509/datasets/effort.raw 基本命令列举个案 list list sex-age in 1/10 , nolabel list sex-age if missing(edu) 描述数据 describe计数 count count if foreign==1 计算 display 查看变量 codebook 报告变量的分布 inspect 清理内存 clear 退出 exit 创建路径 mkdir (Make directory) 转向路径 cd (change directory) cd “path” cd .. cd . cd \ 显示当前路径 pwd (the path of the current working directory) 显示内容 dir ls sysdir 拷贝文件 copy 删除文件 erase (每次一个文件) 压缩文件 compress 显示内容长度 set scrollbufsize 设置memory 大小 set memory expand 10 duplicates drop log 文件捕获日志文件没有关闭的错误：capture log close //关闭所有打开的日志文件 打开一个log文件：log using 文件名 若执行某一指令后的结果没有必要记录下来，则可事先用指令“log off ”暂停记录, 需要记录时再用“log on”继续记录, 最后用“log close”关闭文件。若已存在 log using result1, replace ；要在其后进行添加，则键入log using result1, append 只记录命令的、没有命令执行结果输出的：cmdlog using filename [, append replace] ​提示：对一次数据处理可以同时打开两类日志文件。前者用于数据分析，后者用于形成do-file文件。 基本操作查找变量 lookfor改变变量名 rename old_varname new_varname 生成新变量 generate newvar = exp [if exp][in range]generate bmi=weight/(height^2) 重新编码 recode recode sex (2=0)(1=1)(else=.),gen(gender) 按某变量排序 sort varlist 排位变量 order order edu2, last 移动变量 move 替换原变量的取值 replace replace sex=. if sex==9 Continuation LinesWhen you are typing on the command window a command can be as long as needed. In a do-file you will probably want to break long commands into lines to improve readability. To indicate to Stata that a command continues on the next line you use ///, which says everything else to the end of the line is a comment and the command itself continues on the next line. For example you could write: graph twoway (scatter lexp loggnppc) /// (lfit lexp loggnppc) Oldhands might write graph twoway (scatter lexp loggnppc) /* (lfit lexp loggnppc) */ which “comments out” the end of the line. 在stata中加入时间复制时间后，destring date, replace ignore(&quot;-&quot;)变成201206格式即可 加入时间那一列的名称叫做TIME命令tesset : Declare data to be time-series datatesset TIME 就把数据转换为时间序列的了gen t= _n 把数据从1开始排序到最后tsset t就没有gaps的问题了 回归之后显示结果：disp e(N)显示数据量]]></content>
      <categories>
        <category>develop</category>
      </categories>
      <tags>
        <tag>stata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex Learning]]></title>
    <url>%2F2015%2F12%2F20%2FLatex-Learning%2F</url>
    <content type="text"><![CDATA[打开mikTe 安装宏包1、运行 cmd;2、输入 mpm命令，打开MikTex Package Manager界面；3、进入Repository菜单，选择Change Package Repository…命令;4、从Change Package Repository窗口中选择Packages shall be installed from the Internet, 点击下一步；5、选择一个可连接的地址（第一可用）进行同步；6、等待数据库同步完成；7、同步完成后，在Name中输入algorithm并确定；8、鼠标右击algorithms记录选择，Install选项即可。（此处为已安装状态，所以为灰色状态。 数学公式数学公式必须在数学模式下使用，即数学公式必须放在数学环境下，数学模式可以有很多种表示形式，不同的数学模式的显示效果是不同的，ams(美国数学学会)还特别定义了一组数学模式宏包，可在导言区调用该宏包，然后在正文中使用该宏包的命令，关于数学模式和ams的讨论放在后面进行，这里主要说明Latex的基本数学公式命令。一些常见的数学环境包括1234\[数学公式\]、$数学公式$、$$数学公式$$、\begin&#123;equation&#125;数学公式\end&#123;equation&#125; 等等。以下是一些基本的数学公式命令。 A．角标(上下标) 上标命令：^{} 下标命令：_{} 上下标命令放在需要插入上下标的地方，花括弧内为上下标内容，当角标为单个字符时，可以不用花括号；如果角标为多字符或多层次，必须用花括号.举例：x^2, x_1^2, x^{(n)}_{22}, ^{16}O^{2-}_{32}, x^{y^{z^a}}, x^{y_z} 分别显示为： x^2, x_1^2, x^{(n)}_{22}, ^{16}O^{2-}_{32}, x^{y^{z^a}}, x^{y_z}如果使用文字作为角标，首先要把文字放在\mbox{}文字模式中，另外要加上改变文字大小的命令，例如： \partial f_{\mbox{\tiny 极大值}} 如果不加改变大小的命令，则输出为：[Latex学习笔记]数学公式基本命令 - 激进的猫 - 激进的猫 当角标位置看起来不明显时，可以强制改变角标大小或层次，举例如下： y_N, y_{_N}, y_{_{\scriptstyle N}} 第一种为正常输出，但输出效果不明显；第二种将一级角标改为二级角标，字体也自动变为二级角标字体；第三种将一级角标改为二级角标，但强制将字体改为一级角标字体。 B．分式 分式命令：\frac{分子}{分母}。 对于行内短分式，可用斜线/输入，例如：(x+y)/2 举例： 行内分式 \(\frac{x+y}{y+z} \) 行间分式 \[\frac{x+y}{y+z}\] 上面的例子表明行内分式字体比行间分式字体小，因为行内分式使用的是角标字体。可以人工改变行内分式的字体大小，例如这个行内公式$\displaystyle\frac{x+y}{y+z}$ (显示为：[Latex学习笔记]数学公式基本命令 - 激进的猫 - 激进的猫)的大小和行间公式是一样的。 连分式： 123\begin&#123;displaymath&#125; x_0+\frac&#123;1&#125;&#123;x_1+\frac&#123;1&#125;&#123;x_2+\frac&#123;1&#125;&#123;x_3+\frac&#123;1&#125;&#123;x_4&#125;&#125;&#125;&#125;\end&#123;displaymath&#125; 可以通过强制改变字体大小使得分子分母字体大小一致，例如：1234\newcommand&#123;\FS&#125;[2]&#123;\displaystyle\frac&#123;#1&#125;&#123;#2&#125;&#125;\begin&#123;displaymath&#125;x_0+\FS&#123;1&#125;&#123;x_1+\FS&#123;1&#125;&#123;x_2+\FS&#123;1&#125;&#123;x_3+\FS&#123;1&#125;&#123;x_4&#125;&#125;&#125;&#125;\end&#123;displaymath&#125; 其中第一行命令定义了一个新的分式命令，规定每个调用该命令的分式都按\displaystyle的格式显示分式；分式放在displaymath环境中。分数线长度值是预设为分子分母的最大长度，如果想要使分数线再长一点，可以在分子或分母两端添加一些间隔，例如：\frac{1}{2}, \frac{\;1\;}{\;2\;}(显示为：$\frac{1}{2}, \frac{\;1\;}{\;2\;}$，第一个分式是正常的分式，第二个分式在分子(分母)前后都加入个一个间隔命令\;) C．根式 二次根式命令：\sqrt{表达式} ．如果表达式是单个字符，则不需要花括号，但需要在字符和sqrt间加入一个空格。n次根式命令：\sqrt[n]{表达式} 被开方表达式字符高度不一致时，根号上面的横线可能不在同一条直线上；为了使横线在同一直线上，可以在被开方表达式中插入一个只有高度没有宽度的数学支柱(\mathstrut)，例如：1\[ \sqrt&#123;a&#125;+\sqrt&#123;b&#125;+\sqrt&#123;c&#125;, \qquad \sqrt&#123;\mathstrut a&#125;+\sqrt&#123;\mathstrut b&#125;+\sqrt&#123;\mathstrut c&#125; \] 当被开方表达式较高时，开方次数的位置显得略低，解决办法为：将开方次数改为上标，并拉近与根式的水平距离，即将命令中的[n]改为^n!，例如： \begin{eqnarray} \sqrt{1+\sqrt[p]{1+\sqrt[q]{1+a}}}\\ \sqrt{1+\sqrt[^p\!]{1+\sqrt[^q\!]{1+a}}} \end{eqnarray} 显示为 \begin{eqnarray} \sqrt{1+\sqrt[p]{1+\sqrt[q]{1+a}}}\\ \sqrt{1+\sqrtp\!{1+\sqrtq\!{1+a}}} \end{eqnarray} (注意比较两个根式的开方次数的显示位置) 命令\surd生成根号上没有横线的根式，例如：\surd{x+y+z}显示为$\surd{x+y+z}$ D ．求和与积分 求和命令：\sum_{k=1\}^n (求和项紧随其后，下同) 积分命令：\int_a^b 例如：无穷级数\sum_{k=1}^\infty \frac{x^n}{n!} (显示为：$\sum_{k=1}^\infty \frac{x^n}{n!}$ )可化为积分$\int_0^\infty e^x$(显示为：$\int_0^\infty e^x$ ), 也即\[\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x\] (显示为：$\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x$) 改变上下限位置的命令：\limits(强制上下限在上、下侧) 和 \nolimits(强制上下限在右侧) 行内公式上下限在积分、求和符号上侧：\sum\limits_{k=1}^n 和 \int\limits_a^b，例如$\sum\limits_{n=0}^\infty x^n$ 或 $\int\limits_a^b$ (分别显示为：[Latex学习笔记]数学公式基本命令 - 激进的猫 - 激进的猫、[Latex学习笔记]数学公式基本命令 - 激进的猫 - 激进的猫) 行间公式上下限在积分、求和符号右侧：[ \sum\nolimits_{k=1}^n ]，例如： \[\sum\nolimits_{k=1}^\infty x^n=\frac{1}{1+x}\] (显示为：$[\sum\nolimits_{k=1}^\infty x^n=\frac{1}{1+x}]$ ) E．下划线、上划线等12上划线命令：\overline&#123;公式&#125;下划线命令：\underline&#123;公式&#125; 例如：\overline{\overline{a^2}+\underline{ab}+\bar{a}^3} (显示为：$\overline{\overline{a^2}+\underline{ab}+\bar{a}^3}$ ) 12上花括弧命令：\overbrace&#123;公式&#125;^&#123;说明&#125;下花括弧命令：\underbrace&#123;公式&#125;_&#123;说明&#125; 例如： \[ \underbrace{a+\overbrace{b+\dots+b}^{m\mbox{\scriptsize 个}}+c}_{20\mbox{\scriptsize 个}} \] \underbrace{a+\overbrace{b+\dots+b}^{m\mbox{\scriptsize 个}}+c}_{20\mbox{\scriptsize 个}}F ． 数学重音符号 以 a 为例，；如果字母i或j带有重音，字母i、j应替换为\imath、\jmath$\imath、\jmath $G． 堆积符号 符号堆积命令：\stacrel{上位符号}{基位符号} 说明：基位符号大，上位符号小12&#123;上位公式 \atop 下位公式&#125; 说明：上下符号一样大&#123;上位公式 \choose 下位公式\&#125; 说明：上下符号一样大；上下符号被包括在圆括弧内 例如：12345\begin&#123;eqnarray*&#125; \vec&#123;x&#125;\stackrel&#123;\mathrm&#123;def&#125;&#125;&#123;=&#125;&#123;x_1,\dots,x_n&#125;\\ &#123;n+1 \choose k&#125;=&#123;n \choose k&#125;+&#123;n \choose k-1&#125;\\ \sum_&#123;k_0,k_1,\ldots&gt;0 \atop k_0+k_1+\cdots=n&#125;A_&#123;k_0&#125;A_&#123;k_1&#125;\cdots \end&#123;eqnarray*&#125; 上述代码显示为： \begin{eqnarray} \vec{x}\stackrel{\mathrm{def}}{=}{x_1,\dots,x_n}\\ {n+1 \choose k}={n \choose k}+{n \choose k-1}\\ \sum_{k_0,k_1,\ldots&gt;0 \atop k_0+k_1+\cdots=n}A_{k_0}A_{k_1}\cdots \end{eqnarray} H． 定界符1234567\[()\big(\big)\Big(\Big)\bigg(\bigg)\Bigg(\Bigg)\] 以上代码显示为： () \big(\big) \Big(\Big) \bigg(\bigg) \Bigg(\Bigg)自适应放大命令：\left 和 \right；本命令分别放在左、右定界符前，自动随着公式内容大小调整符号大小$\left(122131231 \right)$]]></content>
      <categories>
        <category>develop</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Research Proposal Guide]]></title>
    <url>%2F2015%2F12%2F13%2Fresearch-proposal-guide%2F</url>
    <content type="text"><![CDATA[cite from others: http://www.library.illinois.edu/learn/research/proposal.html Guidelines on writing a research proposal by Matthew McGranaghanThis is a work in progress, intended to organize my thoughts on the process of formulating a proposal. If you have any thoughts on the contents, or on the notion of making this available to students, please share them with me. Thanks. IntroductionThis is a guide to writing M.A. research proposals. The same principles apply to dissertation proposals and to proposals to most funding agencies. It includes a model outline, but advisor, committee and funding agency expectations vary and your proposal will be a variation on this basic theme. Use these guidelines as a point of departure for discussions with your advisor. They may serve as a straw-man against which to build your understanding both of your project and of proposal writing. Proposal WritingProposal writing is important to your pursuit of a graduate degree. The proposal is, in effect, an intellectual scholastic (not legal) contract between you and your committee. It specifies what you will do, how you will do it, and how you will interpret the results. In specifying what will be done it also gives criteria for determining whether it is done. In approving the proposal, your committee gives their best judgment that the approach to the research is reasonable and likely to yield the anticipated results. They are implicitly agreeing that they will accept the result as adequate for the purpose of granting a degree. (Of course you will have to write the thesis in acceptable form, and you probably will discover things in the course of your research that were not anticipated but which should be addressed in your thesis, but the minimum core intellectual contribution of your thesis will be set by the proposal.) Both parties benefit from an agreed upon plan. The objective in writing a proposal is to describe what you will do, why it should be done, how you will do it and what you expect will result. Being clear about these things from the beginning will help you complete your thesis in a timely fashion. A vague, weak or fuzzy proposal can lead to a long, painful, and often unsuccessful thesis writing exercise. A clean, well thought-out, proposal forms the backbone for the thesis itself. The structures are identical and through the miracle of word-processing, your proposal will probably become your thesis. A good thesis proposal hinges on a good idea. Once you have a good idea, you can draft the proposal in an evening. Getting a good idea hinges on familiarity with the topic. This assumes a longer preparatory period of reading, observation, discussion, and incubation. Read everything that you can in your area of interest. Figure out what are the important and missing parts of our understanding. Figure out how to build/discover those pieces. Live and breathe the topic. Talk about it with anyone who is interested. Then just write the important parts as the proposal. Filling in the things that we do not know and that will help us know more: that is what research is all about. Proposals help you estimate the size of a project. Don’t make the project too big. Our MA program statement used to say that a thesis is equivalent to a published paper in scope. These days, sixty double spaced pages, with figures, tables and bibliography, would be a long paper. Your proposal will be shorter, perhaps five pages and certainly no more than fifteen pages. (For perspective, the NSF limits the length of proposal narratives to 15 pages, even when the request might be for multiple hundreds of thousands of dollars.) The merit of the proposal counts, not the weight. Shoot for five pithy pages that indicate to a relatively well-informed audience that you know the topic and how its logic hangs together, rather than fifteen or twenty pages that indicate that you have read a lot of things but not yet boiled it down to a set of prioritized linked questions. Different Theses, Similar ProposalsThis guide includes an outline that looks like a “fill-in the blanks model” and, while in the abstract all proposals are similar, each proposal will have its own particular variation on the basic theme. Each research project is different and each needs a specifically tailored proposal to bring it into focus. Different advisors, committees and agencies have different expectations and you should find out what these are as early as possible; ask your advisor for advice on this. Further, different types of thesis require slightly different proposals. What style of work is published in your sub-discipline? Characterizing theses is difficult. Some theses are “straight science”. Some are essentially opinion pieces. Some are policy oriented. In the end, they may well all be interpretations of observations, and differentiated by the rules that constrain the interpretation. (Different advisors will have different preferences about the rules, the meta-discourse, in which we all work.) In the abstract all proposals are very similar. They need to show a reasonably informed reader why a particular topic is important to address and how you will do it. To that end, a proposal needs to show how your work fits into what is already known about the topic and what new contribution your work will make. Specify the question that your research will answer, establish why it is a significant question, show how you are going to answer the question, and indicate what you expect we will learn. The proposal should situate the work in the literature, it should show why this is an (if not the most) important question to answer in the field, and convince your committee (the skeptical readers that they are) that your approach will in fact result in an answer to the question. Theses which address research questions that can be answered by making plan-able observations (and applying hypothesis testing or model selection techniques) are preferred and perhaps the easiest to write. Because they address well-bounded topics, they can be very tight, but they do require more planning on the front end. Theses which are largely based on synthesis of observations, rumination, speculation, and opinion formation are harder to write, and usually not as convincing, often because they address questions which are not well-bounded and essentially unanswerable. (One ‘old saw’ about research in the social sciences is that the finding is always: “some do and some don’t”. Try to avoid such insight-less findings; finding “who do and who don’t” is better.) One problem with this type of project is that it is often impossible to tell when you are “done”. Another problem is that the nature of argument for a position rather than the reasoned rejection of alternatives to it encourages shepherding a favored notion rather than converging more directly toward a truth. (See Chamberlain’s and Platt’s articles). A good proposal helps one see and avoid these problems. Literature review-based theses involve collection of information from the literature, distillation of it, and coming up with new insight on an issue. One problem with this type of research is that you might find the perfect succinct answer to your question on the night before (or after) you turn in the final draft —- in someone else’s work. This certainly can knock the wind out of your sails. (But note that even a straight-ahead science thesis can have the problem of discovering, late in the game, that the work you have done or are doing has already been done; this is where familiarity with the relevant literature by both yourself and your committee members is important.) A Couple of Models for ProposalsA Two Page (Preliminary Proposal) ModelHere is a model for a very brief (maybe five paragraph) proposal that you might use to interest faculty in sitting on your committee. People who are not yet hooked may especially appreciate its brevity. In the first paragraph, the first sentence identifies the general topic area. The second sentence gives the research question, and the third sentence establishes its significance. The next couple of paragraphs gives the larger historical perspective on the topic. Essentially list the major schools of thought on the topic and very briefly review the literature in the area with its major findings. Who has written on the topic and what have they found? Allocate about a sentence per important person or finding. Include any preliminary findings you have, and indicate what open questions are left. Restate your question in this context, showing how it fits into this larger picture. The next paragraph describes your methodology. It tells how will you approach the question, what you will need to do it. The final paragraph outlines your expected results, how you will interpret them, and how they will fit into the our larger understanding i.e., ‘the literature’. The (Longer) Standard ModelThe two outlines below are intended to show both what are the standard parts of a proposal and of a science paper. Notice that the only real difference is that you change “expected results” to “results” in the paper, and usually leave the budget out, of the paper. A Basic Proposal Outline:12345678910111213141516171819Introduction Topic area Research question Significance to knowledgeLiterature review Previous research others &amp; yours Interlocking findings and Unanswered questions Your preliminary work on the topic The remaining questions and inter-locking logic Reprise of your research question(s) in this contextMethodology Approach Data needs Analytic techniques Plan for interpreting resultsExpected resultsBudgetBibliography (or References) A Basic Thesis Outline12345678910111213141516171819Introduction Topic area Research question (finding?) Significance to knowledgeLiterature review Previous research others &amp; yours Interlocking findings and Unanswered questions Your preliminary work on the topic The remaining questions and inter-locking logic Reprise of your research question(s) in this contextMethodology Approach Data needs Analytic techniques Plan for interpreting resultsResultsDiscussion and ConclusionsBibliography Another outline (maybe from Gary Fuller?).12345678910111213141516Introduction Topic area Research Question and its significance to knowledgeLiterature review Previous research Your preliminary work on the topic The remaining questions and their inter-locking logic Reprise of your resulting question in this contextMethodology Approach to answering the question Data needs Analytic techniques Plan for interpreting resultsBudgetExpected resultsBibliography / References Each of these outlines is very similar. You probably see already that the proposal’s organization lends itself to word-processing right into the final thesis. It also makes it easy for readers to find relevant parts more easily. The section below goes into slightly more detail on what each of the points in the outline is and does.The Sections of the Proposal The IntroductionTopic AreaA good title will clue the reader into the topic but it can not tell the whole story. Follow the title with a strong introduction. The introduction provides a brief overview that tells a fairly well informed (but perhaps non-specialist) reader what the proposal is about. It might be as short as a single page, but it should be very clearly written, and it should let one assess whether the research is relevant to their own. With luck it will hook the reader’s interest. What is your proposal about? Setting the topical area is a start but you need more, and quickly. Get specific about what your research will address. Question Once the topic is established, come right to the point. What are you doing? What specific issue or question will your work address? Very briefly (this is still the introduction) say how you will approach the work. What will we learn from your work? Significance Why is this work important? Show why this is it important to answer this question. What are the implications of doing it? How does it link to other knowledge? How does it stand to inform policy making? This should show how this project is significant to our body of knowledge. Why is it important to our understanding of the world? It should establish why I would want to read on. It should also tell me why I would want to support, or fund, the project. Literature ReviewState of our knowledge The purpose of the literature review is to situate your research in the context of what is already known about a topic. It need not be exhaustive, it needs to show how your work will benefit the whole. It should provide the theoretical basis for your work, show what has been done in the area by others, and set the stage for your work. In a literature review you should give the reader enough ties to the literature that they feel confident that you have found, read, and assimilated the literature in the field. It might do well to include a paragraph that summarizes each article’s contribution, and a bit of ‘mortar’ to hold the edifice together, perhaps these come from your notes while reading the material. The flow should probably move from the more general to the more focused studies, or perhaps use historical progression to develop the story. It need not be exhaustive; relevance is ‘key’. Outstanding questions This is where you present the holes in the knowledge that need to be plugged, and by doing so, situate your work. It is the place where you establish that your work will fit in and be significant to the discipline. This can be made easier if there is literature that comes out and says “Hey, this is a topic that needs to be treated! What is the answer to this question?” and you will sometimes see this type of piece in the literature. Perhaps there is a reason to read old AAG presidential addresses. Research Questions in Detail Your work to dateTell what you have done so far. It might report preliminary studies that you have conducted to establish the feasibility of your research. It should give a sense that you are in a position to add to the body of knowledge.Methodology Overview of approachThis section should make clear to the reader the way that you intend to approach the research question and the techniques and logic that you will use to address it. Data Collection This might include the field site description, a description of the instruments you will use, and particularly the data that you anticipate collecting. You may need to comment on site and resource accessibility in the time frame and budget that you have available, to demonstrate feasibility, but the emphasis in this section should be to fully describe specifically what data you will be using in your study. Part of the purpose of doing this is to detect flaws in the plan before they become problems in the research. Data Analysis This should explain in some detail how you will manipulate the data that you assembled to get at the information that you will use to answer your question. It will include the statistical or other techniques and the tools that you will use in processing the data. It probably should also include an indication of the range of outcomes that you could reasonably expect from your observations. Interpretation In this section you should indicate how the anticipated outcomes will be interpreted to answer the research question. It is extremely beneficial to anticipate the range of outcomes from your analysis, and for each know what it will mean in terms of the answer to your question.Expected Results This section should give a good indication of what you expect to get out of the research. It should join the data analysis and possible outcomes to the theory and questions that you have raised. It will be a good place to summarize the significance of the work.It is often useful from the very beginning of formulating your work to write one page for this section to focus your reasoning as you build the rest of the proposal.Bibliography This is the list of the relevant works. Some advisors like exhaustive lists. I think that the Graduate Division specifies that you call it “Bibliography”. Others like to see only the literature which you actually cite. Most fall in between: there is no reason to cite irrelevant literature but it may be useful to keep track of it even if only to say that it was examined and found to be irrelevant.Use a standard format. Order the references alphabetically, and use “flag” paragraphs as per the University’s Guidelines. Tips and TricksRead. Read everything you can find in your area of interest. Read. Read. Read. Take notes, and talk to your advisor about the topic. If your advisor won’t talk to you, find another one or rely on ‘the net’ for intellectual interaction. Email has the advantage of forcing you to get your thoughts into written words that can be refined, edited and improved. It also gets time stamped records of when you submitted what to your advisor and how long it took to get a response. Write about the topic a lot, and don’t be afraid to tear up (delete) passages that just don’t work. Often you can re-think and re-type faster than than you can edit your way out of a hopeless mess. The advantage is in the re-thinking.Very early on, generate the research question, critical observation, interpretations of the possible outcomes, and the expected results. These are the core of the project and will help focus your reading and thinking. Modify them as needed as your understanding increases. Use some systematic way of recording notes and bibliographic information from the very beginning. The classic approach is a deck of index cards. You can sort, regroup, layout spatial arrangements and work on the beach. Possibly a slight improvement is to use a word-processor file that contains bibliographic reference information and notes, quotes etc. that you take from the source. This can be sorted, searched, diced and sliced in your familiar word-processor. You may even print the index cards from the word-processor if you like the ability to physically re-arrange things. Even better for some, is to use specialized bibliographic database software. Papyrus, EndNote, and other packages are available for PCs and MacIntoshs. The bib-refer and bibTex software on UNIX computers are also very handy and have the advantage of working with plain ASCII text files (no need to worry about getting at your information when the wordprocessor is several generations along). All of these tools link to various word-processors to make constructing and formating your final bibliography easier, but you won’t do that many times anyway. If they help you organize your notes and thinking, that is the benefit. Another pointer is to keep in mind from the outset that this project is neither the last nor the greatest thing you will do in your life. It is just one step along the way. Get it done and get on with the next one. The length to shoot for is “equivalent to a published paper”, sixty pages of double spaced text, plus figures tables, table of contents, references, etc. is probably all you need. In practice, most theses try to do too much and become too long. Cover your topic, but don’t confuse it with too many loosely relevant side lines.This is not complete and needs a little rearranging.The balance between Introduction and Literature Review needs to be thought out. The reader will want to be able to figure out whether to read the proposal. The literature review should be sufficiently inclusive that the reader can tell where the bounds of knowledge lie. It should also show that the proposer knows what has been done in the field (and the methods used). The balance may change between the proposal and the thesis. It is common, although not really desirable, for theses to make reference to every slightly related piece of work that can be found. This is not necessary. Refer to the work that actually is linked to your study, don’t go too far afield (unless your committee is adamant that you do ;-).Useful References: Krathwohl, David R. 1988. How to Prepare a Research Proposal: Guidelines for Funding and Dissertations in the Social and Behavioral Sciences . Syracuse University Press. Recent National Science Foundations Guidelines for Research Proposals can be found on the NSF website, www.nsf.gov. Chamberlain, T.C. “The Method of Multiple Working Hypotheses”, reprinted in Science, Vol 148, pp754-759. 7 May 1965. Platt, J. “Strong Inference” in Science, Number 3642, pp. 347-353, 16 October 1964. Strunk and White The Elements of Style Turabian, Kate. 1955 (or a more recent edition) A Manual for Writers of Term Papers, Theses and Dissertations, University of Chicago Press. Mortimer J. Adler and Charles Van Doren. 1940 (‘67, ‘72 etc). How to Read a Book. Simon and Schuster Publishers. New York City, NY.]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>guide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BMA Introduction]]></title>
    <url>%2F2015%2F10%2F25%2FBMA-Intro%2F</url>
    <content type="text"><![CDATA[Bayesian Model AveragingBasic IdeasAssume we want to investigate some problem by statistical method, and we have data D. several possible statistical models are $M_1$, $M_2$…The number of models could be quite large. For example, if we consider only regression models but are unsure about which of $p$ possible predictors to include, there could be as many as $2^p$. BMA uses Bayesian statistics to select possible candidate predictors and help use construct the right model. Methology Introductionconsider the unknown quantity of interest $\Delta$ and observed data $D$. we have possible models,$M_1$, $M_2$…$M_k$ , to do the estimation and forecasting. Then by a simple application of thelaw of total probability, the BMA posterior distribution of $\Delta$ is p(\Delta | D)=\sum\limits_{k = 1}^K {p(\Delta |D,M_k)p(M_k|D)} \ \ (1)where $p(\Delta |D,M_k)$ is the posterior distribution of $\Delta$ given the model $M_k$, and $p(M_k|D)$ is the posterior probability that $M_k$ is the correct model, given that one of the models considered is correct. Thus the BMA posterior distribution of $\Delta$ is a weighted average of the posterior distributions of $\Delta$ under each of the models, weighted by their posterior model probabilities.(take the above equation as a weighted average case). note that $p(\Delta |D,M_k)$ and $p(M_k|D)$ can be probability density functions, probability mass functions, or cumulative distribution functions. The posterior model probability of $M_k$ is given by p(M_k|D) = \frac{p(D| M_k)p( M_k)} { \sum \limits_{l = 1}^K {p(D| M_l ) p(M_l)} } (2)$p(D| M_k)$ is the integrated likelihood of model $M_k$ , obtained by integrating (not maximizing)over the unknown parameters p(D|M{k}) = \int p(D|\theta{k},M{k})p(\theta{k}|M{k}){\rm d}\theta{k}=\int (likelihood \times prior)\ {\rm d}\theta_{k}where $θ_k$ is the parameter of model $M_k$ and $p(D| \theta_k, M_k)$ is the likelihood of $\theta_k$ under model $M_k$ . The prior model probabilities are often taken to be equal, so they cancel in (2), but the BMA package allows other formulations also information criterionfrom the above (3), we see that the integrated likelihood $p(D| M_k )$ is a high dimensional integral that can be hard to calculate analytically. recalling the Maximum likelihood selection creteria, we found that $p(D| M_k )$ can be the simple and surprisingly accurate BIC approximation, which greatly simplify the model selection and calculation procedure 2 \log p(D|M_{k})\approx2\log p(D|\tilde{\theta}_{k})-d_{k}\log(n)=-BIC_{k}where $d_{k}=\dim(\theta_{k})$ is the number of independent parameters in $M_k $, and $\tilde{\theta}_k$ is the maximum likelihood estimator (equal to the ordinary least squares estimator for linear regression coefficients). And for linear regression, BIC has the simple form: BIC_{k}=n\log(1-R_{k}^{2})+p_{k}\log{n}up to an additive constant, where $R_k^2$ is the value of $R^2$ and $P_k$ is the number of regressors for the kth regression model. eg: to illustrate what $\Delta$ is: When interest focuses on a model parameter, say a regression parameter such as $\beta_1 $ , (1) can be applied with $\Delta =\beta_1$ . The BMA posterior mean of $\beta_1$ is just a weighted average of the posterior means of $\beta_1$ under each of the models: E[\beta_{1}|D]=\sum \hat{\beta}_{1} p(M_{k}|D)which can be viewed as a model-averaged Bayesian point estimator. $\hat{\beta}_{1}$ is the posterior mean of $\beta_1$ under model $M_k$, and this can be approximated by the corresponding maximum likelihood estimator, $\hat{\beta}_{1}$ (Raftery, 1995). For a survey and literature review of Bayesian model averaging, see Hoeting et al. (1999) ApplicationLinear REgressionwe use the UScrime dataset on crime rates in 47 U.S. states in 1960 (Ehrlich, 1973); this isavailable in the MASS library. There are 15 potential independent variables, all associated with crime rates in the literature. The last two, probability of imprisonment and average time spent in state prisons, were the predictor variables of interest in the original study, while the other 13 were control variables. All variables for which it makes sense were logarithmically transformed. The commands are: 1234567891011library(BMA)library(MASS)data(UScrime)x.crime&lt;- UScrime[,-16]y.crime&lt;- log(UScrime[,16])x.crime[,-2]&lt;- log(x.crime[,-2])crime.bicreg &lt;- bicreg(x.crime, y.crime)summary (crime.bicreg, digits=2) plot (crime.bicreg,mfrow=c(4,4)) # c(4,4) 意思是图形以4行4列共16个集体显示 This summary yields Figure 1. (The default number of digits is 4, but this may be more thanneeded.) The column headed “p!=0” shows the posterior probability that the variable is in the model (in %). The column headed “EV” shows the BMA posterior mean, and the column headed “SD” shows theBMA posterior standard deviation for each variable. The following five columns show the parameter estimates for the best five models found, together with the numbers of variables they include, their $R^2$ values, their BIC values and their posterior model probabilities. Figure 2: BMA posterior distribution of the coefficient of the average time in prison variable in thecrime dataset. The spike at 0 shows the posterior probability that the variable is not in the model.(在0点的那条竖线的纵坐标值代表不在模型中的概率是多少) The curve is the model-averaged posterior density of the coefficient given that the variable is in the model, approximated by a finite mixture of normal distributions, one for each model that includes the variable. The density is scaled so that its maximum height is equal to the probability of the variable being in the model. Logistic RegressionWe illustrate BMA for logistic regression using the low birthweight data set of Hosmer and Lemeshow(1989), available in the MASS library. The dataset consists of 189 babies, and the dependent variablemeasures whether their weight was low at birth. There are 8 potential independent variables of whichtwo are categorical with more than two categories: race and the number of previous premature labors(ptl). Figure 5 shows the output of the commands 12345678910library(BMA)library(MASS)data(birthwt)birthwt$race &lt;- as.factor (birthwt$race)birthwt$ptl &lt;- as.factor (birthwt$ptl)bwt.bic.glm &lt;- bic.glm (low ~ age + lwt+ race + smoke + ptl + ht + ui + ftv,data=birthwt, glm.family="binomial")summary (bwt.bic.glm,conditional=T,digits=2) The function bic.glm in BMA can accept a formula, as here, instead of a design matrix and dependent variable (the same is true of bic.surv but not of bicreg) Survival AnalysisBMA can account for model uncertainty in survival analysis using the Cox proportional hazards model.We illustrate this using the well-known Primary Biliary Cirrhosis (PBC) data set of Fleming and Harrington (1991). This consists of 312 randomized patients with the disease, and the dependent variable was survival time; 187 of the records were censored. There were 15 potential predictors. The data set is available in the survival library, which is loaded when BMA is loaded. 12345678910111213141516library(BMA)library(MASS)data(pbc)x.pbc&lt;- pbc[1:312,]surv.t&lt;- x.pbc$timecens&lt;- x.pbc$statusx.pbc&lt;- x.pbc[,-c(6,7,10,17,19)]x.pbc$bili&lt;- log(x.pbc$bili)x.pbc$alb&lt;- log(x.pbc$alb)x.pbc$protime&lt;- log(x.pbc$protime)x.pbc$copper&lt;- log(x.pbc$copper)x.pbc$sgot&lt;- log(x.pbc$sgot)pbc.bic.surv &lt;- bic.surv(x.pbc,surv.t,cens)summary(pbc.bic.surv,digits=2)plot(pbc.bic.surv,mfrow=c(4,4))imageplot.bma(pbc.bic.surv) SummaryBased on R package BMA, we can easily deal with Bayesian model averaging for linear regression, generalized linear models, and survival or event history analysis using Cox proportional hazards models. More specific coding can refer to the manual of BMA. But based on the above knowledge, we are able to do some simple application by BMA.]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>econometrics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-process Introduction In Python]]></title>
    <url>%2F2015%2F09%2F28%2Fmulti-process%20introduction%20in%20python%2F</url>
    <content type="text"><![CDATA[多进程介绍由于CPython实现中的GIL的限制，python中的多线程其实并不是真正的多线程，如果想要充分地使用多核CPU的资源，在python中大部分情况我们需要使用多进程。 这也许就是python中多进程类库如此简洁好用的原因所在。在python中可以向多线程一样简单地使用多进程。 简单多进程程序我们先看一个简单的例子，从而对多进程有一个直观的认识： 123456789101112131415161718192021222324252627282930313233import multiprocessing as mpimport os, time, random # 子进程要执行的代码def run_work(ID): print('Run process %s (%s)...' % (ID, os.getpid())) print('process %s is done' %(ID)) if __name__=='__main__': print 'Parent process %s.' % os.getpid() print 'Process will start.' record=[] for i in range(3): p = mp.Process(target=run_proc, args=(i,)) p.start() record.append(p) for p in record: p.join() print 'Process end.'#### 结果为####Parent process 6848.Process will start.Run process 0 (4900)...process 0 is doneRun process 1 (5720)...process 1 is doneRun process 2 (6508)...process 2 is doneProcess end. multiprocessing 是Python下面支持多进程的模块。进程任务的编类似线程操作，首先是定义一个工作函数(work_fun)，接着是对进程的声明，利用multiprocessing.Process([group[, target[, name[, args[, kargs]]]]])创建新的进程。其中重要的参数如下： target: 函数名 name: 进程名 args: 函数的参数 kargs: keywords参数这里，target = work_fun , work_fun函数有ID这个参数，那么args=(i,) 只有一个参数的时候要加上,（tuple形式的参数传入）。声明好进程后，就可以启动进程运行了。启动进程采用 .start()函数。其余Process主要函数有： run() 默认的run()函数调用target的函数，可以在子类中覆盖该函数，重写如何启动 start() 启动该进程 join([timeout]) 父进程被停止，直到子进程被执行完毕。当timeout为None时没有超时，否则有超时。进程可以被join很多次，但不能join自己 注意：由于各种问题，python multiprocessing在windows下面编写的时候需要放在if name==’main‘: 下避免各种各样的错误 run函数时默认执行的，如果不是写子类(class）不需要关注。start函数目的在于启动进程，进程运行完work_fun后自动结束。但不同进程结束时间未必相同，这就需要最后来一个同步：.join()方法，保证子进程全部结束，主进程退出。程序运行完毕。这样一个简单的多进程运行框架就构建完毕了。 主进程和子进程的关系一般来说，主进程运行结束后，子进程未必运行结束，这时候就存在选择，等待所有进程结束，或者主进程结束后子进程也随之结束。所以python多进程中设置了daemon方法，目的在于是否执行主进程结束后，子进程结束的命令。下面两个例子可以直观的告诉我们主进程和子进程的关系： 123456789101112131415161718192021222324252627282930313233343536373839## daemon 为false ##import multiprocessingimport timedef worker(interval): print("work start:&#123;0&#125;".format(time.ctime())); time.sleep(interval) print("work end:&#123;0&#125;".format(time.ctime()));if __name__ == "__main__": p = multiprocessing.Process(target = worker, args = (1,)) p.start() print "end of the main process!"### 结果 ####end!#work start:Tue Apr 21 21:29:10 2015#work end:Tue Apr 21 21:29:13 2015import multiprocessingimport timedef worker(interval): print("work start:&#123;0&#125;".format(time.ctime())); time.sleep(interval) print("work end:&#123;0&#125;".format(time.ctime()));if __name__ == "__main__": p = multiprocessing.Process(target = worker, args = (3,)) p.daemon = True p.start() print "end of the main process!"### 结果 ####end! 主进程结束，子进程随之结束是一个比较安全的做法，但很多时候子进程运行与主进程运行乃至各个子进程间运行速度不一致，如何让主进程最后结束呢？添加join命令即可： 12345678910111213141516171819import multiprocessingimport timedef worker(interval): print("work start:&#123;0&#125;".format(time.ctime())); time.sleep(interval) print("work end:&#123;0&#125;".format(time.ctime()));if __name__ == "__main__": p = multiprocessing.Process(target = worker, args = (3,)) p.daemon = True p.start() p.join() print "end!"### 结果 ####work start:Tue Apr 21 22:16:32 2015#work end:Tue Apr 21 22:16:35 2015#end! 接下来将会介绍如何将多进程程序写成类的形式。 转换为类12345678910111213141516171819202122232425import multiprocessingimport timeclass ClockProcess(multiprocessing.Process): def __init__(self, interval): multiprocessing.Process.__init__(self) self.interval = interval def run(self): n = 5 while n &gt; 0: print("the time is &#123;0&#125;".format(time.ctime())) time.sleep(self.interval) n -= 1if __name__ == '__main__': p = ClockProcess(3) p.start() #### 结果 #####the time is Sat Sep 26 11:10:38 2015#the time is Sat Sep 26 11:10:41 2015#the time is Sat Sep 26 11:10:44 2015#the time is Sat Sep 26 11:10:47 2015#the time is Sat Sep 26 11:10:50 2015 从上面的例子中可以看到，编写子类，需要继承multiprocessing.Process，然后可以重写run函数进行编辑。接下来将会介绍如何利用进程池来操作 进程的同步进程间运行不可避免会涉及到同步问题，防止进程同时访问某个存储空间造成读写冲突是编写多进程程序一个一个核心问题。multiprocessing 提供了多种方法来帮助我们实现进程间的同步问题： lock利用“锁”（lock）方式来限制同时访问某个内存空间避免出错的方式是多进程操作中常用的同步方法。下面一个例子就是简单的使用lock来限制访问： 123456789101112from multiprocessing import Process, Lock def l(lock, num): lock.acquire() print "Hello Num: %s" % (num) lock.release() if __name__ == '__main__': lock = Lock() for num in range(20): Process(target=l, args=(lock, num)).start() SemaphoreSemaphore用来控制对共享资源的访问数量，例如池的最大连接数 123456789101112131415import multiprocessingimport timedef worker(s, i): s.acquire() print(multiprocessing.current_process().name + "acquire"); time.sleep(i) print(multiprocessing.current_process().name + "release\n"); s.release()if __name__ == "__main__": s = multiprocessing.Semaphore(2) for i in range(5): p = multiprocessing.Process(target = worker, args=(s, i*2)) p.start() EventEvent用来实现进程间同步通信。同步的方式可以利用设定时间来等待同步，也可以利用set函数来进行同步下面的例子说明了这一点： 1234567891011121314151617181920212223242526272829303132333435363738import multiprocessingimport timedef wait_for_event(e): print("wait_for_event: starting") e.wait() print("wairt_for_event: e.is_set()-&gt;" + str(e.is_set()))def wait_for_event_timeout(e, t): print("wait_for_event_timeout:starting") e.wait(t) print("wait_for_event_timeout:e.is_set-&gt;" + str(e.is_set()))if __name__ == "__main__": e = multiprocessing.Event() w1 = multiprocessing.Process(name = "block", target = wait_for_event, args = (e,)) w2 = multiprocessing.Process(name = "non-block", target = wait_for_event_timeout, args = (e, 2)) w1.start() w2.start() time.sleep(3) e.set() print("main: event is set")### 结果 ####wait_for_event: starting#wait_for_event_timeout:starting#wait_for_event_timeout:e.is_set-&gt;False#main: event is set#wairt_for_event: e.is_set()-&gt;True pipPipe方法返回(conn1, conn2)代表一个管道的两个端。Pipe方法有duplex参数，如果duplex参数为True(默认值)，那么这个管道是全双工模式，也就是说conn1和conn2均可收发。duplex为False，conn1只负责接受消息，conn2只负责发送消息。 send和recv方法分别是发送和接受消息的方法。例如，在全双工模式下，可以调用conn1.send发送消息，conn1.recv接收消息。如果没有消息可接收，recv方法会一直阻塞。如果管道已经被关闭，那么recv方法会抛出EOFError。 12345678910111213141516171819202122232425262728293031323334import multiprocessingimport timedef proc1(pipe): while True: for i in range(10): print "send: %s" %(i) pipe.send(i) time.sleep(1)def proc2(pipe): while True: print "proc2 rev:", pipe.recv() time.sleep(1)def proc3(pipe): while True: print "PROC3 rev:", pipe.recv() time.sleep(1)if __name__ == "__main__": pipe = multiprocessing.Pipe() p1 = multiprocessing.Process(target=proc1, args=(pipe[0],)) p2 = multiprocessing.Process(target=proc2, args=(pipe[1],)) #p3 = multiprocessing.Process(target=proc3, args=(pipe[1],)) p1.start() p2.start() #p3.start() p1.join() p2.join() #p3.join() QueueQueue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。put方法用以插入数据到队列中，put方法还有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，该方法会阻塞timeout指定的时间，直到该队列有剩余的空间。如果超时，会抛出Queue. Full异常。如果blocked为False，但该Queue已满，会立即抛出Queue.Full异常。 get方法可以从队列读取并且删除一个元素。同样，get方法有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，那么在等待时间内没有取到任何元素，会抛出Queue.Empty异常。如果blocked为False，有两种情况存在，如果Queue有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出Queue.Empty异常。 12345678910111213141516171819202122232425import multiprocessingdef writer_proc(q): try: q.put(111, block = False) except: pass def reader_proc(q): try: print q.get(block = False) except: passif __name__ == "__main__": q = multiprocessing.Queue() writer = multiprocessing.Process(target=writer_proc, args=(q,)) writer.start() reader = multiprocessing.Process(target=reader_proc, args=(q,)) reader.start() reader.join() writer.join() 进程池pool利用Python并行操作可以节约大量的时间。当被操作对象数目不大时，可以直接利用multiprocessing中的Process动态成生多个进程，10几个还好，但如果是上百个，上千个目标，手动的去限制进程数量却又太过繁琐，这时候我们就可以利用进程池pool来操作了。Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来它。首先来看一个简单的例子 1234567891011121314151617181920import multiprocessingimport timedef func(ID): print("ID: %d", % ID) time.sleep(3) print("end of process %s" % os.getpid())if __name__ == "__main__": pool = multiprocessing.Pool(processes = 3) for i in range(4): pool.apply_async(func, (i, )) #维持执行的进程总数为processes=3，当一个进程执行完毕后会添加新的进程进去 print "starting the pool !!!" pool.close() pool.join() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 print("Sub-process(es) done.") 关键函数解释： apply_async(func[, args[, kwds[, callback]]]) 它是非阻塞运行与之相对应的是apply(func[, args[, kwds]])是阻塞的，也就是说运行完一个再运行下一个， close() 关闭pool，使其不在接受新的任务。 terminate() 结束工作进程，不在处理未完成的任务。 join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用。 创建一个进程池pool，并设定进程的数量为3，xrange(4)会相继产生四个对象[0, 1, 2, 4]，四个对象被提交到pool中，因pool指定进程数为3，所以0、1、2会直接送到进程中执行，当其中一个执行完事后才空出一个进程处理对象3。因为非阻塞，主函数执行独立于进程的执行，所以运行完for循环后直接输出“starting the pool !!!”，主程序在pool.join（）处等待各个进程的结束。若为阻塞的例子： 12345678910111213141516171819#coding: utf-8import multiprocessingimport timedef func(ID): print("ID: %d", % ID) time.sleep(1) print("end of process %s" % os.getpid())if __name__ == "__main__": pool = multiprocessing.Pool(processes = 3) for i in range(4): pool.apply(func, (i, )) #维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去 print "starting the pool !!!" pool.close() pool.join() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 print "Sub-process(es) done." 若想返回各个进程的结果该如何操作呢？可以利用如下的例子，这告诉我们，利用pool我们可以实现并行计算。 12345678910111213141516171819import multiprocessingimport time,osdef func(ID): print("ID: %d" % ID) time.sleep(1) print("end of process %s" % os.getpid()) return "done"+ str(ID)if __name__ == "__main__": pool = multiprocessing.Pool(processes=4) result = [] for i in xrange(10): result.append(pool.apply_async(func, (i, ))) pool.close() pool.join() for res in result: print ":::", res.get() print("Sub-process(es) done.") 使用多个函数来进行计算的例子： 123456789101112131415161718192021222324252627282930import multiprocessingimport os, time, randomdef worker_1(interval): print "worker_1" time.sleep(interval) print "end worker_1"def worker_2(interval): print "worker_2" time.sleep(interval) print "end worker_2"def worker_3(interval): print "worker_3" time.sleep(interval) print "end worker_3"if __name__=='__main__': function_list= [worker_1,worker_2,worker_3] print "parent process %s" %(os.getpid()) pool=multiprocessing.Pool(4) for func in function_list: pool.apply_async(func,args=(1,)) #Pool执行函数，apply执行函数,当有一个进程执行完毕后，会添加一个新的进程到pool中 print 'Waiting for all subprocesses done...' pool.close() pool.join() #调用join之前，一定要先调用close() 函数，否则会出错, close()执行后不会有新的进程加入到pool,join函数等待素有子进程结束 print 'All subprocesses done. exiting ...' 利用Queue 多进程并发运行处理数据从前面我们知道，Queue是一个非常方便的进行进程间数据传输的工具，而同时我们也可以利用Queue这个特性来进行数据流的多进程并发设计,这里的关键在于如何设置退出机制，先学习一个例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import multiprocessingimport timeclass work_process(multiprocessing.Process): def __init__(self, task_queue, result_queue,flag): multiprocessing.Process.__init__(self) self.task_queue = task_queue # 任务队列 self.result_queue = result_queue # 结果队列 self.flag=flag # 退出标记 def run(self): proc_name = self.name while True: # 循环进行 next_task = self.task_queue.get() # 获取任务队列的数据 print('flag is %d' %self.flag) if next_task is None: ### 退出机制设计 # Poison pill means shutdown print ('%s: Exiting' % proc_name) self.task_queue.task_done() break print ('%s: %s' % (proc_name, next_task)) answer = next_task() # __call__() self.task_queue.task_done() self.result_queue.put(answer) #存储记录 print('proces is done') returnclass Task(object): def __init__(self, a, b): self.a = a self.b = b def __call__(self): ## we can use Task() to excute this method return '%s * %s = %s' % (self.a, self.b, self.a * self.b) def __str__(self): return '%s * %s' % (self.a, self.b)if __name__ == '__main__': # Establish communication queues tasks = multiprocessing.JoinableQueue() results = multiprocessing.Queue() flag=[0] # Start process num_process = multiprocessing.cpu_count() print ('Creating %d consumers' % num_process) record = [ work_process(tasks, results,flag[0]) for i in range(2) ] for w in work_process: w.start() # Enqueue jobs num_jobs = 10 for i in range(num_jobs): tasks.put(Task(i, i+1)) # Add a exit mechanism for each thread for i in range(4): tasks.put(None) flag[0]=1 # Wait for all of the tasks to finish tasks.join() # Start printing results while num_jobs: result = results.get() print ('Result:', result) num_jobs -= 1 从上面可以看到，程序中添加了两个退出机制的选择，但只有一个有效,就是利用queue在最后设置退出机制。那么我们可以从此知道如何在多进程情况下设置退出机制然后进行queue队列的并行计算。值得注意的是，tasks.join() 必须是JoinableQueue() 这个quene下面才能使用，同时，要避免出错必须加上在工作函数中加上self.task_queue.task_done()这个命令才行。否则就会出bug!]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Agricultural Household Model Introduction]]></title>
    <url>%2F2015%2F09%2F25%2FAgricultural-Household-Model-Intro%2F</url>
    <content type="text"><![CDATA[Agricultural Household Modelreference development microecoomics IntroductionIn developing countries, people who are farmer are very special. Because they earn at least part of their livelihood through work in their own enterprises(the land). At the same time, they also can consume a portion of their product(they grow crops, harvest them, sell them or eat them). We know that due to the unique feature of agriculture, household labour is often an important input into the production process of the farmer’s production activity. Consequently, Consequently, individuals make simultaneous decisions about production (the level of output, the demand for factors, and the choice of technology) and consumption (labor supply and commodity demand). This mixture just describes common daily life of most families in developing countries and provides starting point (Agricultural Household Model) for our analysis. Baseline Model OverviewThe canonical model of an agricultural household includes a utility function, defined over consumption by each member of the household (usually can be simplified by one or two members), and a budget constraint, which incorporates production on assets owned by the household. Consider a typical household with two household members whose aggregate utility is represented by a household utility function: \max U({c_1},{c_2},{l_1},{l_2})\ \ (1)where each member gets utility from consuming a good ($c_1$ and $c_2$ ) and from leisure( $l_1$ and $l_2$ ). Utility is an increasing function of both $c$ and $l$ The most simple agricultural household models assume that each household faces a complete set of competitive markets. (This includes, in more general models than the one presented here, a complete set of markets for time- and state-indexed commodities.) Such assumption can be released in the following extending part About the Budget Constrains, we have the following setup: Let $p$ be the price of output, and $w$ be the wage of output (for simplicity, that the labour of the two family members is homogeneous.) The household can produce the good on its farm according to the concave production function $F(L,A)$, where A is the area of the farm cultivated by thehousehold and $L$ is the amount of labour used on the farm. Let $E_i^L$ be personi’s endowment of time, $E^A$ the household’s endowment of land, and $r$ the price of one unit of land. \displaylines{ p({c_1} + {c_2}) + w{L^h} + r{A^h} \le F(L,A) + w(L_1^m + L_2^m) + r{A^m}\ \ (*) \cr L = L_1^f + L_2^f + {L^h} \ \ (3)\cr A = {A^f} + {A_h} \ \ (4)\cr {E^A} = {A^f} + {A^m},E_i^L = L_i^f + L_i^m + {l_i},i \in \{ 1,2\} \ \ (5)\cr {c_i},{l_i},L_i^f,L_i^m,{A^f},{A^m} \ge 0,\;i \in \{ 1,2\} \ \ (6)\cr}(*) is the budget constraint which tells us cash expenditures on consumption, hired labour($L^h$), and rented land ($A^h$) cannot exceed cash revenues from farming, market labour (rented labour $L_i^m$, and land rented out ($A^m$). the rest equations are resource constraints: labouruse on the farm is household labour used on the farm plus hired labour; land use on the farm is owned land used on the farm plus hired land; the household’s land endowment is used on its own farm or rented out, and each individual’s time endowment equals their labour use on the farm, plus market labour time, plus leisure time. The maximization is with respect to consumption and leisure, hired labour and land, and householdlabour and land supplied to the market and used on the household farm. when we substitute the (3)-(5) into (*), we have: \displaylines{ p({c_1} + {c_2}) + w({l_1} + {l_2}) \le \Pi + w(E_1^m + E_2^m) + r{E^A} \ \ (7)\cr \Pi = F(L,A) - wL - rA \ \ (8)\cr {c_i},{l_i},L,A \ge 0,\;i \in \{ 1,2\} \ \ (9)\cr}(7) is called the ‘full-income’ constraint: the value of consumption cannot exceed the value of the household’s endowment plus farm profits. The household’s problem is now to maximize (1) (with respect to $L,A,c_{i}$ and $l_{i}$) subject to (7)–(9). Three Key things to notices: none of the variables in (7), the equation for farm profits is in (1) all of the variables in (1) are on the left side (and none are on the right side) of (7) all variables on the right side of (7) other than Π are fixed As long as $U(\cdot)$ is characterized by local non-satiation(increasing), then (7) is binding at the solution and the maximized value of $U(\cdot)$ is increasing in $\Pi$ . This implies that $L$ and $A$ can first be chosen to maximize prifit $\Pi$, and thus maximize the right side of (7), and then $c_{1},c_{2},l_{1}$ and $l_{2}$ can be chosen to maximize (1) subject to (7). This ability to “separate” the maximization problem into two steps is called the separability property of this model of agricultural households, because the production decisions of the household are separable from the household’s consumption choices. The separation property is robust to the non-existence of some markets. For example, if there is no land market, then replace A by $E^{A}$ in (8) and set $r=0$. The problem remains recursive, and the household chooses labour inputs to maximize profits given the household’s endowment of land. This choice is independent of the household’s preferences or endowment of labour. An analogous result is true if there is no labour market but land can be traded freely. Thus, the household’s decision-making process can be divided in two stages: first, farm profit is maximized, and then utility is maximized given the full income budget constraint. we can simplify the model and describe it by graph. Suppose that U(·) is such that at all prices and wages $c_{1}=c_{2}=c$ and $l_{1}=l_{2}=l$. Again, assuming that there is no market for land, the household chooses $c,l,L$. $F(L,E^{A})$ is the production function on the household farm, given land endowment $E^{A}$. Given the real wage rate w/p, farm profits are maximized at \Pi(w/p.E^{A}) using $L^{}$ units of labour on the farm ($L^{}=argmax\{F(L,E^{A})-(w/p)L\}$), Then, given the budget constraint $pc=wE^{L}+\Pi(w/p.E^{A})-wl$. household utility is maximized by choosing consumption $c^{}$ and leisure $l^{}. To further illustrate the $\Pi(w/p.E^{A})=F(L,E^{A})-(w/p)L$ , $w/p$ can be regarded as the slop of $\Pi(w/p.E^{A})$, maximization requires $\Pi(w/p.E^{A})$be on the edge of $F(L,E^{A})$. The same principal when we turn to the maximization problem of $U(c,l)$. 3 Incomplete MarketIf multiple markets are incomplete, the separation property may no longer hold. The household no longer maximizes profit functions, and production decisions depend upon the preferences and endowments of the household. 3.1 imperfections in both the land and labour marketsA classic example is the problem of a household that faces imperfections in both the land and labour markets. Suppose again that there is no market for land, but now add the possibility that there is some involuntary unemployment in the rural labour market. The household cultivates its endowment of land, and might face a binding constraint on the amount of labour it can supply off its own farm. The household problem (now assuming just one person in the household) is: \max_{c,l,L^{H},L^{F}}U(c,l)\ \ (10)subject to \begin{matrix} pc & = & F({L^f} + {L^h},{E^A}) - w{L^h} + w{L^m} \ \ (11)\cr {E^L} & = & l + {L^f} + {L^m} \ \ \ \ \ (12) \cr {L^m} & \le & M \ \ (13) \cr \end{matrix} $L^{h}$ is labour hired by the household to work on its farm,$L^{f}$is the household’s own labour on its farm, $L^{m}$is the time spent by the household working for a wage. and $M$ is the maximum amount of time the household can spend working for a wage as a result of some (here unmodelled) labour market rationing not bindingif $L^{m}\leq M$ is not binding, then (11) becomes $pc+wl=F(L,EA)-wL+wE^{L}$, where $L$ is the amount of labour used on the farm. In this case, the household maximizes profits and the separation property holds. If separation holds, and the production function has constant returns to scale (CRTS), then all farms are identical. With CRTS, we can write $F(L,E^{A})=E^{A}f(L/E^{A})$, and the first-order condition for labour use is $w=f\prime(L/E^{A})$. All unconstrained farmers facing the same wage will use the same amount of labour per hectare, and achieve the same yield (output per unit of area) and output per unit of labour. bindingIn this case, $L^{m}=M,L^{h}=0$. The household’s problem becomes: \max_{c,l}U(c,l)\ \ (14)subject to pc=F(E^{L}-M,E^{A})+wM \ \ (15)first order conditions $U_{l}/U_{c}=F_{L}$. The household’s problem is illustrated in figure 2 inner axes demonstrate production on the household’s farm, with output on the vertical axis and labour input on the horizontal axis. $M$ hours are spent working in the market, earning $wM$. The household’s remaining labour time $(L^{f}$) is spent on the farm, producing q^{}. So the household works $M+L^{f}$ hours and consumes $pc^{}=wM+F(L^{f},E^{A})$ units of the good. The household achieves a maximized utility of $U(c^{},l^{})$ at point $A$. The household’s production choice clearly depends on its preferences and its endowment, and the separation property does not hold. This sort of market structure could give rise to an oft-observed pattern in the rural areas of less developed countries. Many observers find that small farms are often cultivated more intensively than large farms. More labour per unit area is used on small farms, and yields are larger on these smaller farms. Consider a household with more land than the household consuming at point $A$ , but facing the same wage and labour market constraint. If this household were to cultivate with the same intensity as household $A$, it would have to choose to produce and consume at point $C$ in the figure. If leisure is a normal good,$C$ will not be chosen. Instead, the household will choose to produce and consume at a point such as $B$ ,cultivating its larger farm less intensively than the smaller farm of household $A$ by implicitly differentiating the first-order condition: \frac{dL}{dE^{A}}=...]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>development economics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Write Markdown With Sublime Text]]></title>
    <url>%2F2015%2F07%2F20%2Fwrite-markdown-with-sublime-text%2F</url>
    <content type="text"><![CDATA[安装下载Sublime Text 2安装 安装Package Control按Ctrl + ` 打开console粘贴代码到console并回车重启Sublime Text 2import urllib2,os;pf=’Package Control.sublime-package’;ipp=sublime.installed_packages_path();os.makedirs(ipp) if not os.path.exists(ipp) else None;open(os.path.join(ipp,pf),’wb’).write(urllib2.urlopen(‘http://sublime.wbond.net/&#39;+pf.replace(‘ ‘,’%20’)).read()) 安装Markdown Preview按Ctrl + Shift + P输入pci 后回车(Package Control: Install Package)稍等… ^_^输入Markdown Preview回车 编辑按Ctrl + N 新建一个文档按Ctrl + Shift + P使用Markdown语法编辑文档语法高亮，输入ssm 后回车(Set Syntax: Markdown)五、在浏览器预览Markdown文档 按Ctrl + Shift + P输入mp 后回车(Markdown Preview: current file in browser)此时就可以在浏览器里看到刚才编辑的文档了]]></content>
      <categories>
        <category>Skills</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Treatment Effects Introduction]]></title>
    <url>%2F2015%2F06%2F27%2FTreatment%20Effects%20Introduction%2F</url>
    <content type="text"><![CDATA[Treatment Effectdefinition: The term ‘treatment effect’ refers to the causal effect of a binary (0–1) variable on an outcome variable of scientific or policy interest. The term ‘treatment effect’ originates in a medical literature concerned with the causal effects of binary, yes-or-no ‘treatments’, such as an experimental drug or a new surgical procedure. Economics examples include the effects of government programmes and policies, such as those that subsidize training for disadvantaged workers, and the effects of individual choices like college attendance. Given a data-set describing the labour market circumstances of trainees and a non-trainee comparison group, we can compare the earnings of those who did participate in the programme and those who did not. Any empirical study of treatment effects would typically start with such simple comparisons. In general, omitted variables bias (also known as selection bias) is the most serious econometric concern that arises in the estimation of treatment effects. The link between omitted variables bias, causality, and treatment effects can be seen most clearly using the potential-outcomesIn general, omitted variables bias (also known as selection bias) is the most serious econometric concern that arises in the estimation of treatment effects. The link between omitted variables bias, causality, and treatment effects can be seen most clearly using the potential-outcomesframework. ModelFirst assume a binary treatment. For each population unit, two possible outcomes: $y(0)$ (the outcome without treatment) and $y(1)$ (the outcome with treatment). The binary “treatment” indicator is $D$, where $D=1$ denotes “treatment”, The nature of $y(0)$ and $y(1)$ – discrete,continuous, some mix -is ok. The gain from treatment is: y(1)-y(0)For a particular unit $i$, the gain from treatment is$y_{i}(1)-y_{i}(0)$ Problem: For each unit i, only one of $y(0)$ and $y(1)$ is observed. In effect, we have a missing data problem (even though we willeventually assume a random sample of units) Two parameters are of primary interest: ATE(average treatmenteffect) and ATT(average treatment effect on the treated ) ATE \tau_{ATE}=E[y(1)-y(0)] The expected gain for a randomly selected unit from the population. This is sometimes called the average causal effect. ATT \tau_{ATT}=E[y(1)-y(0)|D=1] With heterogeneous treatment effects, (2) and (3) can be very different. ATE averages across gain from units that might never be subject to treatment. how we estimate $\tau_{ATT}$ and $\tau_{ATE}$ depends on what weassume about treatment assignment. the problem in estimating is that sample is limited, which means we only know $y(1|D=1)$ and $y(0|D=0)$we can not know the $y(1|D=0)$ Sampling AssumptionsAssume independent, identically distributed observations from the underlying population. The data we would like to have is $(y_{i}(0),y_{i}(1)) : i = 1, . . . ,N$ But we only observe $D_{i}$ and y_{i}=(1-D_{i})y_{i}(0)+D_{i}y_{i}(1)=y_{i}(0)+D_{i}{y_{i}(1)-y_{i}(0)}in other word( each individual can only be observed in one of the possible treatments) Random sampling rules out treatment of one unit having an effect onother unitsEstimation under Random Assignment Strongest form of random assignment, $[y(0),y(1)]$ is independent of $D$ E(y|D=1)-E(y|D=0)=E(Y(1))-E(y(0))=\tau_{ATE}=\tau_{ATT} under mean independence and the means on the left hand side can beestimated by using sample averages on the two subsamples To start with, we consider a linear structural outcome equation, and homogeneous effects: y=\beta d+\epsilonDID (different in different method)Definition Two groups: D = 1 Treated units D = 0 Control units Two periods: T = 0 Pre-Treatment period T = 1 Post-Treatment period Potential outcome $Y_{d}(t )$ $Y_{1i}(t )$ outcome unit i attains in period t when treatment between t and t − 1 $Y_{0i}(t )$ outcome unit i attains in period t with control between t and t − 1 ESITMATE ATETFocus on estimating the average effect of the treatment on thetreated \alpha_{ATT}=E[Y_{1}(1)-Y_{0}(1)|D=1] Tables Post-period(t=1) Pre-period(t=0) Treated D=1 $E[Y_{1}(1);D=1]$ $E[Y_{0}(0);D=1]$ Control D=0 $E[Y_{0}(1);D=0]$ $E[Y_{0}(0);D=0]$ ProblemMissing potential outcome: $E[Y_{0}(1)|D = 1]$, i.e., what is the average post-period outcome for the treated in the absence of the treatment? solving strategy Before vs. After Use: $E [Y (1)|D = 1] − E [Y (0)|D = 1]$ Assumes: $E [Y_{0}(1)|D = 1]= E [Y_{0}(0)|D = 1]$ Treated vs. Control in Post-Period Use: $E [Y (1)|D = 1] − E [Y (1)|D = 0]$ Assumes: $E [Y_{0}(1)|D = 1]= E [Y_{0}(1)|D = 0]$ Difference-in-Differences Use: $\{E [Y (1)|D = 1] − E [Y (1)|D = 0] \}- \{ E [Y (0)|D = 1] − E [Y (0)|D = 0]\}$ Assumes: $E[Y_{0}(1) − Y_{0}(0)|D = 1] = E[Y_{0}(1) − Y_{0}(0)|D = 0]$ The rest of the content is in the slides[resource link]]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>econometrics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Magoosh GRE Analytical Writing Notes]]></title>
    <url>%2F2014%2F10%2F25%2Fwriting-notes%2F</url>
    <content type="text"><![CDATA[writing tipsClarity and Concison!!!bad:Independent thought is highly valued in society, but it is better that knowledge, if it is arrived at, is achieved through the collection of a group of peoplegood Independent thought, while highly valuable, ultimately needs to be validated by society. Logical flow由反转正可能更为合适。badmany people have contributed helpful inventions, so technology is making life easier for us.contributed invention -&gt;!!! NO technology can make life easierinvention NO technologycontribute NO life easiergoodThe constant influx of new technologies can lead us to feel overwhelmed. Nevertheless, many of these technologies can make our lives easier. Style and Sentence Variety 同位语，变换多样单词 be specific Grammer and Spellingsynonym—同位语the dog are barking 是严重的错误 time management brainstorm and outline (3min) write intro and thesis statement(3min) write the body and example (20 min) write conclusion (2 min) Finish intro (2min) tro to ISSUE TASK Direction -&gt; agree or disgree agree | disagree support your position extra info Choose a side choose a side but need to look at both side. 两边都要照顾到，有一个主要的方向 concession point other side is also valid in some cases 对每一个大类中的topic 都要讨论一下pros and cons write thesis write body write conclusion finish intro ISSUE writingThesis kepp thesis short and sweet choose a side most important sentence 一定要短小精炼，把核心写出来，不要长！就一句话！！！ Body举例子，假设时。越具体越好！！！！！！结尾时，要回归到ISSUE上，点题。 concession point consider other side of argument qualify one of your previous points conclusion restate thesis and body do not introduce new information最后可以让步强化 intro introduce topic 用自己语言分析topic然后往自己想占的立场说话 声明自己的立场 on how to writing wellyour lead must capture the reader immediately and force him to keep reading. It must cajole him with freshness, or novelty, or paradox, or humor, or surprise, or with an unusual idea, or an interesting fact, or a question. Anything will do, as long as it nudges his curiosity and tugs at his sleeve. 结尾要有惊喜，这就像一根诱人口水的大骨头，首先它要有嚼头，然后它得告诉读者，这是最后一根了。 VERBS. 不到万不得已，不要用被动动词 Use active verbs unless there is no comfortable way to get around using a passive verb. Prune out the small words that qualify how you feel and how you think and what you saw: “a bit,” “a little,” “sort of,” “kind of,” “rather,” “quite,” “very,” “too,” “pretty much,” “in a sense” and dozens more. They dilute your style and your persuasiveness. The Dash 破折号的用法 The dash is used in two ways. One is to amplify or justify in the second part of the sentence a thought you stated in the first part. “We decided to keep going—it was only 100 miles more and we could get there in time for dinner.”By its very shape the dash pushes the sentence ahead and explains why they decided to keep going. The other use involves two dashes, which set apart a parenthetical thought within a longer sentence. “She told me to get in the car—she had been after me all summer to have a haircut—and we drove silently into town.” An explanatory detail that might otherwise have required a separate sentence is dispatched along the way. But 开头能用that 不要用whichwhich 更为精确If your sentence needs a comma to achieve its precise meaning, it probably needs “which.” “Which” serves a particular identifying function, different from “that.” (A) “Take the shoes that are in the closet.” This means: take the shoes that are in the closet, not the ones under the bed. (B) “Take the shoes, which are in the closet.” Only one pair of shoes is under discussion; the “which” usage tells you where they are. Note that the comma is necessary in B, but not in A. Do – prune out every word that does not perform a necessary function. Strip each sentence to its cleanest components. A clear sentence is no accident. Do – use the thesaurus liberally. Learn the small gradations between words that seem to be synonyms. Do – try to improve the rhythm by reversing the order of a sentence, substituting a word that has freshness or oddity, and by varying the lengths of sentences. Do – make your first sentence the best one – your lead must capture the reader. Do – make each sentence lead into the next. Readers think linearly. Do – Take special care with the last sentence in your paragraph – its the springboard to the next paragraph. Do – make your paragraphs short. Readers think in segments. Do – pay special attention to the last sentence. The perfect ending should take your reader slightly by surprise and yet seem exactly right. Do – Read it aloud to see how it sounds and re-edit – then do it again. Clear writing is the result of lots of tinkering. On the other hand: Don’t – use passive verbs unless there is no comfortable way to use an active verb. Don’t – use adverbs that convey the same meaning as your strong active verb – prune it out. Don’t – use adjectives when the concept is already in your carefully chosen noun – prune it out. Don’t – use small words that qualify how you feel: “a bit,” “a little,” “sort of,” and dozens more. Good writing is lean and confident. Don’t – use concept nouns:Instead of – “The common reaction is incredulous laughter.”Write – “Most people just laugh with disbelief.” Don’t – use the exclamation point unless you must, do use the period more frequently, don’t forget the versatile dash, and cut down on the use of semi-colons and colons. If you don’t know how to punctuate, get a grammar book. Intro to Argument challenge argument 找文章的逻辑漏洞 Discuss ways to improve the argument 寻找能够提高文章说服力的途径 Directions Indentify stated/unstated assumption Strengthen and weaken argument Logical Fallacies things changed 【3年前成功，今年也成功】-错 assumed cause and effect 【一个在另一个前发生，所以前一个导致后一个】-错 numbers and percentage assumptions 【数据和比例问题】 vague language 【in better shape than others 】 not all X are alike 【以局部推断总体】 don’t trust a survey apple aren’t oranges 【different cities 】-错误类比 argument taskintro keep intro short and sweet use standard/boilerplate language body focus on three logical fallacies use a paragraph for each fallacy strengthen argument conclusion keep short and sweet Written with StackEdit.]]></content>
      <categories>
        <category>Skills</category>
      </categories>
      <tags>
        <tag>GRE</tag>
      </tags>
  </entry>
</search>
